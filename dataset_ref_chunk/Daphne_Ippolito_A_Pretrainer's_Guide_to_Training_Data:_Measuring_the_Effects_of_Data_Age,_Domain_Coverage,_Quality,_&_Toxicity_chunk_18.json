{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_A_Pretrainer's_Guide_to_Training_Data:_Measuring_the_Effects_of_Data_Age,_Domain_Coverage,_Quality,_&_Toxicity_chunk_18.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are two methods used to distinguish between text that looks like Wikipedia and books?", "answer": " PaLM and GLaM", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What are the four categories of personally identifiable information detected by a basic classifier?", "answer": " Names, phone numbers, addresses, and emails", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What is the Flesch-Kincaid readability test based on?", "answer": " Number of words per sentence and number of syllables per word", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " How is the average word length measured?", "answer": " Measured in characters", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What does the Type-Token Ratio measure?", "answer": " The ratio of unique tokens to total tokens", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What does the sentiment score evaluate?", "answer": " The overall sentiment of the text along a spectrum from positive to negative", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " How is the temporal information in the pretraining data estimated?", "answer": " By counting instances of dates from 2000 to 2025 in each corpus", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What are the types of datasets that quality filters are typically applied to?", "answer": " Large, heterogeneous datasets such as C4", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What does Figure 9 show?", "answer": " Feature differences across C4 and the Pile, and time snapshots of C4", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}, {"question": " What is the main focus of Figure 12?", "answer": " Temporal misalignment in finetuning and its effects on task performance", "ref_chunk": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}], "doc_text": "PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), is used to distinguish between text that looks like Wikipedia and books from other text, as described in Section 2.2. Personally Identifiable Information (PII) A basic classifier, similar to Google Cloud NLP (2023a), detects the presence of four categories of personally identifiable information: names, phone numbers, addresses, and emails. Readability The Flesch\u2013Kincaid readability test (Kincaid et al., 1975) is applied to each document, assigning documents a grade level based on the number of words per sentence and number of syllables per word. Average Word Length Measured in characters. Document Length Measured in characters. Non-ASCII Characters Measured as a percentage of all characters in the document. All-caps Words Measured as a percentage of all words in the document. Type-Token Ratio A measure of the lexical diversity, or the ratio of unique tokens to total tokens (Bender, 2013). Sentiment The score assigned by a classifier similar to Google Cloud NLP (2023b), evaluating the overall sentiment of the text along a spectrum from positive to negative. Temporal information in pretraining data While we collected versions of C4 at four different years, each of these versions may also contain data from prior years. We estimate the temporal information in the pretraining data by counting instances of dates from 2000 to 2025 in each corpus. We do see that there are many mentions of the year of collection, with a quick dropoff of about 5 years earlier (see Figure 10). This is necessarily a limited experiment as an article written in 2016 may still mention something occurring in the future in 2019. However, since website creation dates are not part of the web-scrape, we use this as a proxy to estimate website creation dates. D.2 Breakdown of the Quality Filter on Pile Domains While the quality filters are typically applied to large, heterogeneous datasets such as C4, we also ran the quality classifier on the Pile to get a better understanding of what types of datapoints actually passed the quality filtering thresholds. The results are shown in Figure 11. 34 0.121.2x1.2x1.0x1.0x 0.371.3x1.3x1.0x1.1x 2013201620192022Profanity 0.031.4x1.5x1.0x1.3x 2.91e-03.8x.9x1.0x1.3x 0.261.1x1.1x1.0x.8x %Non-AsciiCharacters HasPhone 0.161.3x1.3x1.0x1.0x TypeTokenRatio 0.021.3x1.3x1.0x1.0x SexuallyExplicit 10.9x.9x1.0x.9x HasEmail 1.16.9x.9x1.0x1.0x HasPersonName 0.581.0x1.0x1.0x1.0x Toxicity 0.101.4x1.4x1.0x1.0x Readability 0.021.3x1.3x1.0x1.4x Features Across Time (C4) 0.032.2x2.4x1.0x1.9x 25921.1x1.2x1.0x1.3x TextQuality HasAddress %AllCaps Sentiment NumberOfCharacters 0.261.0x1.2x 0.161.0x.9x Features across C4 and The Pile 0.581.0x.9x 0.031.0x1.2x 0.021.0x1.7x 0.121.0x1.0x C4Pile 1.161.0x.8x 0.031.0x.7x 2.91e-031.0x1.9x 101.0x1.8x 0.101.0x1.1x 0.021.0x1.2x 25921.0x2.4x 0.371.0x1.1x (a) Time in C4 (b) C4 vs the Pile Figure 9: Feature differences across C4 and the Pile, and time snapshots of C4 Bar height indicates average feature value of each dataset, except for the PII categories which show the fraction of datapoints containing that PII type. The numbers are the fraction difference between the dataset and the baseline, which in this case is C4. The gray dashed line and gray number show the actual value for the baseline. 20% 20% 30% 20002003200620092012201520182021 20% 20002003200620092012201520182021Year Percentage of year mentions in datasetC4 2022 10% 30% 10% 10% C4 2016 30% 30% 10% 20002003200620092012201520182021 C4 2019 C4 2013 0% 20002003200620092012201520182021 0% 0% 0% 20% 0.06 Social 1.0 0.4 0 Data with a Quality Score >= Cutoff Code Academic 0.8 OpenWeb 0.025 Books 0.6 0.0 CC 1.2Number of examples1e8 Legal 0.2 0.1 PubMed Wiki 0.2Filtering cutoff Figure 10: Date instances in each of the C4 temporal pretraining versions. Figure 11: Breakdown of domains in the Pile after fil- tering for multiple quality cutoffs. 35 E Experimental Results In this section, we lay out the raw results for our toxicity, quality, and temporal degradation evaluations, spanning several evaluation datasets. E.1 Temporal Degradation Results Luu et al. (2021) measure the temporal degradation due to finetuning and evaluation misalignment. Before attempting to evaluate misalignment effects specifically for pretraining, we mimic their finetuning experiments. Figure 12 shows our results, which corroborate the findings of (Luu et al., 2021). 20102012201420162010201220142016 2012201320142015201620172018201920202021Eval Years2009-20132014-20152016-20172018-20192020-2021 100.0100.095.273.167.256.453.171.074.768.094.697.8100.097.682.067.857.673.673.667.885.692.494.386.891.892.880.582.579.869.173.785.087.558.667.586.991.294.590.581.549.564.875.338.149.672.778.290.594.893.3PoliAff 2014201520162017201820192014-20152016-20172018-2019 31.932.125.217.121.539.029.914.419.534.834.715.619.520.620.625.8NewSum 98.497.995.491.394.386.997.998.494.893.295.690.697.498.693.792.495.490.0AIC 20102012201420162010201220142016Finetune Years 92.751.058.652.655.392.377.674.881.783.990.983.675.182.684.287.0PubCLS 85.685.485.783.081.682.185.483.784.784.483.283.982.184.284.482.284.985.4TwiERC 201420152016201720182019Eval Years2014-20152016-20172018-2019Finetune Years Figure 12: A replication of how temporal misalignment in finetuning affects task performance (Luu et al., 2021). In contrast to Figure 3, which shows the effects of pretraining misalignment, this figure focuses on the more well established effect of finetuning misalignment. Next we share the original evaluation results from which we computed the temporal degradation values for both finetuning and pretraining. These contain a cross-section of the scores produced using a given pretraining year (y-axis), finetuning year(s) (y-axis), for an evaluation year (x-axis). These results, Tables 10 to 13, are provided for both LM-XL and LM-Small, for comparison. 36 Table 10: Left: Full results on the PubCLS temporal task splits from (Luu et al., 2021). This task evaluates news article source classification, measured with Accuracy. Right: Full results on the NewSum summarization task temporal splits from (Luu et al., 2021), evaluated in Rouge-L. Pretrain Time Finetune Time 2010 Eval Time 2014 2012 2016 Pretrain Time Finetune Time 2010 Eval Time 2014 2012 LM-XL LM-XL 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 93.7 60.2 83.1 78.7 93.8 55.2 81.5 76.9 92.9 53.4 81.3 72.3 90.5 52.4 80.9 72.3 51.9 94.6 85.6 84.7 51.6 93.9 86.2 82.9 50.6 90.5 83.2 81.1 49.9 90.4 80.7 81.7 58.4 78.4 90.8 86.2 59.2 79.5 92.8 84.3 58.6 75.9 90.6 83.4 58.4 76.4 89.3 83.0 52.5 75.6 84.8 87.6 53.2 77.0 85.6 89.6 52.2 72.9 82.8 84.8 52.4 73.9 81.1 86.1 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 33.3 21.4 19.9 19.9 31.9 21.4 20.2 19.6 31.8 21.4 18.6 19.5 30.7 21.6 19.5 19.1 32.8 39.5 35.0 21.2 33.3 39.0 35.0 20.8 31.6 39.1 33.8 20.1 30.8 38.2 35.5 20.4 24.6 30.0 35.1 21.1 27.1 30.1 34.5 20.0 24.8 29.3 34.0 21.4 24.4 30.1 35.0 19.9 LM-Small LM-Small 2013 2016 2019 2022 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 2010 2012 2014 2016 92.9 55.4 78.2 70.5 93.0 56.7 77.3 69.9"}