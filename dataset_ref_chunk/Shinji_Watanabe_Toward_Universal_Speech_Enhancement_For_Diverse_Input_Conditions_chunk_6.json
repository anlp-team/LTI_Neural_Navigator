{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Toward_Universal_Speech_Enhancement_For_Diverse_Input_Conditions_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of model was devised in the paper?,        answer: A single speech enhancement model called USES    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What tasks can the proposed model handle?,        answer: Denoising and dereverberation in diverse input conditions altogether    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What is the purpose of preparing the transcription manually for the DNS1 test data?,        answer: To serve as the reference for the test data    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What phenomenon is commonly observed in the SE model on DNS1 and CHiME-4?,        answer: The SE model does not improve ASR performance due to introduced artifacts by enhancement in mismatched conditions    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What do the experiments on a wide range of datasets show about the proposed model?,        answer: The proposed model can achieve very competitive performance for both speech separation and speech enhancement tasks    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What aspect of SE literature is revealed through the benchmark designed in the paper?,        answer: Generalizability across different domains    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What is the hope for the contribution made in the paper?,        answer: To attract more efforts toward building universal SE models for real-world speech applications    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What is the URL where the DNS1 transcription manual is available?,        answer: github.com/Emrys365/DNS_text    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " In what year was the Springer handbook of speech processing published?,        answer: 2008    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}, {"question": " What is the purpose of the SEGAN model mentioned in the references?,        answer: Speech enhancement generative adversarial network    ", "ref_chunk": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}], "doc_text": "in Table 4. Note that on DNS1 (reverb) and CHiME-4 (Real), the SE model does not improve the ASR perfor- mance, which is a commonly observed phenomenon [44\u201346] due to the introduced artifacts by enhancement in mismatched conditions. 4. CONCLUSION In this paper, we have devised a single speech enhancement model USES that can handle denoising and dereverberation in diverse input conditions altogether, including variable microphone channels, sam- pling frequencies, signal lengths, and different environments. Ex- periments on a wide range of datasets show that the proposed model can achieve very competitive performance for both speech separa- tion and speech enhancement tasks. We further design a benchmark for evaluating the universal SE performance across various condi- tions, which also reveals some less-explored aspects in the SE liter- ature such as the generalizability across different domains. We hope this contribution can attract more efforts toward building universal SE models for real-world speech applications. 11For DNS1 test data, we prepare the transcription manually as the refer- 10The training of both models is well converged with our setup. ence, which is available at github.com/Emrys365/DNS_text. 2.3 2.8 5.9 30.4 4.1 4.6 6.2 6.9 7.1 5.0 5. REFERENCES [1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492\u20131501, 2017. [3] A. Li et al., \u201cGlance and gaze: A collaborative learning frame- work for single-channel speech enhancement,\u201d Applied Acous- tics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., \u201cFRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,\u201d in ICASSP, 2022, pp. 9281\u20139285. [5] Y. Xu et al., \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7\u201319, 2014. [6] Z.-Q. Wang et al., \u201cComplex spectral mapping for single- and multi-channel speech enhancement and robust ASR,\u201d IEEE/ACM Trans. ASLP., vol. 28, pp. 1778\u20131787, 2020. [7] A. Li et al., \u201cTaylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,\u201d in Proc. IJCAI, 2022, pp. 4193\u20134200. [8] L. Liu et al., \u201cA mask free neural network for monaural speech enhancement,\u201d in Interspeech, 2023, pp. 2468\u20132472. [9] S. Pascual et al., \u201cSEGAN: Speech enhancement generative adversarial network,\u201d in Interspeech, 2017, pp. 3642\u20133646. [10] S. Maiti and M. I. Mandel, \u201cSpeech denoising by parametric resynthesis,\u201d in ICASSP, 2019, pp. 6995\u20136999. [11] S.-W. Fu et al., \u201cMetricGAN+: An improved version of Met- ricGAN for speech enhancement,\u201d in Interspeech, 2021, pp. 201\u2013205. [12] Y.-J. Lu et al., \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in ICASSP, 2022, pp. 7402\u20137406. [13] J. Serr\u00e0 et al., \u201cUniversal speech enhancement with score- based diffusion,\u201d arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., \u201cLA-VocE: Low-SNR audio-visual speech en- hancement using neural vocoders,\u201d in ICASSP, 2023, pp. 1\u20135. [15] Y. Luo et al., \u201cEnd-to-end microphone permutation and num- ber invariant multi-channel speech separation,\u201d in ICASSP, 2020, pp. 6394\u20136398. [16] T. Yoshioka et al., \u201cVarArray: Array-geometry-agnostic con- tinuous speech separation,\u201d in ICASSP, 2022, pp. 6027\u20136031. [17] A. Pandey et al., \u201cTime-domain ad-hoc array speech enhance- ment using a triple-path network,\u201d in Interspeech, 2022, pp. 729\u2013733. [18] Z. Chen et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP, 2020, pp. 7284\u20137288. [19] K. Saito et al., \u201cSampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,\u201d in Proc. EUSIPCO, 2021, pp. 321\u2013325. [20] J. Paulus and M. Torcoli, \u201cSampling frequency independent dialogue separation,\u201d in Proc. EUSIPCO, 2022, pp. 160\u2013164. [21] J. Yu and Y. Luo, \u201cEfficient monaural speech enhancement with universal sample rate band-split RNN,\u201d in ICASSP, 2023, pp. 1\u20135. [22] C. Valentini-Botinhao et al., \u201cSpeech enhancement for a noise- robust text-to-speech synthesis system using deep recurrent neural networks,\u201d in Interspeech, 2016, pp. 352\u2013356. [23] C. K. Reddy et al., \u201cThe INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d in Interspeech, 2020, pp. 2492\u20132496. [24] E. Vincent et al., \u201cAn analysis of environment, microphone and data simulation mismatches in robust speech recognition,\u201d Computer Speech & Language, vol. 46, pp. 535\u2013557, 2017. [25] K. Kinoshita et al., \u201cThe REVERB challenge: A common eval- uation framework for dereverberation and recognition of rever- berant speech,\u201d in Proc. IEEE WASPAA, 2013, pp. 1\u20134. [26] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in ICASSP, 2019, pp. 696\u2013 700. [27] C. Li et al., \u201cESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,\u201d in Proc. IEEE SLT, 2021, pp. 785\u2013792. [28] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scan- ning network for speech separation,\u201d in ICASSP, 2022, pp. 6842\u20136846. [29] Z.-Q. Wang et al., \u201cTF-GridNet: Making time-frequency do- main models great again for monaural speaker separation,\u201d in ICASSP, 2023, pp. 1\u20135. [30] J. Chen et al., \u201cDual-path transformer network: Direct context- aware modeling for end-to-end monaural speech separation,\u201d in Interspeech, 2020, pp. 2642\u20132646. [31] S. Cornell et al., \u201cLearning filterbanks for end-to-end acoustic beamforming,\u201d in ICASSP, 2022, pp. 6507\u20136511. [32] M. S. Burtsev et al., \u201cMemory transformer,\u201d arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, \u201cPrefix-Tuning: Optimizing continu- ous prompts for generation,\u201d in Proc. ACL/IJCNLP, 2021, pp. 4582\u20134597. [34] K.-W. Chang et al., \u201cAn exploration of prompt tuning on gen- erative spoken language model for speech processing tasks,\u201d in Interspeech, 2022, pp. 5005\u20135009. [35] J. R. Hershey et al., \u201cDeep clustering: Discriminative embed- dings for segmentation and separation,\u201d in ICASSP, 2016, pp. 31\u201335. [36] Z.-Q. Wang et al., \u201cMulti-channel deep clustering: Discrimina- tive spectral and spatial embeddings for speaker-independent speech separation,\u201d in ICASSP, 2018, pp. 1\u20135. [37] J. Le Roux et al., \u201cSDR\u2014half-baked or well done?\u201d ICASSP, 2019, pp. 626\u2013630. [38] Y.-J. Lu et al., \u201cTowards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,\u201d in ICASSP, 2022, pp. 9201\u20139205. [39] A. W. Rix et al., \u201cPerceptual evaluation of speech quality (PESQ)\u2014a new method for speech quality assessment of tele- phone networks and codecs,\u201d in ICASSP, vol."}