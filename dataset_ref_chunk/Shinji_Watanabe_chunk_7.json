{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the first paper described in the text?", "answer": " Investigating a single SE model that can handle diverse input conditions", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What are the two main goals mentioned for the proposed SE model in the first paper?", "answer": " Being independent of microphone channels, signal lengths, and sampling frequencies, and designing a universal benchmark", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What type of system is described in the second paper mentioned in the text?", "answer": " System for low-resource domain adaptation track in Spoken Language Understanding Grand Challenge", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What approach does the system in the second paper use for ASR and NLU?", "answer": " Pipeline approach with Whisper fine-tuning for ASR and BART fine-tuning for NLU", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What was the exact match accuracy achieved by the system in the second paper for the reminder/weather domain?", "answer": " 63.3/75.0 (average: 69.15)", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What does the third paper discussed in the text focus on?", "answer": " Integration of pipeline and E2E SLU systems for Spoken Semantic Parsing", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What is the output of level combination mentioned in the third paper used for?", "answer": " To get an exact match accuracy of 80.8", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " Where did the community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir occur?", "answer": " Nara, Japan", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What substitution in the PA gene of influenza A(H3N2) virus caused reduced susceptibility to baloxavir?", "answer": " E199G", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}, {"question": " What is the primary focus of the study described in the last paper mentioned in the text?", "answer": " Joint modeling of spoken language understanding tasks with integrated dialog history", "ref_chunk": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}], "doc_text": "dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']] ['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']] ['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']] ['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']] ['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our"}