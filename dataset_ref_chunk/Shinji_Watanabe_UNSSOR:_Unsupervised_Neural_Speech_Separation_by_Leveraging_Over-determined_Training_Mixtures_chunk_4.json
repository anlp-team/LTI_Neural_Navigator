{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_UNSSOR:_Unsupervised_Neural_Speech_Separation_by_Leveraging_Over-determined_Training_Mixtures_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the formula used to compute the closed-form solution in the text?", "answer": " (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What is the name given to the speaker image estimated in the described manner?", "answer": " FCP-estimated image", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What method does the text propose to use for filtering to approximate the reverberant image Xp(c) at non-reference microphones?", "answer": " Non-causal filters", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What modification is made to the loss function in (4) to address time alignment issues?", "answer": " The DNN estimates are also filtered when computing the loss on the reference microphone, and \u02c6gp(c, f ) is constrained to be causal.", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " Why is it noted that the filters in MCLP and (6) have very different physical meanings?", "answer": " MCLP does inverse filtering to approximate target sources, while (6) does forward filtering to estimate relative RIRs.", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What issue does the proposed loss term ISMS aim to alleviate in DNN outputs?", "answer": " The frequency permutation problem", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What is the shared variance term assumed in IVA to solve the frequency permutation problem?", "answer": " D(t, c)", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " How is the FCP performed in the text?", "answer": " Independently in each frequency from the others", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What are the popular solutions designed to address the frequency permutation problem?", "answer": " Cross-frequency correlation of spectral patterns and direction-of-arrival estimation", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}, {"question": " What are the non-trivial changes in physical meanings between forward filtering and inverse filtering?", "answer": " Forward filtering estimates relative RIRs, while inverse filtering approximates target sources.", "ref_chunk": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}], "doc_text": "closed-form solution can be readily computed: (cid:16) 1 P p\u2032=1|Yp\u2032(t, f )|2(cid:17) p\u2032=1|Yp\u2032|2(cid:17) (cid:80)P (cid:80)P + \u03be \u00d7 max \u02c6gp(c, f ) = (cid:16) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(cid:101)\u02c6Z(c, t, f )H(cid:17)\u22121 (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:101)\u02c6Z(c, t, f )(Yp(t, f ))\u2217, where (\u00b7)\u2217 computes complex conjugate. We then plug \u02c6gp(c, f ) into (4) and compute the loss. Note that to compute the relative RIR, ideally we should filter \u02c6Z(c) to approximate Xp(c) (i.e., replacing Yp in (6) with Xp(c)), but Xp(c) is unknown. In (6), we instead linearly filter \u02c6Z(c) to approximate Yp, and earlier studies [29, 60] suggest that the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would be an estimate of Xp(c, t, f ), if \u02c6Z(c) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as FCP-estimated image: \u02c6X FCP p (c, t, f ) = \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ). It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and Yp as in (4). Although (6) appears similar to multi-channel linear prediction (MCLP) [61, 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does forward filtering, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does inverse filtering, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of [29]). Although there were earlier efforts in estimating relative RIRs [56], they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics. 4.4 Time alignment issues and alternative loss functions In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \u02c6Z(c) time-aligned with the speaker image X1(c) (i.e., \u02c6Z(c) is an estimate 5 (6) (7) (8) of X1(c)). Since the reference microphone may not be the microphone closest to speaker c, it is best to use non-causal filters when filtering \u02c6Z(c) to approximate the reverberant image Xp(c) at non-reference microphones that are closer to source c than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN\u2019s capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source c than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters. To address this issue, we make a simple modification to the loss function in (4): LMC = (cid:88)P p=1 \u03b1pLMC,p = (cid:88)P p=1 \u03b1p (cid:88) t,f F (cid:16) Yp(t, f ), (cid:88)C c=1 \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \u02c6gp(c, f ) to be causal and that (cid:101)\u02c6Z(c, t, f ) only stacks current and past frames. This way, the resulting \u02c6Z(c) would not be time-aligned with the revererberant image captured at the reference microphone (i.e., X1(c)) or any other non-reference microphones. Because of the causal filtering, \u02c6Z(c) would be more like an estimate of the reverberant image captured by a virtual microphone that is closer to speaker c than all the P microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker c than any of the speaker images captured by the P microphones due to the causal filtering. To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., Xp(c)), we use the FCP-estimated image computed in (8) (i.e., \u02c6X FCP (c)) as the output. p 4.5 Addressing frequency permutation problem In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the frequency permutation problem [41], which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37\u201341] and spatial clustering [45\u201348]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47, 64] and direction-of-arrival estimation [65]. However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation. To deal with frequency permutation, IVA [42\u201344] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: w(c, f )HY(t, f ) \u223c N (0, D(t, c)), where w(c, f ) \u2208 CP is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker c at frequency f , and D(t, c) \u2208 R is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41\u201344]. Motivated by IVA, we design the following loss term, named intra-source magnitude scattering (ISMS), to alleviate the frequency permutation problem in DNN outputs: LISMS"}