{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_New_Benchmark_of_Aphasia_Speech_Recognition_and_Detection_Based_on_E-Branchformer_and_Multi-task_Learning_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the popular speech corpus mentioned in the text?,        answer: AphasiaBank    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " What does the dataset in AphasiaBank contain?,        answer: Spontaneous conversations between investigators and Aphasia patients, as well as conversations with healthy individuals as the control group.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " How are the training, validation, and test sets obtained in AphasiaBank?,        answer: By drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity level.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " What are the four severity levels in AphasiaBank and their corresponding AQ score ranges?,        answer: Mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25).    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " What is the purpose of splitting the control group in AphasiaBank?,        answer: To ensure data splits are representative across all severity levels.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " How are the recordings sliced into sentences in AphasiaBank?,        answer: Using the timestamps provided in the CHAT transcripts.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " What happens to sentences shorter than 0.3 seconds or longer than 30 seconds in AphasiaBank?,        answer: They are removed.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " What corpus is used to test the generalizability of the design in the text?,        answer: DementiaBank Pitt corpus    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " How is the audio from the challenge test set in DementiaBank Pitt corpus enhanced?,        answer: With noise removal and volume normalization.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}, {"question": " What do the proposed methods achieve according to Table 1 in the text?,        answer: They achieve a Word Error Rate (WER) improvement.    ", "ref_chunk": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}], "doc_text": "postcodes, punctuations, comments, explanations, special utterance terminators, and spe- cial form markers (4) Remove markers of word errors, inter- ruption, paralinguistics, pauses, overlap precedes, local events, gestures, and unrecognized words. (5) Remove all empty sen- tences after the above steps. 3.1.2. AphasiaBank and DementiaBank AphasiaBank [36] is a popular speech corpus among the exist- ing work. The dataset contains spontaneous conversations be- tween investigators and Aphasia patients. It also includes con- versations with healthy individuals as the control group. All experiments in this paper are performed using the English sub- set. Similar to [5], we obtain the training, validation, and test set by drawing 56%, 19%, and 25% percent of Aphasic speakers from each severity. There are four severity levels, each corre- sponding to a range of AQ scores: mild (AQ > 75), moderate (50 < AQ \u2264 75), severe (25 < AQ \u2264 50), and very severe (0 \u2264 AQ \u2264 25) [36]. The control group is split using the same ratio and merged with patients\u2019 data. Doing so ensures our data splits are representative across all severity levels. We then slice the recordings into sentences using the timestamps provided in the CHAT transcripts while cleaning them as described in Sec- tion 3.1.1. After that, sentences shorter than 0.3 seconds or longer than 30 seconds are removed. Before data augmenta- tion, the training set contains 42.7 hours of patient data and 22.7 hours of control group data while the test data contains 20.1 and 10.1 hours. Details can be found in our code release. Dementia speech recognition and detection have been a popular research topic as well [37\u201342]. We use the Dementia- Bank Pitt corpus [20] to test the generalizability of our design. Similar to recent studies [37, 38], we use the ADReSS chal- lenge [43] test set, which is a subset of the DementiaBank Pitt corpus, for evaluation and the remaining data in the corpus for training and validation. We note that audio from the challenge test set has been enhanced with noise removal and volume nor- malization, while the transcripts have been preprocessed. To preserve a consistent data pipeline, we instead use the original recordings and transcripts from the Pitt corpus as our test data. Details can be found in our code base. Model Patient Control Overall WER WER WER Baselines Conformer E-Branchformer 40.3 36.2 35.3 31.2 38.1 34.0 Proposed Methods E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 26.4 26.3 26.2 26.3 26.3 26.3 26.3 17.0 16.9 16.9 16.8 16.9 16.9 16.9 22.2 22.2 22.1 22.1 22.1 22.2 22.1 Table 1: Word error rate (WER) of proposed methods evaluated on AphasiaBank. 3.2. Experimental Setups Baseline: We \ufb01rst build two ASR systems using Con- former [27] and E-Branchformer [18], as described in Sec- tion 2.3. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads. The E-Branchformer encoder has 12 blocks, each with 1024 hidden units, and 4 at- tention heads. The cgMLP module has 3072 units and the con- volution kernel size is 31. Both systems use a Transformer de- coderwith 6 blocks, each having 2048 hidden units and 4 atten- tion heads. The Conformer and E-Branchformer models have 44.2 and 45.7 million trainable parameters respectively. For the detection task, we reproduce the Aphasia detection experiment from a previous study. The detector is a support vector machine (SVM) that takes in linguistic features extracted from the oracle transcript to predict a binary classi\ufb01cation label [4]. Proposed Method: We \ufb01rst build a system with learned acous- tic representations extracted from WavLM [19] as the input to the E-Branchformer encoder. Using it as a foundation, we build tag-based and InterCTC-based detectors as described in Sec- tion 2.4. We also investigate the impact of tag insertion posi- tions: prepending, appending, and both. Meanwhile, we apply InterCTC to the 6th and the 9th encoder layer respectively, and analyze their performance difference. We set both the CTC and InterCTC weight to 0.3 and the inference beam size to 10. In all experiments, we use speed perturbation with ratios of 0.9 and 1.1, as well as SpecAugment [44], to augment the data. \u22123 and We choose the Adam optimizer with a learning rate of 10 \u22126. We employ warmuplr learning rate a weight decay of 10 scheduler with 2500 warm-up steps and a gradient clipping of 1. Each \ufb01nal model is selected by averaging the 10 checkpoints with the highest validation accuracy out of 40 epochs. More details can be found in our code base. 3.3. Results and Discussion Overall, the proposed systems achieve both accurate Aphasia speech recognition and detection at the same time. As shown in Table 1, switching from Conformer to E-Branchformer leads to a signi\ufb01cant ASR performance improvement by 4.1 WER ab- solute. Adding WavLM reduces the WER further by 11.8. This proves the effectiveness of using a state-of-the-art ASR encoder and SSLR for Aphasia speech recognition. Surprisingly, both types of detectors lead to a slightly better ASR performance than the vanilla ASR model (0.1 WER reduction). This implies that the ASR predictions can be re\ufb01ned based on Aphasia detec- tion results. We compare the ASR performance of our systems with previous work in detail in Table 2. Our systems obtained signi\ufb01cant lower WER for mild, moderate, and severe patients, Model Metric Patient Control Overall Overall Mild Moderate Severe Very severe PER DNN-HMM [6] PER DNN-HMM + MOE [45] Wav2vec2 (zero-shot) [4] WER BLSTM-RNN+i-Vector+LM [8] WER WER Wav2vec2 [5] 36.8 56.0 - - 47.4 33.1 - 33.7 23.6 52.8 41.6 - 41.1 36.8 61.0 49.2 36.4 62.9 75.8 63.2 59.1 - 37.5 - - - 47.1 - - E-Branchformer+WavLM +Tag-prepend +InterCTC-6 +InterCTC-6/Tag-prepend 26.3 26.3 26.3 16.9 16.9 16.9 34.5 34.7 34.8 32.8 32.6 32.9 22.3 22.3 22.1 Table 2: The recognition word error rate of proposed methods and existing work on the AphasiaBank English subset. The metrics are phoneme error rate (PER) and word error rate (WER). Note that existing studies use different data splits than ours. 72.5 71.7 73.3 22.2 22.1 22.1 WER"}