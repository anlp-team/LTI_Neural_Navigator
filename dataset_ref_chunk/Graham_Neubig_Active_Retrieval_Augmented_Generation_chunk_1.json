{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Active_Retrieval_Augmented_Generation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the proposed method discussed in the text?", "answer": " Forward-Looking Active REtrieval augmented generation (FLARE)", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the main issue with large language models (LMs) mentioned in the text?", "answer": " They have a tendency to hallucinate and create factually inaccurate output.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " How do retrieval augmented LMs commonly operate?", "answer": " They commonly use a retrieve-and-generate setup where they retrieve documents based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is one advantage of active retrieval augmented generation over standard retrieval augmented LMs?", "answer": " Active retrieval augmented generation allows for continually gathering information throughout the generation process, which is essential for more general scenarios involving long text generation.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the significance of FLARE in the context of the study?", "answer": " FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of the method.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What are some tasks that large language models (LMs) have been successful in?", "answer": " Tasks such as long-form QA, open-domain summarization, and chain-of-thought reasoning.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " Why is it important to augment LMs with retrieval components?", "answer": " To address hallucination and create factually accurate content by looking up relevant information from external knowledge resources.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What type of information needs are common in short-form knowledge-intensive generation tasks?", "answer": " The information needs are clear in the user\u2019s input, making it sufficient to retrieve relevant knowledge once based on the input.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What challenges do long-form generation tasks present compared to short-form generation tasks?", "answer": " Long-form generation presents complex information needs that are not always evident from the input alone, requiring continual gathering of multiple pieces of knowledge throughout the generation process.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the ultimate goal of Forward-Looking Active REtrieval augmented generation (FLARE)?", "answer": " The goal is to anticipate future content by iteratively retrieving relevant information based on the prediction of the upcoming sentence to regenerate sentences with low-confidence tokens.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}], "doc_text": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}