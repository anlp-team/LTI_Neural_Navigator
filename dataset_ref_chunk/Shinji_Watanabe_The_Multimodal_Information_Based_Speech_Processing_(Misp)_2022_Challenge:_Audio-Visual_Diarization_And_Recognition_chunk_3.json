{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_The_Multimodal_Information_Based_Speech_Processing_(Misp)_2022_Challenge:_Audio-Visual_Diarization_And_Recognition_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of Track 1 in the competition?", "answer": " To train the AVSD model using external audio data and video data.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What should participants do if they want to use additional video data in Track 1?", "answer": " Inform the organizers in advance about the data sources.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What does the baseline AVDR system consist of?", "answer": " An AVSD module followed by an AVSR module.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What type of data did the previous work use compared to the current challenge focusing on far-field audio and video?", "answer": " Mid-field audio and video.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What model is used to generate visual embeddings and an initial diarization result in the AVSD module?", "answer": " Visual voice activity detection (V-VAD) model.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What process is used to fuse the results of 6-channel audio in the AVSD module?", "answer": " DOVER-Lap.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What is used as the final evaluation index in the AVSD module?", "answer": " cpCER.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What is the training process for the AVSD module?", "answer": " Train the V-VAD model, then the audio network and decoder block, and finally train the whole network jointly.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What is the architecture of the AVSR module?", "answer": " Adopts a DNN-HMM hybrid system.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}, {"question": " What system is applied to train a GMM-HMM system on all far-field audio data in the AVSR module?", "answer": " Kaldi.", "ref_chunk": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}], "doc_text": "detection and speaker error, respectively, and TOTAL is the sum of durations of all reference speakers\u2019 speech. In Track 1, external audio data can be used to train the AVSD model, such as VoxCeleb 1, 2 [20, 21], CN-Celeb [22], and other public datasets. Additional video data is also allowed to be used. However, participants should inform the organizers in advance about such data sources, so that all competitors know about them and have an equal opportunity to use them. 2.3. Track 2: Audio-Visual Diarization and Recognition (1) 3. BASELINE AVDR SYSTEM Fig. 1 shows the baseline AVDR system, which consists of an AVSD module followed by an AVSR module. The AVSD module also serves as the baseline system for Track 1. In this section, we elab- orate the architecture and training process of the AVSD and AVSR modules, and provide the details about joining the AVSD and AVSR modules for decoding. 3.1. Architecture and Training of the AVSD Module We follow our previous work [24] as our baseline. The difference is that the preceding work used the data from the mid-\ufb01eld audio and video, while the current challenge focuses on the far-\ufb01eld audio and video. Track 2 moves beyond AVSD and also considers the task of speech recognition, i.e., transcribing the speech into its verbatim As shown in the AVSD module in Fig. 1, our system has three encoder modules. In the visual encoder module, lip ROIs are used as Authorized licensed use limited to: TU Delft Library. Downloaded on August 24,2023 at 10:38:09 UTC from IEEE Xplore. Restrictions apply. Text (2) Table 1. Speaker diarization results on Dev set (in %) Table 2. Diarization and recognition results on Dev set (in %) System FA MISS SPKERR DER System S D I cpCER ASD VSD AVSD 0.01 6.64 4.01 19.88 8.17 5.86 11.36 3.89 3.22 31.25 18.69 13.09 input of the network which consists of lipreading model [25], con- former blocks [26], and a BLSTM layer. The whole network can be regarded as a visual voice activity detection (V-VAD) model to gen- erate visual embeddings and an initial diarization result. Next, we use the audio dereverberated by NARA-WPE [27] and the diariza- tion result from the V-VAD model to compute i-vectors as speaker embeddings. Besides, through an FBank feature extractor and sev- eral 2D CNN layers, audio embeddings can also be extracted. In the decoder block, three types of embeddings are combined \ufb01rst and sev- eral BLSTM with projection (BLSTMP) layers are utilized to further extract features and get speech or non-speech probabilities for each speaker. In the post-processing stage, we \ufb01rst perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches as in [28]. Furthermore, DOVER-Lap [29] is used to fuse the results of 6-channels audio. ASR(OS) AVSR(OS) 40.84 35.78 27.33 27.82 0.51 0.36 68.68 63.96 ASD+ASR VSD+ASR VSD+AVSR AVSD+AVSR 31.83 39.25 35.17 35.94 44.34 31.22 31.01 29.45 4.27 0.66 0.61 0.68 80.44 71.13 66.79 66.07 and Tdur and crop the lip region of SPKi in every frame as the vi- sual input of the AVSR module. For the far-\ufb01eld 6-channel audio in Sessionk, we \ufb01rst perform WPE and BeamformIt for the raw 6- channel audio and segment the whole beamformed audio according to Tstart as audio input of the AVSR module. Finally, we j concatenate the decoded text of each utterance belonging to SPKi in Sessionk according to time order. j and Tdur j During the evaluation, due to the problem of permutation invari- ant training (PIT) and annotated segment text correspondence, we adopt cpCER as the \ufb01nal evaluation index. The training process is as follows: \ufb01rst, we use the parameters of the pre-trained lipreading and train the V-VAD model. Then, we freeze the visual network parameters and train the audio network and decoder block. Finally, we unfreeze the visual network parameters, and train the whole network jointly. 3.2. Architecture and Training of AVSR Module 4. RESULTS AND ANALYSIS In this section, we \ufb01rst introduce the experimental results of the base- line systems. Next, we point out the dif\ufb01culties of the MISP2022 challenge by providing examples, and analyze the good performance of AVDR system. Challenge participants can use this information to particularly focus on solving these issues in order to improve perfor- mance above the baseline. The AVSR model adopts a DNN-HMM hybrid system [18]. Firstly we apply the NARA-WPE [27] and BeamformIt [30] to the far- \ufb01eld 6-channel speech. Then, the FBank features extracted from the audio and the Lip ROIs cropped from the video were seg- mented on the basis of the speaker diarization results. The front-end module composed of 3D convolution and ResNet-18 is used to ex- tract lip-movement information for the video modality and outputs embeddingV. Meanwhile, the front-end module composed of 1D convolution and ResNet-18 is used to extract audio features and obtain embeddingA. The audio-visual features, embeddingAV, are extracted by the multi-stage temporal convolutional network (MS- TCN) [31] modules. Next, the posterior probabilities are obtained by the other MS-TCN modules. Finally, text is decoded from the posterior probabilities by using GMM-HMM, 3-gram model and DaCiDian. During the training stage, oracle speaker diarization results are used. Kaldi [32] is applied to train a GMM-HMM system on all far-\ufb01eld audio data. The training of the DNN-based acoustic model uses Cross Entropy loss and Adam optimizer for 100 epochs with initial learning rate of 0.0003 and cosine scheduler. More details of the experiment can be found in [18]. 3.3. Joint Decoding During inference, the RTTM \ufb01le as the output of the AVSD mod- ule contains the information of the Session, SPK, Tstart, and Tdur. This information can be used for calculating DER in Track 1 and preprocessing far-\ufb01eld video and far-\ufb01eld 6-channel audio in Track 2. For Sessionk, a set of utterance identi\ufb01er (SPKi, Tstart , Tdur ) j are available, where Sessionk and SPKi denote k-th session and i- th speaker in this session, Tstart denote the start time and the duration of the j-th utterance for SPKi."}