{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Learning_Performance-Improving_Code_Edits_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the three effective strategies for adapting LLMs for code optimization mentioned in the text?,        answer: The three effective strategies are retrieval-based prompting, performance-conditioning, and self-play.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " What is the average speedup achieved by the best model mentioned in the text?,        answer: The best model achieves an average speedup of 6.86\u00d7.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " How much of the test set did the best model optimize by at least 10%?,        answer: The best model optimizes 87.68% of the test set by at least 10%.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " What is Deep-PERF, and what does it use to generate performance improvement patches?,        answer: Deep-PERF is a model that uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " What does AlphaCode leverage language models for?,        answer: AlphaCode leverages language models to generate solutions to competitive programming problems in natural language.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " How is the dataset for adapting code LLMs constructed in the text?,        answer: The dataset is constructed based on performance-improving edits made by human programmers in a range of competitive programming tasks from CodeNet.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " How many pairs are obtained in the resulting dataset for training, validation, and test sets?,        answer: The training set has 77,967 pairs, the validation set has 2,544 pairs, and the test set has 982 pairs.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " How is correctness evaluated in the dataset construction process?,        answer: Correctness is evaluated through unit tests, and a program is rejected if a single test fails.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " What challenges are associated with benchmarking program performance?,        answer: Challenges include code instrumentation introducing overhead, substantial variance across executions, and the need for careful benchmarking to avoid over-reporting program optimization results.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}, {"question": " What simulator is used for measuring program performance in the text?,        answer: The text mentions using the gem5 full system detailed microarchitectural simulator of state-of-the-art processors.    ", "ref_chunk": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}], "doc_text": "like PIE. \u2022 We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting, performance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data obtained from self-play, achieves an average speedup of 6.86\u00d7, and optimizes 87.68% of the test set by at least 10%. Related work. Beyond the approaches described above, machine learning has been applied to improve performance by refactoring code (Mens & Tourw\u00e9, 2004; Agnihotri & Chug, 2020), identify compiler trans- formations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013; Huang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019), optimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff, 2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep- PERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement patches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each latent representation maps to a different category of code edits, and canonicalized code representations to au- tomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM from scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo- rithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate solutions to competitive programming problems in natural language, but it does not attempt to improve the performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b; Nijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization. 2 PERFORMANCE IMPROVING EDITS (PIE) DATASET We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y\u2217 = [y\u2217 For each trajectory Y\u2217, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)\u2212time(y>i)) > 10% where time (y) is the measured latency of program y (i.e., the relative time time(yi) improvement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to be inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to create these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment. We split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive programming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474 problems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems. For each pair in the test set, we also record the fastest human submission execution time for that problem; in Section 4, we include this running time as a comparison point. 1 , yu 1, y\u2217 2, . . . , y\u2217 n]. Test cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through unit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem. To improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a fine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5; 3 Preprint. Under review. after excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases per problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for additional details. Performance measurement using gem5. Benchmarking program performance is notoriously difficult. For instance, code instrumentation introduces overhead, and there is substantial variance across executions due to numerous factors, including server load and idiosyncrasies introduced by the operating system. If bench- marking is not performed carefully, it is easy to mistakenly over-report program optimization results. With enough samples and variance, benchmarking the same exact program can easily lead us to report significant optimizations. To illustrate the challenges, consider HYPERFINE Peter (2023), a Rust library designed to precisely bench- mark binaries. We benchmarked 500 programs \u201cpairs\u201d where the \u201cslow\u201d and \u201cfast\u201d programs are identical. Ideally, we should have source time target time = 1 (i.e., the two programs have identical performance). However, we observed a mean speedup of 1.12\u00d7, with a standard deviation of 0.36, and the top 5% of pairs exhibited a speedup of 1.91\u00d7. These results underscore the significant challenges in performance measurement. To address this challenge, we measure program performance using the gem5 (Binkert et al., 2011) full system detailed microarchitectural simulator of state-of-the-art processors. Executing deterministic pro- grams in gem5 provides fully deterministic performance results, enabling reliable isolation of the impact of performance-improving edits and reproducibility. We use the Verbatim configuration of the Intel Skylake architecture from gem5.2. An advantage of this approach is that our framework can be applied to other platforms like ARM or RISC-V without having access to hardware for those platforms. 3 ADAPTING CODE LLMS TO PROGRAM OPTIMIZATION 3.1 FEW-SHOT PROMPTING Instruction-prompting. We use prompts instructing the LLM to improve the performance of the given program, an approach commonly referred to as instruction prompting (Mishra et al.,"}