{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are three observations that can be made from the experiment results in Table 2?", "answer": " The three observations are that SAMA is generally more compute/memory efficient than the baselines, the cost of algorithmic adaptation for adaptive optimizers is marginal, and the efficiency gap widens as compute/memory is distributed across multiple GPUs.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " Why was the experiment conducted with up to 4 V100 GPUs only?", "answer": " The experiment was limited to up to 4 V100 GPUs due to the limited compute resources available.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " What is the purpose of reweighting samples from the continued pretraining task with meta learning?", "answer": " The purpose is to minimize negative transfer by identifying and exploiting relevant data while disregarding low-quality samples.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " How does SAMA-based data optimization compare to DAPT and TARTAN-MT in the experiment results in Table 3?", "answer": " SAMA-based data optimization leads to improvements in downstream performance on almost all datasets, unlike DAPT and TARTAN-MT.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " What method does the text propose to automatically meta-learn the importance weight of each training data for data pruning?", "answer": " The text proposes to use Meta-Weight-Net (MWN) with SAMA, distributed training, uncertainty of prediction, and training data at both base and meta levels.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " What are the four major modifications to Meta-Weight-Net (MWN) suggested in the text?", "answer": " The four major modifications are replacing the iterative differentiation meta gradient algorithm with SAMA, enabling distributed training, using uncertainty of prediction in addition to loss value, and using training data at both base and meta levels.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " What is the bilevel optimization formulation of the method proposed in the text for automatic meta-learning of data importance weight?", "answer": " The bilevel optimization formulation is L(Dtrain; \u03b8\u2217(\u03bb))\u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| \u2211(x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8).", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " How many epochs were run for meta learning with SAMA for ImageNet-1k and CIFAR-10 respectively?", "answer": " 30 epochs for ImageNet-1k and 50 epochs for CIFAR-10.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " What are the two types of datasets used for comparison in the experiment results in Figure 3 for GBML-based data pruning?", "answer": " The datasets compared are ImageNet-1k and CIFAR-10.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}, {"question": " What is the main advantage of GBML-based data pruning with SAMA over heuristics-based data pruning according to the text?", "answer": " GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also achieves improved memory/compute efficiencies.", "ref_chunk": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}], "doc_text": "from Wrench with a fixed global batch size of 48, and summarize the result in Table 2. Three observations that can be made here are that (1) SAMA is generally more compute/memory efficient than the base- lines, (2) the cost of algorithmic adaptation for adaptive optimizers is marginal as expected, and (3) the efficiency gap further widens as we distribute compute/memory across multiple GPUs with our efficient DDP communication strategy. Since we were only able to conduct experiments with up to 4 V100 GPUs due to the limited compute resources, exploring extremely large-scale GBML with larger GPU servers remains a promising future research direction. In Appendix F, we provide a more extensive ablation study for each component of SAMA, as well as compare test accuracy, GPU memory usage, and throughput of SAMA against various meta learning algorithms on IMDB and AGNews datasets from the Wrench benchmark. GPUs Memory Throughput Neumann CG SAMA-NA SAMA SAMA SAMA 26.0 28.4 13.7 14.3 10.4 7.4 1 1 1 1 2 4 82.9 82.1 144.1 142.0 241.2 396.7 Table 2: Memory and throughput analy- sis on AGNews with 4 V100 GPUs. 4.2 Continued Pretraining of Large Language Models DAPT/TAPT [24] empirically demonstrate that additional pretraining (i.e., continued pretraining) of the generic language model on the domain or task-specific data can further improve downstream performance on diverse benchmarks. However, the inclusion of low-quality samples for continual pertaining tasks can potentially hinder pretraining by amplifying negative interference [64], which could lead to suboptimal downstream performance. Here, we attempt to minimize such negative transfer by reweighting samples from the continued pretraining task with meta learning. To this end, we adopt the auxiliary learning technique from TARTAN [11] and simplify the two-stage pretraining- finetuning pipeline into a one-stage multitask learning pipeline with the reweighting scheme applied to the pretraining loss. The bilevel optimization formulation is as follows: \u03bb\u2217 = argmin Lf t(Df t; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 Lf t(Df t; \u03b8) + 1 |Dpt| (cid:88) x\u2208Dpt w(x; \u03bb) \u00b7 Lpt(x; \u03b8) 7 where Lf t/Lf t are finetuning/pretraining loss functions, Df t/Dpt are finetuning/pretraining datasets, and w(\u00b7; \u03bb) is the data reweighting network. Following the experiment setup in TARTAN [11], we use task-specific data and a masked language modeling loss in our auxiliary task and perform experiments with RoBERTa-base on 4 datasets from the original DAPT/TAPT paper. We compare our SAMA- based data optimization against DAPT and TARTAN-MT. We exclude TAPT and TARTAN-Meta respectively because (1) TAPT consistently underperforms TARTAN-MT [9] and (2) TARTAN-Meta uses additional validation data in the meta level of the downstream tasks, making the comparison unfair. We report our experiment results in Table 3. ChemProt HyperPartisan ACL-ARC SciERC Average Baseline DAPT [24] TARTAN-MT [11] SAMA (ours) 82.70 (0.45) 84.17 (0.50) 84.18 (0.30) 84.49 (0.13) 89.03 (2.25) 87.23 (3.65) 94.64 (0.91) 95.18 (0.03) 68.17 (2.52) 71.84 (4.78) 72.41 (1.94) 71.63 (1.68) 79.83 (0.89) 80.42 (1.57) 80.83 (0.71) 81.84 (0.08) 79.93 80.92 83.02 83.29 Table 3: Experiment results for auxiliary learning with the continued pretraining task. Following [24], we report test micro-F1 for ChemProt and macro-F1 for the other datasets. The number in parentheses indicates the standard deviation for each experiment over 3 runs. As shown above, SAMA-based data optimization leads to improvements in downstream performance on almost all datasets. This indirectly demonstrates that SAMA-based data reweighting can identify more/less relevant data in the auxiliary task and accordingly up-/down-weight them, unlike TARTAN- MT which allocates equal importance weights on all auxiliary data. Therefore, we expect that our method would likely benefit from additional auxiliary data by automatically figuring out and exploiting only relevant data, whereas TARTAN-MT is much more susceptible to negative transfer. While we only used task-specific data in our auxiliary task for the fair comparison with TARTAN-MT, extending auxiliary data to domain-specific or even general text data and comparing SAMA against DAPT or TARTAN-MT would be an intriguing future research direction. Finally, we analyze the GPU memory usage of different-sized RoBERTa in this experiment and present the result in Figure 1. The figure clearly shows the superior memory efficiency of SAMA with the increasing model size. 4.3 Scale-Agnostic Efficient Data Pruning Data pruning [47, 59, 61, 63] has recently received the limelight in the machine learning community as a means to both improve training efficiency and reduce (semantic) redundancy in training data. In particular, Sorscher et al. [59] showed both theoretically and experimentally that neural scaling laws can be beaten by data pruning. Nevertheless, they point out that the optimal data pruning metric varies across different dataset scales and further research in scalable data pruning metrics is needed. Here, we propose to forgo hand-designed data pruning metrics, and rather automatically meta-learn the importance weight of each training data following Meta-Weight-Net (MWN) [58] with four major modifications. First, we replace their iterative differentiation meta gradient algorithm with SAMA to achieve improved memory/compute efficiencies. Second, we further speed up meta learning by enabling distributed training with our efficient communication strategy. Third, we use the uncertainty of the prediction in addition to the loss value as an input to MWN to better estimate importance weight of each training data. Last, we use training data both in the base and the meta levels, assuming no additional validation data. A bilevel optimization formulation of our method is as follows: L(Dtrain; \u03b8\u2217(\u03bb)) \u03bb\u2217 = argmin \u03bb s.t. \u03b8\u2217(\u03bb) = argmin \u03b8 1 |Dtrain| (cid:88) (x,y)\u2208Dtrain w(L, U; \u03bb) \u00b7 L(x, y; \u03b8) where w(\u00b7; \u03bb) is MWN that takes the loss value L and the uncertainty U of the training sample (x, y) as an input and outputs the importance weight. Under this setup, we run meta learning with SAMA for 30 / 50 epochs respectively for ImageNet-1k / CIFAR-10 and obtain the pruning metrics by averaging the importance weights of the last 5 epochs. We compare our method to several popular static/dynamic data pruning baselines, and present the results in Figure 3. As expected, GBML-based data pruning with SAMA not only outperforms heuristics-based data pruning but also"}