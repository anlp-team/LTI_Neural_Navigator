{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Reproducing_Whisper-Style_Training_Using_An_Open-Source_Toolkit_And_Publicly_Available_Data_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the reason for the degradation in OWSM v3 on LibriSpeech and WSJ?", "answer": " The shift of data distributions from v2 to v3", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " How many languages does the v3 dataset contain compared to v2?", "answer": " 151 vs 23", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " What is the difference in model size between v3 and v2?", "answer": " 889M vs 712M", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " How long does it take to decode OWSM v3 for each 30-second utterance using greedy search?", "answer": " 2.3 seconds", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " What type of search is employed for English speech recognition in Table 2?", "answer": " Greedy search", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " Why is Whisper large not included in the comparison in Table 2?", "answer": " Because it is significantly larger than the other models", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " In which benchmarks do OWSM models outperform Whisper?", "answer": " LibriSpeech and Switchboard", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " For which language does OWSM v3 outperform Whisper by a large margin?", "answer": " Japanese", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " What trend is observed across different versions of OWSM in terms of ASR capability?", "answer": " The English ASR capability is largely improved from v1 to v2", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}, {"question": " How does OWSM v2 perform compared to v1 in all languages?", "answer": " Drastically improved", "ref_chunk": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}], "doc_text": "newly added (see Section 2.2). OWSM v3 has slight degradations on Lib- riSpeech and a large degradation on WSJ. This is probably due to the shift of data distributions from v2 to v3. As shown in Table 1, our v3 dataset contains significantly more languages compared to v2 (151 vs 23), but the model size is only slightly increased (889M vs 712M). Hence, the model has to adjust its capacity from English to other languages or from one type of speech to another type. This issue might be mitigated with larger models and more diverse data. We will explore it in the future. Please refer to the last paragraph in Section 2.5 for more discussions. We have also investigated the inference speed. Specifically, we select 50 utterances of 30 seconds from our prepared TEDLIUM dev set, and decode OWSM v3 with greedy search using a single NVIDIA A40 GPU. The average decoding time for each 30-second utterance is 2.3 seconds. 3.2. Multilingual speech recognition 3. EXPERIMENTS 3.1. English speech recognition Table 2 presents word error rates (WER) on standard English ASR benchmarks. Greedy search is employed without any external lan- guage models. To ensure fair comparison, we prepare all test data in ESPnet and evaluate Whisper in the same setup instead of report- ing results from their paper [15]. The text is normalized using the English or basic normalizer provided by Whisper. Whisper large is not included since it is significantly larger than the other models. Al- though many public ASR corpora are combined, our English training data is still significantly smaller than that of Whisper (73k vs 438k hours). However, our OWSM models achieve competitive results in most benchmarks. OWSM models even outperform Whisper on LibriSpeech and Switchboard. Table 3 shows the ASR results on multilingual benchmarks. In gen- eral, OpenAI Whisper achieves better performance than our OWSM, because Whisper employs significantly more training data in all lan- guages except Japanese. For Japanese, OWSM v3 outperforms Whisper by a large margin (CER: 11.3 vs 25.3) thanks to the larger amount of training data (19k vs 7k hours) from ReazonSpeech [42]. Notably, OWSM v2 achieves the best results on the English and Chinese test sets from Multilingual LibriSpeech and AISHELL, respectively, despite being trained on less data. The trend across different versions of OWSM is consistent with that in Section 3.1. OWSM v2 is drastically improved compared to v1 in all languages, which verifies the benefits of scaling up. OWSM v3 outperforms v2 in a few languages but achieves comparable or slightly worse results in the others. Again, this is likely because the model needs to adjust its capacity to support much more languages in v3. By comparing different versions of OWSM, we observe that its English ASR capability is largely improved from v1 to v2, demonstrating the effectiveness of scaling up in terms of the number of model parameters and the amount of training data. However, 3.3. Long-form speech recognition Similar to Whisper, OWSM performs long-form ASR by consecu- tively transcribing 30-second audio segments and shifting the win- Table 5: Examples of ASR on 30-second audio segments, generated by OWSM v2 using greedy search. Utterances can be segmented in different ways, but the predicted timestamps are usually accurate. Differences between the reference and prediction are marked in red. # Groundtruth from the dev set of MuST-C v2 Prediction by OWSM v2 1 <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.50><4.28> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.38><19.64> But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.<28.52> <en><asr><0.00> I\u2019m going to talk today about energy and cli- mate.<3.52><4.26> And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.<18.40><19.62> But energy and climate are extremely important to these people, in fact more important than to anyone else on the planet.<28.52> 2 <en><asr><0.00> Several years ago here at TED, Peter Skill- man introduced a design challenge called the marshmallow challenge.<5.60><5.80> And the idea\u2019s pretty simple: Teams of four have to build the tallest free-standing structure out of 20 sticks of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.52><16.52> The marshmallow has to be on top.<18.18><18.54> And, though it seems really simple, it\u2019s ac- tually pretty hard because it forces people to collaborate very quickly.<25.04><25.42> And so, I thought this was an interesting idea, and I incorporated it into a design workshop.<29.72> <en><asr><0.00> Several years ago here at TED, Peter Skillman introduced a design challenge called the Marshmellow Challenge, and the idea is pretty simple.<7.32><7.50> Teams of four have to build the tallest freestanding structure out of 26 of spaghetti, one yard of tape, one yard of string and a marshmallow.<16.50><16.54> The marsh- mallow has to be on top.<18.20><18.54> And though it seems really simple, it\u2019s actually pretty hard because it forces people to collaborate very quickly.<25.04><25.44> And so I thought this was an interest- ing idea, and I incorporated it into a design workshop.<30.00> Table 6: BLEU % (\u2191) of speech translation. OpenAI Whisper sup- ports any-to-English translation. OWSM can support more direc- tions. The sizes of training sets (in hours) are also provided. Table 8: WER/CER % (\u2193) of OWSM v3 using different decoding algorithms in ESPnet. Dataset Metric CTC Attention Joint CTC/attention Dataset Source MuST-C English CoVoST German Chinese Japanese Spanish French Target German Chinese Japanese Spanish French English OpenAI Whisper OWSM v2 OWSM v3 hours small medium hours result hours result NA 14k 14k 1.0k 0.5k 0.5k 28.5 20.5 10.5 23.4 28.5 14k 14k 1.0k 0.5k 0.5k 27.9 20.7 9.4 22.5 26.2 4.3k 26.2 6.3 12k 8.9k 15.9 6.7k"}