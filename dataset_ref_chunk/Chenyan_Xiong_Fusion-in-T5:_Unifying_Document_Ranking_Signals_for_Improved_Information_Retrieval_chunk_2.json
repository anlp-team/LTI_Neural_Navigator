{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Chenyan_Xiong_Fusion-in-T5:_Unifying_Document_Ranking_Signals_for_Improved_Information_Retrieval_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of FiT5?,answer: FiT5 performs re-ranking on a set of candidate documents retrieved by a first-stage retriever, incorporating ranking features and information from all the documents.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " What is the input format for FiT5?,answer: The input to FiT5 consists of a query, a document, and a ranking feature packed using a specific template.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " How is global attention introduced in FiT5?,answer: Global attention is introduced in the late layers of the encoder to incorporate information from all the documents in the set.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " What is the role of the [CLS] token in FiT5?,answer: The representation of the [CLS] token captures global information and is added back to the hidden representation.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " What labels are used in the MS MARCO and TREC DL datasets?,answer: MS MARCO labels are binary sparse labels, while TREC DL labels are dense judgments on a four-point scale.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " How is FiT5 compared to other models in the experiment?,answer: FiT5 is compared to models like BERT Re-ranker and monoT5 in terms of performance and efficiency.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " What is the input to the global attention layer in FiT5?,answer: The representations of the first tokens from all encoder layers are fed into the global attention layer.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " How is the final relevance score obtained in FiT5 during inference?,answer: The final relevance score is obtained from the normalized probability of the token true.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " When do global attention modules start being added in FiT5?,answer: Global attention modules are added starting from the third to last layer of the model.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}, {"question": " What is the baseline model compared to FiT5 in the experiment?,answer: The baseline models include typical two-stage retrieve-and-rerank pipelines like BERT Re-ranker and monoT5.", "ref_chunk": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}], "doc_text": "performs re-ranking on a set of candidate doc- uments D = {d1, d2, ..., dn} retrieved by a first- stage retriever, given a query q. Unlike typical re-ranking models which calculate si solely based on the query and one document text, denoted as si = f (q, di), FiT5 goes beyond the approach by further incorporating the ranking feature ri and the information from all the documents in D, which can be formulated as si = f (q, di, ri, D). Figure 1 presents the overall architecture of FiT5. FiT5 is based on the encoder-decoder model T5 (Raffel et al., 2020). It takes a triple of (q, di, ri) as the input and outputs a relevant score si. Global attention is introduced in the late layers of the en- coder to incorporate information from other doc- uments in D. We describe the input and output format in \u00a73.2 and the global attention in \u00a73.3. 3.2 Input and Output We pack (q, di, ri) using a template to form the input to FiT5. The template consists of slots for input data and several prompt tokens, defined as Query: [q] Title: [t] Feature: [f] Passage: [d] Relevant: (1) where, [q], [t] and [d] are slots for text features, corresponding to the query q, the title and the body of the document di, respectively. [f] is the slot for the feature ri (i.e. the retrieval score in this paper), represented as a normalized, discretized integer. The model is fine-tuned to decode the token \u201ctrue\u201d or \u201cfalse\u201d according to the input. During in- ference, the final relevance score is obtained from the normalized probability of the token \u201ctrue\u201d. 3.3 Global Attention In the document set D, there may exist many re- lated documents that may share similar content with the current example. The distinctions between Model # Params TREC DL\u201919 MRR@10 MAP NDCG@10 MRR NDCG@10 MRR MS MARCO TREC DL\u201920 First Stage Retrieval BM25 coCondenser (2022) Two-stage Ranking (coCondenser \u2192 *) 110M BERT Re-ranker (2019) 220M monoT5 (2020) FiT5 227M Three-stage Ranking (For Reference) 132M HLATR-base (2022) 342M HLATR-large (2022) 2\u00d73B Expando-Mono-Duo (2021) \u2013 \u2013 18.7 38.3 39.2 40.6 43.9 42.5 43.7 42.0 19.5 37.6 38.6 39.9 43.3 \u2013 \u2013 \u2013 50.58 71.45 70.12 72.55 77.63 \u2013 \u2013 \u2013 70.36 86.75 83.80 84.79 87.40 \u2013 \u2013 \u2013 47.96 67.97 69.23 67.73 75.24 \u2013 \u2013 78.37 65.85 84.41 82.26 85.05 85.48 \u2013 \u2013 87.98 Table 1: Overall results on MS MARCO and TREC DL 19 & 20. these documents may not be captured effectively via point-wise inference over the \u201clocal\u201d informa- tion (q, di, ri). To enhance the effectiveness of ranking, we propose global attention in FiT5 to enable the model to better comprehend and differ- entiate these documents in the ranking process. In FiT5, each (q, di, ri) pair first runs through l \u2212 1 transformer encoder layers independently, as in vanilla T5. Global attention is injected into every layer j \u2265 l. The representation of the first token [CLS] (prepended to the input), denoted as hj i,[CLS] \u2208 Rc, is picked out from the normal self- attention: i,[CLS], \u02c6Hj hj i = TF(Hj\u22121 i ) where \u02c6Hj i denotes the remaining part of the hidden representation, c is the hidden size and TF is the transformer layer. The representations of the first tokens from all n encoders are then fed into a global attention layer: 1,[CLS], ..., \u02c6hj \u02c6hj =Global_Attn(hj n,[CLS] 1,[CLS], ..., hj n,[CLS]) (2) (3) 2016) and evaluate it on the development set and TREC Deep Learning Tracks (TREC DL) 2019 & 2020 (Craswell et al., 2020, 2021). MS MARCO labels are binary sparse labels derived from click data with often one positive document per query. TREC DL labels are dense judgments on a four- point scale from irrelevant to perfectly relevant and thus are more comprehensive (Craswell et al., 2020, 2021). We report MRR@10, MAP and MS MARCO, and NDCG@10, MRR on TREC DL. Implementation Details We use T5-base model (Raffel et al., 2020) as the backbone of our model. Global attention modules are added starting from the third to last layer (i.e. l = 10). We re-rank the top 100 documents from coCondenser (Gao and Callan, 2022) ans use coCondenser retrieval score as ranking features in the template (Eq. 1). We first train a FiT5 without the features to warm-up the model for 400k steps, and then train it with features for 1.5k steps to obtain the final model. It is acceptable to incorporate more additional ranking features in a template to optimize the model. Finally, the globally-attended representation \u02c6hj i,[CLS] is added back to the hidden representation: Hj i = [hj i,[CLS] + \u02c6hj i,[CLS]; \u02c6Hj i ] In this way, the global information is modeled in the representation of the [CLS] token and can be further leveraged by the following layer(s). This provides a chance for the model to adjust the repre- sentation according to other relating documents. (4) Baselines We compare FiT5 with typical two- stage retrieve-and-rerank pipelines including BERT Re-ranker (Nogueira and Cho, 2019) and monoT5 (Nogueira et al., 2020). These re-rankers simply assign a score for a (q, di) text pair. To have a fair comparison, the first-stage retrieval for such pipelines is kept the same as FiT5. We also report the performance of three-stage ranking pipelines HLATR (Zhang et al., 2022) and Expando-Mono- Duo (Pradeep et al., 2021) for reference. 4 Experimental Methodology 5 Evaluation Results Datasets and Metrics We train FiT5 on MS MARCO passage ranking dataset (Nguyen et al., This section presents the overall results of FiT5, and analyses its effectiveness. Model monoT5 monoT5 (w/ feature) FiT5 (w/o feature) FiT5 (linear combination) FiT5 MARCO DL\u201919 DL\u201920 67.73 72.55 40.56 68.73 72.12 40.95 70.02 74.94 42.79 70.95 75.41 43.65 75.24 77.63 43.93 Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL. Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11)"}