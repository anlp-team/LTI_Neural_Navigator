{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_A_Pretrainer's_Guide_to_Training_Data:_Measuring_the_Effects_of_Data_Age,_Domain_Coverage,_Quality,_&_Toxicity_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the major models that use significant data pre-processing and toxicity/quality filters?", "answer": " BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " According to the text, what benefits are suggested by the widespread adoption of data pre-processing and toxicity/quality filters?", "answer": " Significant implicit benefits, even though they are not often externally reported", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " What kind of improvements does GLaM empirically report from filtering, particularly on Natural Language Generation tasks?", "answer": " Performance improvements", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " What caution do a few works in academia provide against the use of detoxification techniques?", "answer": " They caution against the use of data filters, which can reduce model perplexity on underrepresented communities", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " How do Meade et al. (2022) explain the correlation between improvements on bias benchmarks and deteriorations in general language modeling abilities?", "answer": " Improvements on bias benchmarks correlates with deteriorations in general language modeling abilities", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " What do works in the vision domain show about data filtering benefits and performance?", "answer": " Data filtering has important detoxification benefits but can reduce performance or introduce other biases", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " What are some of the proposed remedies for the effects of temporal degradation in modeling text?", "answer": " Proposed remedies include finetuning on more recent data, adaptive/continuous pretraining, data augmentation, and modeling text with its timestamps", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " What poses a challenge in the composition of public datasets like C4 and the Pile?", "answer": " The impending exhaustion of high-quality text data on the web to train compute-optimal larger LMs, at least with existing training efficiency", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " According to the text, what has a great deal of literature dedicated itself to when adapting static pretrained models to new downstream domains?", "answer": " Adapting static pretrained models to new downstream domains using various techniques such as domain adaptive pretraining, intermediate finetuning tasks, dynamically balancing data sources, data selection, augmentation, and active learning", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}, {"question": " What does recent work suggest about scaling model size and the amount of pretraining data in relation to new abilities?", "answer": " New abilities emerge at greater scale, but many of these benefits can be distilled or compressed into smaller models", "ref_chunk": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}], "doc_text": "importance of data distribution. Their summary corroborates our findings regarding domain composition and quality filtering, in particular. Data, Toxicity, & Quality Research into the quality and toxicity of datasets and their resulting models has seen mixed findings. All of the major models report using significant data pre-processing and toxicity/quality filters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest of these now using classifiers. This widespread adoption suggests there are significant implicit benefits, even though they not often externally reported. GLaM does empirically report performance improvements from filtering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). However, in academia, a few works caution against the use of detoxification techniques, including data filters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 2021). Welbl et al. (2021) also reports that a toxicity classifier reduces toxicity more than than applying data toxicity data filters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3\u2019s described quality filter, Gururangan et al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language\u2019s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- optimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. 18 Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered. 10 Conclusion The relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour. These effects can be reduced, but not eliminated, by finetuning. We recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile. For instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains. These countless choices are inherent in curating any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets. Acknowledgements We would like to thank Daniel Smilkov for his technical assistance in characterizing large corpora,"}