{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_KD-DLGAN:_Data_Limited_Image_Generation_via_Knowledge_Distillation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of matching the CLIP visual feature of real samples with the discriminator features of fake samples and vice versa in the text?", "answer": " The purpose is to lower the real-fake discriminability and make it harder for the discriminator to distinguish between real and fake samples.", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " How is the aggregated loss LAGG AGKD formulated?", "answer": " LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)|", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " What is the hyper-parameter p used for in the GAN training process?", "answer": " The hyper-parameter p controls the designed aggregated loss LAGG AGKD, where the loss is applied with probability p or skipped with probability 1-p.", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " How is the overall AGKD loss LAGKD formulated?", "answer": " LAGKD = LKD AGKD", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " What is the goal of Correlated Generative Knowledge Distillation (CGKD)?", "answer": " The goal is to improve the generation diversity by enforcing diverse correlations between generated images and texts in CLIP.", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " How are the correlations between generated images and CLIP text features built in CGKD?", "answer": " The correlations are built by calculating the inner products between CLIP visual features of generated images and CLIP text features.", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " What is the pairwise diversity loss LP D CGKD defined as?", "answer": " LP D CGKD = \u2211\u2211 Cos(CT[i, :], CT[j, :]), i=1,j=1,j \u2260 i", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " How is the correlation distillation from CT to CS defined in CGKD?", "answer": " The correlation distillation is defined as L1 loss between CT and CS: LKD CGKD = |CT \u2212 CS|", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " How is the overall CGKD loss LCGKD formulated?", "answer": " LCGKD = LP D CGKD + LKD CGKD", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}, {"question": " What is the overall training objective of the proposed KD-DLGAN?", "answer": " The overall training objective is to minimize LG (generator loss) and maximize LD (discriminator loss), where LD includes Ld, LAGKD, and LCGKD.", "ref_chunk": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}], "doc_text": "fake samples to challenge the discriminator learning. Speci\ufb01cally, we match the CLIP vi- sual feature of real samples I(x) and discriminator features of fake samples Df (G(z)), as well as CLIP visual feature of fake samples I(G(z)) and discriminator features of real samples Df (x) by the L1 loss. Such design lowers the real-fake discriminability and makes it harder to distinguish real-fake samples for the discriminator. The aggregated loss LAGG AGKD can be formulated by: LAGG AGKD = |I(x) \u2212 Df (G(z))| + |I(G(z)) \u2212 Df (x)| For effective GAN training, the designed aggregated loss LAGG AGKD is controlled by a hyper-parameter p, where the loss is applied with probability p or skipped with probability 1-p. We empirically set p at 0.7 in our trained networks. Thus, the aggregated loss L(cid:48)AGG AGKD can be re-formulated by: L(cid:48)AGG AGKD = (cid:26) LAGG 0, AGKD, if q \u2264 p, if q > p, where q is a random number sampled between 0 and 1. The overall AGKD loss LAGKD can be formulated by: AGKD + L(cid:48)AGG LAGKD = LKD AGKD 3.4. Correlated Generative Knowledge Distillation Correlated generative knowledge distillation (CGKD) aims to improve the generation diversity by two steps: 1) it enforces the diverse correlations between generated images and texts in CLIP with a pairwise diversity loss (LP D CGKD); 2) it distills the diverse correlations from CLIP to the GAN discriminator with a distillation loss (LKD CGKD). To achieve diverse image-text correlations, it \ufb01rst builds the correlations (indicated by inner products) between CLIP visual features of generated images I(G(z)) \u2208 RB\u00d7M and CLIP texts features T \u2208 RK\u00d7M . Here, B is the batch size (3) of generated images, K is the number of texts and M is the features dimension for each text feature or each image fea- ture. For conditional datasets and unconditional datasets, we employ the corresponding image labels as input texts and prede\ufb01ne a set of relevant text labels as input texts, respectively. Details of text selection for our datasets are introduced in the supplementary material. Thus, the cor- relation CT \u2208 RB\u00d7K between I(G(z)) and T (i.e., their L2-normalized inner products) can be de\ufb01ned as follows: CT = I(G(z)) \u00b7 T (cid:48) ||I(G(z)) \u00b7 T (cid:48)||2 , where T (cid:48) is the transpose of T . With the de\ufb01ned correlation, the diverse CLIP image- text correlations can be extracted in a pairwise manner. Speci\ufb01cally, for each image-text correlation CT [i, :] \u2208 RK, we diversify it with another image-text correlation CT [j, : ] \u2208 RK by minimizing the cosine similarity between them. Note [i,:] or [j,:] denotes the i-th or j-th row in CT and j (cid:54)= i. The pairwise diversity loss LP D CGKD can thus be de- \ufb01ned as the sum of the cosine similarity of all pairs: LP D CGKD = K (cid:88) K (cid:88) Cos(CT [i, :], CT [j, :]), i=1 j=1,j(cid:54)=i \u2212\u2192 b ) indicates the cosine similarity between where Cos(\u2212\u2192a , the two vectors \u2212\u2192a and \u2212\u2192 b . Then, the obtained diverse correlations are distilled from CLIP to the GAN discriminator, aiming to improve the generation diversity. We build the correlations CS \u2208 RB\u00d7K between discriminator features of generated samples Df (G(z)) \u2208 RB\u00d7M and CLIP text features T \u2208 RK\u00d7M as follows: CS = Df (G(z)) \u00b7 T (cid:48) ||Df (G(z)) \u00b7 T (cid:48)||2 The correlation distillation from CT to CS is de\ufb01ned by the L1 loss between them: LKD CGKD = |CT \u2212 CS| The overall CGKD loss LCGKD can thus be de\ufb01ned by: LCGKD = LP D CGKD + LKD CGKD 3.5. Overall Training Objective The overall training objective of the proposed KD- DLGAN can thus be formulated by: min G max D LG + LD where LG = Lg as introduced in Eq. 2 and LD = Ld + LAGKD + LCGKD as introduced in Eqs. 1, 3 and 4. (4) (5) Methods 100-shot AFHQ Obama Grumpy Cat Panda Cat Dog DA [53] + KD (CLIP [36]) 45.22 25.62 11.24 38.31 55.13 DA [53] (Baseline) + KD-DLGAN (Ours) 46.87 31.54 \u00b1 0.27 27.08 20.13 \u00b1 0.13 12.06 8.93 \u00b1 0.06 42.44 32.99 \u00b1 0.10 58.85 51.63 \u00b1 0.17 LeCam-GAN [40] + KD-DLGAN (Ours) 33.16 29.38 \u00b1 0.15 24.93 19.65 \u00b1 0.17 10.16 8.41 \u00b1 0.05 34.18 31.89 \u00b1 0.09 54.88 50.22 \u00b1 0.16 InsGen [44] + KD-DLGAN (Ours) 45.85 38.28 \u00b1 0.25 27.48 22.16 \u00b1 0.12 12.13 9.51 \u00b1 0.07 41.33 32.39 \u00b1 0.08 55.12 50.13 \u00b1 0.12 APA [19] + KD-DLGAN (Ours) 43.75 34.68 \u00b1 0.21 28.49 23.14 \u00b1 0.14 12.34 8.70 \u00b1 0.05 39.13 31.77 \u00b1 0.09 54.15 51.23 \u00b1 0.13 ADA [21] + KD-DLGAN (Ours) 45.69 31.78 \u00b1 0.22 26.62 19.76 \u00b1 0.11 12.90 8.85 \u00b1 0.05 40.77 32.81 \u00b1 0.10 56.83 51.12 \u00b1 0.15 Table 1. Comparison with the state-of-the-art over 100-shot and AFHQ: KD-DLGAN outperforms and complements the state-of-the-art data-limited image generation approaches consistently. In addition, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently. All the compared methods employ StyleGAN-v2 [22] as backbone. We report FID(\u2193) averaged over three runs. Panda DAObamaGrumpy CatKD-DLGAN Figure 3. Qualitative comparison with the state-of-the-art over 100-shot: Samples generated by KD-DLGAN are clearly more realistic than those generated by DA [53], the state-of-the-art data-limited generation approach. 4. Experiments 4.2. Experiments with StyleGAN-v2 In this section, we conduct extensive experiments to evaluate our KD-DLGAN. We \ufb01rst introduce the datasets and the evaluation metrics used in our experiments. We then benchmark KD-DLGAN with StyleGAN-v2 [22] and Big- GAN [5]. Moreover, we conduct extensive ablation studies and discussions to support our designs. Table 1 shows unconditional image generation results over 100-shot and AFHQ datasets, where we employ StyleGAN-v2 [22] as the backbone. Following the data settings in DA [53], the models are trained with 100 sam- ples (100-shot Obama, Grumpy Cat, Panda), 160 samples (AFHQ Cat) and 389 samples (AFHQ Dog), respectively. 4.1. Datasets and Evaluation Metrics We conduct experiments over the following datasets: CI- FAR [24], ImageNet [11], 100-shot [53] and AFHQ"}