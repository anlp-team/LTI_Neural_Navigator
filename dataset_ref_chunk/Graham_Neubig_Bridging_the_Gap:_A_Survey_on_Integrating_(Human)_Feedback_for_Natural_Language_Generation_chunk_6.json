{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Bridging_the_Gap:_A_Survey_on_Integrating_(Human)_Feedback_for_Natural_Language_Generation_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one benefit of using human feedback to improve model behavior?", "answer": " It allows the model to better understand user preferences.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " Why is directly using human feedback to improve model behavior not feasible in the general case?", "answer": " It is expensive and time-consuming to ask humans to provide feedback for every model output.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " What is the purpose of learning models of human feedback?", "answer": " To develop models that can predict or approximate human feedback.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " How is a feedback model trained to minimize the agreement loss?", "answer": " By minimizing the squared difference between the human feedback and model feedback.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " How did Stiennon et al. (2020) train preference models on ranking-based feedback?", "answer": " They used a log loss function based on ranking preferences.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " What approach did Zhang et al. (2019) and Zhou et al. (2023b) use in metric learning for NLP?", "answer": " They utilized pre-trained masked LMs to compute similarity scores between generated text/code snippets and references.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " How do reward models developed by Freitag et al. (2022b) correlate with human judgments?", "answer": " They correlate much better with human judgments than widely used lexical metrics such as BLEU and ROUGE.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " What is a key consideration in the initialization of feedback models according to Askell et al. (2021) and Bai et al. (2022a)?", "answer": " The size of the pretrained model.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " How can feedback models benefit from naturally occurring feedback data according to Askell et al. (2021)?", "answer": " It allows feedback models to be trained on much more data.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}, {"question": " How can a feedback model be utilized to improve generation?", "answer": " By leveraging it during the training of the generation model or incorporating it during the decoding process.", "ref_chunk": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}], "doc_text": "model to ad- just its output until it meets the user\u2019s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations. 5 Improving Generation using Human Feedback Models Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming. 5.1 Learning Models of Human Feedback An alternative approach to obtaining human feed- back is to develop models that can predict or ap- Preprint proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn \u2192 F, we want to learn a para- metric (numerical) feedback model \u02c6h\u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \u201cagrees\u201d with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss: Ex,y1,\u00b7\u00b7\u00b7 ,yn\u223cDf [L(\u03d5)] (cid:16)\u02c6h\u03d5(x, y1), \u00b7 \u00b7 \u00b7 , h(x, y1:n) \u03d5\u22c6 = arg min \u03d5 L(\u03d5) = loss (cid:17) For example, if the feedback function we are try- ing to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = (cid:16)\u02c6h\u03d5(x, y) \u2212 h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 \u02c6h\u03d5(x, yn) on ranking-based feedback, using a loss of the form (cid:17)2 . L(\u03d5) = log (cid:16) \u03c3 (cid:16)\u02c6h\u03d5(x, y+1) \u2212 \u02c6h\u03d5(x, y\u22121) (cid:17)(cid:17) (10) such that sample y+1 was preferred to y\u22121 for the same input x: h(x, y\u22121, y+1) = (y\u22121 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023). The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary- 3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively. 8 (8) (9) level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2. Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical). Preprint 5.2 Leveraging Feedback Models to Improve Generation After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process. 5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible"}