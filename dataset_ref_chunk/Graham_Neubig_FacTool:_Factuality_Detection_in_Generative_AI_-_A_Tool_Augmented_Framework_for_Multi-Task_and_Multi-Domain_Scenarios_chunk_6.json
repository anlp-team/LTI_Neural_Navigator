{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_FacTool:_Factuality_Detection_in_Generative_AI_-_A_Tool_Augmented_Framework_for_Multi-Task_and_Multi-Domain_Scenarios_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the 4 metrics used to measure the similarity between machine-extracted claims and golden-extracted claims?,answer: ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " In Table 4, what is reported as the average of the highest similarity?,answer: The average of the highest similarity between each ChatGPT-extracted claim and the corresponding golden-extracted claim", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " Why was ChatGPT chosen as the claim extractor in Exp-II?,answer: Because the context length of Flan-T5 is too short to effectively extract claims from lengthy responses, and ChatGPT is more cost-efficient compared to GPT-4 while maintaining similar effectiveness in claim extraction", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " In Exp-II, what two models are the FACTOOL baselines powered by?,answer: FACTOOL baselines are powered by ChatGPT and GPT-4", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " According to the experimental results, which baseline outperforms all others across all scenarios?,answer: FACTOOL powered by GPT-4", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " What claim-level F1 score does FACTOOL powered by GPT-4 achieve on math problems?,answer: 98.97", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " How does FACTOOL powered by GPT-4 compare to FACTOOL powered by ChatGPT across all scenarios?,answer: FACTOOL with GPT-4 outperforms FACTOOL with ChatGPT", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " Which search engine is shown to be highly robust in finding citations compared to LLM itself?,answer: Google Scholar", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " In which scenario is query generation and agreement verification harder for ChatGPT compared to GPT-4?,answer: KB-QA", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}, {"question": " What response-level F1 score does FACTOOL powered by GPT-4 achieve on scientific literature review?,answer: 94.74", "ref_chunk": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}], "doc_text": "on RoSE (Liu et al., 2022). We treat the reference summary as the generated text x, and the reference ACUs as the golden-extracted claims. We measure the similarity between the machine-extracted (GPT-4, ChatGPT, and Flan-T5 XXL) claims {cc i }i=1\u00b7\u00b7\u00b7nc and golden- extracted claims {cg i }i=1\u00b7\u00b7\u00b7ng using 4 metrics: ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004), and BERTScore. In Tab. 4, we report the av- erage of the highest similarity between each 8We anticipate that the recently released models, gpt-3.5- turbo-0613 and gpt-4-0613, will lower the inference costs for FACTOOL. This expectation arises from their improved ability to produce structured responses, such as those in JSON format. While conducting our experiments on gpt-3.5-turbo-0301 and gpt-4-0314, we often ran into problems where the responses were not valid JSON, requiring us to rerun any samples with invalid response formats. The source code of FACTOOL will be using the latest versions of ChatGPT and GPT-4. Model Metric Precision Recall F1-score GPT-4 ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7394 0.6304 0.7175 0.6632 0.8758 0.7771 0.8625 0.7865 0.7860 0.6772 0.7667 0.7175 ChatGPT ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.7770 0.6520 0.7557 0.6958 0.8285 0.7115 0.8148 0.7521 0.7836 0.6610 0.7655 0.7174 FLAN-T5-XXL ROUGE-1 ROUGE-2 ROUGE-L BERTScore 0.6531 0.5609 0.6428 0.4314 0.8928 0.8157 0.8885 0.6661 0.7326 0.6413 0.7237 0.5408 Table 4: The average similarity between the extracted claims from GPT-4, ChatGPT, and Flan-T5 XXL and the golden ACUs on RoSE. ChatGPT-extracted claim and the corresponding golden-extracted claim in the same sample. (i.e., j ))). (cid:80) i=1 maxng (cid:80)nc i , cg 1 nc j=1(Sim(cc 1 sample_cnt sample Results We demonstrate in Tab. 4 that the claims extracted by GPT-4, ChatGPT, and Flan-T5 closely match the ACUs annotated by humans, as evaluated by ROUGE and BERTScore metrics. Note that in Exp-II, we choose ChatGPT as the claim extractor for two reasons: (1) The context length of Flan- T5 is too short (512 tokens) to effectively extract claims from lengthy responses in our dataset. (2) ChatGPT is more cost-efficient compared to GPT- 4, while maintaining similar effectiveness in claim extraction. 6.2 Exp-II: Framework Evaluation We evaluate FACTOOL and the two Self-Check baselines on the dataset constructed from each sce- nario. Depending on the model used for query gen- eration and agreement verification, we have two FACTOOL baselines: FACTOOL powered by Chat- GPT and FACTOOL powered by GPT-4. We report the accuracy, recall, precision, and F1-score at both the claim and response levels. 6.2.1 Result Tab. 5 shows the claim-level and response-level performance of FACTOOL and the self-check base- lines. We obtain following observations. FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios From Tab. 5, we observe that FACTOOL powered by GPT-4 outperforms all other baselines across all scenarios. FACTOOL powered by GPT-4 achieves an 89.09 claim-level F1 / 71.79 response-level F1 on KB-based QA, a 92.11 claim-level F1 / 92.11 response-level F1 on code generation (remember Claim-Level Response-Level Tasks LLMs Methods Acc. R P F1 Acc. R P F1 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 75.54 69.53 74.25 90.40 81.36 73.45 80.00 79.12 90.91 84.88 80.23 81.25 54.00 54.00 64.00 60.87 47.83 43.48 50.00 50.00 66.67 54.90 48.89 52.63 KB-QA GPT-4 Self-Check (0) Self-Check (3) FACTOOL 77.25 79.83 84.12 84.75 85.88 85.31 85.23 87.36 93.21 84.99 86.61 89.09 54.00 64.00 78.00 95.65 52.17 60.87 50.00 63.16 87.50 65.67 57.14 71.79 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 68.29 68.90 78.05 99.10 100.00 89.19 68.33 68.52 80.49 80.88 81.32 84.62 Code GPT-4 Self-Check (0) Self-Check (3) FACTOOL 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 75.31 77.44 89.02 95.50 96.40 94.59 75.18 76.43 89.74 84.13 85.26 92.11 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 84.15 87.32 97.54 90.24 94.31 97.56 91.36 91.34 99.59 90.80 92.80 98.56 57.00 61.00 78.00 74.47 89.36 93.62 53.03 55.26 69.84 61.95 68.29 80.00 Math GPT-4 Self-Check (0) Self-Check (3) FACTOOL 83.10 92.61 98.24 86.99 96.75 97.97 93.04 94.82 100.00 89.92 95.77 98.97 49.00 65.00 78.00 85.11 89.36 95.74 47.62 58.33 69.23 61.07 70.59 80.36 ChatGPT Self-Check (0) Self-Check (3) FACTOOL 28.69 24.19 97.31 96.00 96.97 84.85 21.82 18.60 100.00 35.56 31.22 91.80 18.00 22.00 99.00 100.00 90.00 90.00 10.87 10.47 100.00 19.61 18.75 94.74 Scientific GPT-4 Self-Check (0) Self-Check (3) FACTOOL 35.75 44.75 98.39 84.85 87.88 90.91 20.29 23.20 100.00 32.75 36.71 95.24 19.00 49.00 99.00 100.00 70.00 90.00 10.99 12.73 100.00 19.80 21.54 94.74 Table 5: Experimental results of FACTOOL powered by ChatGPT and FACTOOL powered by GPT-4 on KB-based QA, Code Generation, Math Problems, and Scientific Literature Review. that claim-level factuality is considered equivalent to response-level factuality in our experiment for code generation), a 98.97 claim-level F1 / 80.36 response-level F1 on math problems, and a 95.24 claim-level F1 / 94.74 response-level F1 on scien- tific literature review. Each of these figures is the highest for their respective tasks. view. On FACTOOL powered by GPT-4 v.s. Self- Check (3) powered by GPT-4, we observe: 95.24 v.s. 36.71 claim-level F1 and 94.74 v.s. 21.54 response-level F1. Here, Google Scholar shown to be highly robust in performing its specified task of finding citations when compared to LLM itself. FACTOOL powered by GPT-4 outperforms all self-check baselines across all scenarios From Tab. 5, we show that FACTOOL with GPT-4 outper- forms all self-check baselines across all scenarios. On FACTOOL powered by GPT-4 v.s. Self-Check (3) powered by GPT-4, we observe: 71.79 v.s. 57.14 response-level F1 on KB-based QA, 92.11 v.s. 85.26 response-level F1 on code generation, 80.36 v.s. 70.59 response-level F1 on math prob- lems, and 94.74 v.s. 21.54 response-level F1 on scientific literature review. FACTOOL powered by GPT-4 outperforms FAC- TOOL powered by ChatGPT FACTOOL pow- ered by GPT-4 outperforms FACTOOL powered by ChatGPT across all scenarios. This trend is especially significant in KB-QA, where query gen- eration and agreement verification are harder for ChatGPT but relatively easier for GPT-4 (89.09 v.s 81.25 claim-level F1 and 71.79 v.s 52.63 response- level F1). On the other hand, in scenarios where query generation and agreement"}