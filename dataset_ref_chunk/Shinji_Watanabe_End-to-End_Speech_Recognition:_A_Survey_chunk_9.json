{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the benefit of combining RNN-T and AED in automatic speech recognition (ASR)?", "answer": " The authors find that combing RNN-T and AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " What are some streaming 1st-pass and attention-based 2nd-pass rescoring techniques mentioned in the text?", "answer": " Deliberation, cascaded encoder, Y-architecture, and Universal ASR are mentioned as different techniques.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " What is a prominent feature of E2E systems discussed in the text?", "answer": " Character-level modeling is discussed as a prominent feature, as it avoids phoneme-based modeling and pronunciation lexica.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " Why are traditional HMM architectures mentioned in the context of character-level modeling?", "answer": " It is mentioned that character-level modeling is viable with classical hybrid HMM architectures and can work even without neural networks.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " What architectural improvements to basic E2E models are discussed in the text?", "answer": " Various algorithmic changes, ways to combine different E2E models, ways to incorporate context, and improved encoder and decoder architectures are discussed.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " How do Watanabe et al. propose to improve performance in ASR models?", "answer": " They propose employing a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention-based models on specific tasks.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " What is contextual biasing, and why is it important in E2E ASR systems?", "answer": " Contextual biasing pertains to biasing phrases like proper nouns in a specific domain, which is important for recognizing words seen infrequently during training and improving beam search performance.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " How have encoder architectures of E2E models evolved over time?", "answer": " The evolution includes moving from long short-term memory recurrent neural networks (LSTMs) to architectures like Transformers and Conformers, which have shown competitive performance.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " What is the training criterion mentioned in the text for end-to-end ASR models?", "answer": " The training criterion to be minimized is the negative log probability of the correct transcription given the input.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}, {"question": " How have architectures like ContextNet, Transformer, and Conformer improved ASR models?", "answer": " These architectures have shown better performance compared to LSTMs by leveraging convolution, self-attention, or combining self-attention with convolution.", "ref_chunk": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}], "doc_text": "needed to predict the current word, e.g., \"one dollar and fifty cents\" \u2192 $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17\u201322% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR \u2013 e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91]. Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84]. IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer. A. Combinations of Models Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC \u2013 which have left-to-right constraints during decoding \u2013 perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5\u201314% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context B. Incorporating Context Contextual biasing to a specific domain, including a user\u2019s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context. One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network). C. Encoder and Decoder Structure There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103]. and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = \u2212 (cid:80)N n=1 log P (Cn|Xn)"}