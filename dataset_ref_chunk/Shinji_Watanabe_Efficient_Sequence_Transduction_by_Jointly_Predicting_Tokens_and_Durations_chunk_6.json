{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Efficient_Sequence_Transduction_by_Jointly_Predicting_Tokens_and_Durations_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the issue with batched inference for TDT models?", "answer": " The issue is that utterances in the batch may have different duration outputs, making it difficult to fully parallelize the computation.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " How can one make the whole batch skip the same number of frames in batched inference for TDT models?", "answer": " By selecting the minimum of predicted durations for all utterances in the batch.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " Why did the method of advancing the whole batch by the minimum predicted duration result in increased insertion errors?", "answer": " Because skipping fewer frames than the model indicates causes the model to emit previously emitted tokens instead of the next token.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " How did the authors propose to solve the issue of increased insertion errors in TDT models?", "answer": " They proposed a modification to the training loss by combining TDT loss with conventional transducer loss with a sampling probability.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " What was compared in the noise robustness analysis between TDT and RNNT ASR models?", "answer": " The noise robustness for TDT and RNNT ASR models was compared by running inference on Librispeech test-clean augmented with noise at different signal-noise-ratios (SNRs).", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " How did TDT models perform compared to conventional transducers in noisy conditions?", "answer": " TDT models performed much better in noisy conditions than conventional transducers, both in terms of accuracy and speed.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " What is the sampled loss proposed by the authors?", "answer": " The sampled loss is a combination of TDT loss with conventional transducer loss with a sampling probability \u03c9.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " What is the inference speed-up seen with batched inference for TDT models?", "answer": " The inference speed-up for TDT models with batched inference is slightly smaller than for the non-batched case.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " What issue does the TDT model help avoid in cases of repeated tokens?", "answer": " TDT models help avoid the issue of performance degradation that RNN-T models face when there are repetitions of the same tokens in the text sequence.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}, {"question": " How did TDT models perform in comparison to conventional RNN-T models in experiments with random repeated digits?", "answer": " TDT models achieved significantly lower error rates compared to conventional RNN-T models, regardless of the type of decoder used.", "ref_chunk": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}], "doc_text": "with batched inference for TDT models is that utterances in the batch may have different duration outputs that denote the number of frames that should be skipped. As a result, it is difficult to fully parallelize the computation for the same batch. One can make the whole batch skip the same number of frames, by selecting the minimum of predicted durations, e.g. if batch-size=4, and predicted durations are {3, 4, 3, 6}, we advance the whole batch by 3 frames. However, we found this method results in significantly increased insertion errors with the same tokens repeated multiple times. This is not surprising since we skip fewer frames than the model indicates, and the model would not be ready to emit the next token, but can only emit previously emitted tokens instead. To solve this issue, we propose a modification to the training loss of our models, by combining TDT loss with conventional transducer loss LTDT with a sampling probability \u03c9: In this section, we compare the noise robustness for TDT and RNNT ASR models. For this, we run inference on Librispeech test-clean augmented with noise in different signal-noise-ratios (SNRs). For each utterance, we ran- domly select a noise sample from MUSAN (Snyder et al., 2015) and Freesound 18. The noise sample is sub-segmented if it\u2019s longer than the utterance, or repeated if it\u2019s shorter than the utterance. The utterance samples are augmented with noise samples in 0, 5, 10, 15, and 20 SNRs. We report the WER and inference time of conventional Transducers and TDT models with configuration 0-8. We found TDT models perform much better in noisy conditions than con- ventional Transducers, both in terms of accuracy and speed (Fig. 6). While RNN-T and TDT models achieve similar WERs for clean speech, TDT models gradually outperform RNNT as more noise is added. The inference time for TDT is practically the same for all SNRs. More details of those experiments are in Appendix E. Lsampled = (cid:40) LTransducer, with probability \u03c9 LTDT, with probability 1 \u2212 \u03c9 (18) Note, the conventional transducer loss LTransducer is com- puted on the token logits only, and the duration logits will not take part in the computation nor get updated. We found that the sampled loss solves the aforementioned perfor- mance degradation issue. Table 7 shows the ASR perfor- mance and inference speed of TDT models when training with \u03c9 = 0.1, and running inference with batch=4. We even see slightly improved ASR accuracy, as well as inference speed-up with batched inference with TDT. 17 17The speed-up for batched inference is slightly smaller than for non-batched case because 1. the overhead related to padding for batched computation and 2. all utterances in the batch advance by the minimum of predicted durations which increases the number of decoding steps. Figure 6. TDT vs RNNT ASR on noisy speech. WER(%) for Librispeech test-clean with noise added at different SNRs. WER on the original test-clean is shown at SNR = +inf. While TDT and RNNT achieve similar WER at low noise conditions, TDT is more robust to noise. 18https://freesound.org/ 8 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations model WER% model max-duration WER time rel. speed up RNNT-LSTM RNNT-stateless TDT [0-2] TDT [0-4] TDT [0-6] TDT [0-8] 59.95 64.62 12.59 9.35 6.12 5.78 RNNT MBT TDT MBT TDT 2 2 4 4 5.11 5.15 5.50 5.05 5.06 244 208 171 161 128 1.17X 1.43X 1.52X 1.91X Table 8. WERs with different Transducer models on TTS gener- ated dataset with repeated digits. MBT TDT 8 8 5.18 5.16 139 115 1.76X 2.12X 5.4. TDT Robustness with respect to repeated tokens We notice that RNN-T model performance significantly degrades when the text sequence has repetitions of the same (subword) tokens, for example: Table 9. Inference and accuracy comparison between 3 type of ASR models: RNNT, multi-blank RNNT (MBT), and TDT. Greedy WER (%) and the total decoding time of the Librispeech test-other with batch = 1. Relative speed-up is measured against the RNNT. For MBT max-duration=4 means MBT model with big-blank- durations=[2,3,4] in addition to the conventional blank. For TDT models, max-duration=4 means model with durations [0,1,2,3,4]. Ground truth: seven seven seven nine nine nine eight eight eight RNNT w/ LSTM decoder result: seven seven eight eight and TDT models give comparable WERs, larger inference speedup factors are seen with TDT models when using the same max-duration configs. RNNT w/ stateless decoder result: seven nine eight 6. Conclusion We find TDT models are significantly more robust than RNN-Ts for such cases. We use NeMo TTS to generate 100 audios containing random digits repeating 3 - 5 times and run ASR with different models with results in Table 8. We see that while the conventional RNN-Ts achieve very bad word error rates (all of them with error rates more than 50%, regardless of the type of decoder used), TDT models are able to achieve significantly lower error rates, and this effect is more prominent for TDT models with longer durations. With TDT models with durations 0-8, we achieve a word error rate of 5.78%, which is more than 10X error rate reduction compared to conventional Transducers. This set of experiments also shows that TDT models do not suffer from the potential issue of accumulation of errors induced by consecutive duration predictions that might be inaccurate. More details on the analysis of repeated tokens and our experiments can be found in Appendix F. 5.5. TDT Comparison with Multi-blank Transducers In this paper, we propose Token-and-Duration Transducers, which extend conventional Transducer models by adding explicit duration modeling. We present detailed deriva- tions of the extended forward-backward algorithm used for TDT models, as well as the close-form solutions for TDT model training. We show that TDT models are superior to conventional Transducers across multiple se- quence tasks, including speech recognition, speech trans- lation, and spoken language understanding. In all those tasks, we see better or similar performances with TDT models than conventional Transducers, while TDT mod- els run inference significantly faster, with"}