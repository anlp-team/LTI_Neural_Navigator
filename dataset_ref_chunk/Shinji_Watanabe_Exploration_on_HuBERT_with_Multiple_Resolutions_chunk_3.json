{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploration_on_HuBERT_with_Multiple_Resolutions_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What tasks were evaluated in the SUPERB benchmark?", "answer": " Understanding tasks, speaker-related tasks, and frontend processing tasks.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What toolkit was used for ASR fine-tuning evaluation?", "answer": " fairseq toolkit.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What optimizer was used for training all models?", "answer": " AdamW optimizer.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What was used as the downstream module for HuBERT-MR models?", "answer": " Linear projection for CTC loss computation.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " Which task showed the lowest weight contribution for 100ms HuBERT?", "answer": " Speech enhancement (SE).", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What evaluation metric was used for phone recognition (PR) task?", "answer": " Phone error rate (PER).", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What were the key observations when comparing HuBERT-MR-P and HuBERT-MR-H with base HuBERT models?", "answer": " Both HuBERT-MR-P and HuBERT-MR-H outperformed the base HuBERT model trained with different resolutions.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What beam size was used for decoding with Flashlight for wav2letter decoding?", "answer": " Beam size of 500.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What are the evaluation metrics used for ASR fine-tuning?", "answer": " Word error rate (WER).", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}, {"question": " What was highlighted regarding the performance of HuBERT-MR-H in comparison to other models?", "answer": " HuBERT-MR-H outperformed both the base versions of HuBERT and wav2vec2 models.", "ref_chunk": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}], "doc_text": "method as the upsampling function UP, as described in Sec. 2.1. HuBERT-MR-P HuBERT-MR-P HuBERT-MR-P (100,20) (40,20) (100,40,20) HuBERT-MR-H HuBERT-MR-H HuBERT-MR-H (100,40,20) (100,20) (40,20) 6.99( 3.70) 7.13( 3.75) 6.53( 3.61) 6.59( 3.59) 7.01( 3.71) 6.11( 3.31) We evaluate most of the SUPERB tasks, including un- derstanding tasks (phone recognition (PR), automatic speech recognition (ASR), intent classification (IC), and speech trans- lation (ST)), speaker-related tasks (speaker identification (SID), speaker verification (SV), and speaker diarization (SD)), and frontend processing (speech enhancement (SE)). To better understand the results, we also show the performances on wav2vec2-base for comparison [2, 29]. ASR Fine-tuning: We evaluate the ASR fine-tuning task on the Librispeech 100-hour train set with dev-clean and test-clean for development and testing, respectively. The training uti- lizes fairseq toolkit [30]. For all models, we train 100k steps with a maximum token number of 1M to form mini-batches. The training uses an AdamW optimizer with 8k warmup steps and a learning rate of 2e-5. We compared HuBERT-MR- P and HuBERT-MR-H with base HuBERT models, as well as wav2vec2 models and HuBERT-large. Instead of repeat- ing, we use transposed convolution for the UP function of HuBERT-MR-P. For simplification, we use a linear projection for CTC loss computation as the downstream module needed for HuBERT-MR. 3.3. Experimental Results Table 2 presents the experimental results of the SUPERB bench- In most tasks, HuBERT-MR-P showed significant im- mark. provements over the original HuBERT, with the exception of ER and SE. We also analyzed the weight contribution of HuBERT with different representations of HuBERT-MR-P on ASR, SV, ST, and the average overall tasks, which are presented in Table 3. Among the tasks, SE has the lowest weight for 100ms HuBERT (0.15), while ER has the highest weight for 100ms HuBERT (0.26). The results indicate that HuBERT fea- tures from multiple resolutions provide additional benefits and can significantly contribute to various types of tasks. The experimental results of ASR fine-tuning are presented in Tables 4 and 5. Table 4 compares the performance of HuBERT-MR with the original HuBERT models. The follow- ing observations can be found: \u2022 Both HuBERT-MR-P and HuBERT-MR-H outperform the base HuBERT model trained with resolutions of 100ms, 40ms, and 20ms. We present not only the Viterbi decoding results but also the results after language model rescoring. For decoding, we used Flashlight for wav2letter decoding [31] and applied a beam size of 500 with a beam threshold of 100 and a language model weight of 2 for language model rescoring. The language model used was trained on the 4-gram language model training corpus of Librispeech [28]. Although the HuBERT model trained with resolutions of 100ms and 40ms does not achieve similar performance to the one trained with 20ms, their features appear to be comple- mentary to each other, resulting in improved performance for all HuBERT-MR models. We observe that the 100ms-based HuBERT model does not perform well in the task, likely due to the feature sequence being too short for effective CTC- based training. Evaluation metrics We generally follow the evaluation met- rics for SUPERB tasks. We use phone error rate (PER) for PR; word error rate (WER) for ASR; accuracy for ER, IC, and SID; diarization error rate (DER) for SD; equal error rate (EER) for SV; PESQ for SE; BLEU for ST. While for ASR fine-tuning, we use WER. Meanwhile, for efficiency concerns, we also report Floating Point Operations Per Second (FLOPs) and Multiply- Accumulate Operations (MACs) to models. The calculation procedure follows the SUPERB challenge [5]. In Table 5, we compare the performance of HuBERT-MR-H to that of the HuBERT-large and wav2vec2 models. Our findings are as follows: \u2022 HuBERT-MR-H outperforms both the base versions of Hu- BERT and wav2vec2, highlighting the superior performance of this method. Although HuBERT-MR-H is a significant improvement over the base HuBERT model, there is still some performance gap when compared to HuBERT-large. This difference could be Table 5: Fine-tuning results on Librispeech-100h (comparison with other models). Results with language model rescoring are in brackets. * indicates the large model setting. The unlabeled column shows the number of hours used for SSL pre-training. HuBERT- MR-H is discussed in Sec 2.2. Noted that the HuBERT base model with 20ms is our baseline. Model Res.(ms) Unlabeled(h) Param.(M) MACs(G) FLOPs(T) WER(\u2193) HuBERT wav2vec2 wav2vec2* HuBERT* 20 20 20 20 960 960 60K 60K 94.7 95.0 317.4 316.6 1669 1669 4326 4324 3.34 3.34 8.66 8.66 7.73(3.81) 6.54(4.33) 5.90(3.45) 5.40(2.82) HuBERT-MR-H (100,40,20) 960 298.4 3454 6.91 6.11(3.31) (a) 20ms (b) 40ms (c) 100ms Figure 2: Speech re-synthesis using features from HuBERT base models at different resolutions. The high-resolution features capture better envelope information in the time domain (shown in the blue box), while the low-resolution features provide more detailed information in the frequency domain (shown in the green box). See Sec. 3.4 for experimental details and discussion. due to the limited training data (960 Librispeech training sets versus 60K Librilight [32]) and fewer pre-training iterations (all three base HuBERT models use two iterations, while HuBERT-large uses three iterations). HuBERT-MR-H has a similar parameter size to both HuBERT-large and wav2vec2-large after combining three HuBERT models. However, it requires less computational overhead compared to other large models. This reduction is mainly due to the O(T 2) complexity of the transformer lay- ers in computing intermediate hidden representations [33]. While HuBERT-MR-H has lower-resolution networks in its sub-module, it can save computational effort, as shown in MACs and FLOPs in Table 5. the necessary information in the speech. As shown in Figure 2, high-resolution HuBERT features capture better envelope information in each frame of the speech, while low-resolution features have a more detailed formant presentation. This leads us to hypothesize that high- resolution HuBERT may have a better understanding in the time domain, while low-resolution features have more de- tailed information in the frequency domain. This property is similar to Short-time Fourier transformation with different window sizes and shifts, to some extent. 4. Conclusion 3.4. Further Discussion Our experiments show that HuBERT models with different res- olutions can extract"}