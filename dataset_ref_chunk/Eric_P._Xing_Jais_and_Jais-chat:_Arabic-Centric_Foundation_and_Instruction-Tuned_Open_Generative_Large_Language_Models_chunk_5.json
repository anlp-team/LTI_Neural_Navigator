{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many tokens of text does Pajama22 have?", "answer": " 1.2T tokens", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What is the number of parameters in the LLaMA2 model?", "answer": " 70B parameters", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What type of model architecture is Jais based on?", "answer": " standard transformer-based architecture", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What proportion of English and Arabic was found to work better in training data?", "answer": " 1:2 (2\u00d7 more English than Arabic)", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What is the impact of using the GPT-2 tokenizer on common Arabic words?", "answer": " Over-segmented into individual characters", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What tool was used to train the Jais subword tokenizer on a combined corpus of English and Arabic languages?", "answer": " Byte-pair encoding (BPE)", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What kind of positional encodings does Jais utilize?", "answer": " ALiBi positional encodings", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What kind of activation function is used in each transformer block of Jais?", "answer": " SwiGLU", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What does ALiBi penalize in terms of attention scores?", "answer": " By a linearly decreasing amount, proportional to the distance between the relevant key and the query", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}, {"question": " What hyperparameter search technique was used for Jais?", "answer": " Maximal update parametrization (\u00b5P)", "ref_chunk": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}], "doc_text": "Pajama22 have 1.2T tokens of text. The recently-released LLaMA2 has 70B parameters, and it is trained on 2T tokens. As mentioned above, for Arabic, we have 72 billion tokens (after adding 18 billion tokens of translated text). If we apply the Chinchilla scaling law, we would optimally be able to train a model of 6-7B parameters on this data. We could probably train a slightly larger model, as Arabic involves cltificization of conjunctions and pronouns (e.g., and his house is one word in Arabic, but three words in English), and thus the scaling law might differ a bit. Indeed, some of our experiments suggest that one might need as few as 14 tokens per parameter for Arabic; yet, this does not fundamentally change the fact that we do not have enough data to train a 13B parameter Arabic model, let alone a 30B one. One possible solution is to obtain more data, e.g., by adding more Arabic social media posts, but these are generally noisy. Another option is to train on mixed Arabic and English training data, and thus compensate for the missing Arabic tokens with English ones. This latter idea worked well in our experiments: we found that mixing Arabic and English in a proportion of 1:2 (i.e., 2\u00d7 more English than Arabic) works better than training on Arabic only. In the future, we plan to try incorporating a higher proportion of English, but we also need to be careful: for example, the BLOOMz experiments [MWS+23] indicate that adding ten times as much English data results in degradation of the model performance. 3 Model 3.1 Model Architecture Jais is based on a standard transformer-based architecture [VSP+17]. In particular, we use a causal decoder- only model, similar to the one used by GPT-2 [RWC+19] and LLaMA [TLI+23]. Decoder-only models have achieved state-of-the-art performance in generative language tasks. Building upon this base transformer archi- tecture, we use a number of recent improvements from the literature, as well as from our own experiments. Jais Tokenizer: The choice of tokenizer can have a significant impact on the performance of an NLP model [LBM23]. How words are split is influenced by the composition of the corpora used to train the tokenizer [PLMTB23]. A common tokenizer used in LLMs is the GPT-2 tokenizer [RWC+19], which is also used by OPT [ZRG+22] and GPT-3 [BMR+20]. 20https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 21https://ai.meta.com/llama/ 22https://github.com/togethercomputer/RedPajama-Data 9 However, because the GPT-2 tokenizer is primarily trained on English corpora, common Arabic words such as (cid:64) (cid:9)(cid:88)(cid:65) (cid:214)(cid:207) (English \u2018why\u2019) are over-segmented into individual characters [PLMTB23]. This over-segmentation lowers the performance of the model and increases the computational costs compared to using a custom tok- enizer that is specifically designed for the target languages [CL19]. Moreover, in order to increase the scope of multi-linguality, we want the tokenizer to break words into meaningful subwords. This is likely to encourage cross-lingual transfer by better token-level alignment between languages. In order to achieve this, we trained our own subword tokenizer (Jais tokenizer) on a combined corpus of English and Arabic languages using byte-pair encoding (BPE) [SHB16]. To alleviate bias towards one language, we prepared a training corpus of 10B words containing equal proportions of English and Arabic text. Table 4 shows the fertility scores [BCP+90] of Jais tokenizer against the tokenizers of BERT Arabic23 [SAY20], BLOOM [SFA+23], and GPT-2 [RWC+19] on English, Arabic, and code validation datasets. We can observe that the fertility score for the Jais tokenizer is close to 1, even though the vocabulary of Jais has only 84,992 entries, compared to BLOOM, which has 250,000 entries. The result shows the optimality of our custom-made tokenizer over our test corpus as compared to other tokenizers. ALiBi Positional Encodings: Positional embeddings provide information about word order to transformer- based LLMs. A common strategy to manage training complexity is to train the model with a limited context length. Subsequently, during inference, the model is applied to an extended context length using extrapola- tion [SLP+22]. Recent research has indicated that conventional methods of integrating word order into the transformer model, such as learnable positional embeddings, as used in models such as GPT-2 [RWC+19], and sinusoidal encoding, as proposed in [VSP+17], do not perform well when applied to longer contexts [PSL22]. Thus, we use Attention with Linear Biases (ALiBi) positional encodings [PSL22], which support efficient ex- trapolation to long contexts. Rather than modifying the input embeddings, ALiBi penalizes the attention scores by a linearly decreasing amount, proportional to the distance between the relevant key and the query. SwiGLU Activation Function: Activation functions play a pivotal role in the training of neural network models. We use SwiGLU [Sha20] in each transformer block. It combines the advantages of Swish [RZL17] and GLU [Sha20] activations, and has been shown to improve over both of them. Because of SwiGLU\u2019s extra computational overhead, adjustments were made in the hidden dimensionality of the feed forward network to compensate. Rather than apply a filter df f = 4 \u2217 dmodel, we apply a filter that is 8 3 \u2217 dmodel. This ensures that the feed forward network has a FLOP cost that is comparable to that of GeLU activation. Maximal Update Parametrization: Hyperparameter search in LLMs is expensive due to the size of the model and the scale of the dataset used in training. Thus, it is not feasible to do an extensive hyperparameter search on the final model. Fortunately, recent studies have shown that optimal hyperparameter values become stable across neural network sizes when the models have been parametrized using maximal update parametriza- tion (\u00b5P) [YHB+21]. For Jais hyperparameter search, we tuned the optimal values for batch size and learning rate on a 40M-parameter model, and transferred the best values to our 13B-parameter model. 3.2 Model and Training Hyperparameters Table 5 shows the number of layers, heads, and dimensionality for Jais, along with the optimization hyperpa- rameter values and peak learning rates. While training, we sampled a source from the source list described in Section 2 and generated instances with a complete length of 2048 tokens."}