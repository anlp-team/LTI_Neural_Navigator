{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Overview_of_Robust_and_Multilingual_Automatic_Evaluation_Metrics\n\nfor_Open-Domain_Dialogue_Systems_at_DSTC_11_Track_4_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many dialogues are there in the HCEnglish dataset?", "answer": " 2,292", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What is the average cost of annotating each dialogue in the HCChinese dataset?", "answer": " 2.36 US dollars", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What does the metadata provided for the original turns in the CHANEL and DSTC10 datasets allow participants to do?", "answer": " Decide whether or not to use the original turns and their translations in the training of their evaluation models", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What was used to perform the English-to-Chinese translation of the HCChinese data?", "answer": " Tencent MT", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What was the average correlation coefficient for Adequacy scored by six annotators?", "answer": " 0.79", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What was the average compensation for workers collecting annotations in the HCEnglish dataset?", "answer": " \u223c 15$/hr", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " How many dimensions are evaluated in the dialogue system at the turn-level?", "answer": " 4", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What is one of the dialogue-level dimensions evaluated in the dialogue system?", "answer": " Overall quality of and satisfaction with the dialogue", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " What model is used as the baseline model in the study?", "answer": " Multilingual variant of deep AM-FM", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}, {"question": " How many teams participated in Task 1?", "answer": " 4", "ref_chunk": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}], "doc_text": "Hidden: Manually back-translated/paraphrased \u2014 Table 1: Summary of train/development/test datasets. Language Dataset EN ZH ES HCEnglish HCChinese DSTC10 Total HCEnglish HCChinese DSTC10 Total HCEnglish DSTC10 Total Global Turns Dialogues 1,700 59 478 40 114 - 2,292 99 364 15 1,672 160 123 - 2,159 175 55 3 333 - 388 3 4,839 277 Table 2: Summary statistics of the test dataset used for task 1 at turn and dialogue level, and separated by language. In addition, toxicity and sentiment analysis meta- data were provided for the original turns in both the CHANEL and DSTC10 datasets for filtering and dialogue curation purposes, as well as to avoid po- tential biases. These metadata allowed participants to have a better reference of the dataset quality, being of great help for them to decide whether or not to use these original turn and their transla- tions in the training of their evaluation models and, optionally, fine-tune multilingual pre-trained mod- els allowing better performance on the proposed dialogue-oriented tasks. Data Format All data given follow a unified data format to make the storage, handling, and retrieval easier for the participants. Detailed guide- lines are available in the track repository.9 ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) time- based filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to an- notate the dialogues. The entire annotation process spanned a month and incurred costs of approxi- mately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency. Dimensions For HCEnglish, Amazon Mechani- cal Turk (AMT) was used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a con- venience pool of workers used for NLP evalua- tion tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was \u223c 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 2.2 Dimensions Evaluated Since open-domain dialogue systems have multi- facet nature, the evaluation can be accomplished from different perspectives. Since this is the case in both development and test data of task 1 (multilin- gual) and task 2 (robust), we include the following dimensions at turn-level and dialogue-level annota- tions (Mehri et al., 2022): \u2013 Turn-level dimensions: Appropriateness - The response is appropri- ate given the preceding dialogue. Content Richness - The response is informa- tive, with long sentences including multiple entities and conceptual or emotional words. 10Without the convenience pool our annotator agreement was near random. 11https://cloud.tencent.com/product/tmt Grammatical Correctness - Responses are free of grammatical and semantic errors. Relevance - Responses are on-topic with the immediate dialogue history. \u2013 Dialogue-level dimensions: Coherence - Throughout the dialogue, is the system maintaining a good conversation flow. Engageness/Likeability - Throughout the di- alogue, the system displays a likeable person- ality. Informativeness - Throughout the dialogue, the system provides unique and non-generic information. Overall - The overall quality of and satisfac- tion with the dialogue. Furthermore, when choosing the test dimensions, the annotations available in the train and develop- ment data were taken into account to keep them balanced and homogeneous. The dimensions chosen at the turn-level show how much the responses are appropriate, informa- tive including multiple entities and conceptual or emotional words, free of grammatical and semantic errors, and on-topic with the immediate dialogue history. The dimensions chosen at the dialogue- level show how much the system maintains a good conversation flow, engages well with the user, pro- vides unique and non-generic information, and the overall quality of the system. Table 3 summarizes the dimensions for each test data set. As can be seen, the DSTC10 set only has human turn-level annotations. 2.3 Baseline We provide a multilingual variant of deep AM- FM (Zhang et al., 2021) (used previously during Track5 at DSTC10) as the baseline model. The for- mulation of both AM and FM remains unchanged except that we switch their original English-based pre-trained language models to multilingual mod- els. For the adequacy metric (AM), we use XLM- R12 (Conneau et al., 2020) to extract sentence-level embeddings of both the response and the last sen- tence in the corresponding dialogue context. Then, the cosine similarity of the two embeddings is the AM score assigned to the corresponding response. For the fluency metric (FM), we adopt the multi- 12https://huggingface.co/sentence-trans formers/xlm-r-100langs-bert-base-nli-sts b-mean-tokens Sets Dimensions DSTC10 DSTC10-turn ChatEval-turn JSALT-turn A CR GC R A A HCChinese HCChinese-dial C EL HCChinese-turn O I CR GC R HCEnglish HCEnglish-dial O HCEnglish-turn A CR GC R C EL I Test data Test-dial Test-turn C EL O A CR GC R I C: Coherence I: Informativeness A: Appropriateness EL: Engageness/Likeability O: Overall CR: Content Richness GC: Grammatical Correctness R: Relevance Table 3: the dimensions (human- annotations) available for each dataset used in the test data, both at the turn and dialogue level. Summary of lingual GPT-213 as the backbone language model. The conditional probability of the response w.r.t. the context given by the multilingual GPT-2 model serves as the FM score of the response. The final AM-FM score is the arithmetic mean of both met- ric scores. All information related to the baseline model, such as code and data, can be found in this GitHub repository.14 2.4 Participants In Task 1, 4 teams participated, which provided a total of 16 submissions. Participants were asked to provide a brief description of the system for their proposals. The two system descriptions provided by the participants are shown below: Team 4 Their approach utilizes two submetric groups, XLM-R and ChatGPT, for evaluating dia- logue responses. The"}