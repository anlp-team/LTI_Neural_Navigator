{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Study_on_the_Integration_of_Pipeline_and_E2E_SLU_Systems_for_Spoken_Semantic_Parsing_Toward_Stop_Quality_Challenge_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to the text, which models perform better than traditional E2E models?", "answer": " 2-pass and compositional E2E SLU models", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " What does Table 2 show about fine-tuning Whisper?", "answer": " It can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data.", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " What is the Word Error Rate (WER) achieved by Whisper on the ASR task?", "answer": " 2.2", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " How does using Whisper transcripts in 2-pass and compositional E2E SLU model during inference impact performance?", "answer": " It leads to significant performance gains.", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " Why were the authors not able to investigate incorporating the Whisper model in their Compositional and 2 pass E2E SLU model?", "answer": " Due to time constraints, but they plan to investigate this in future work.", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " What is the WER achieved by using the ASR transcript produced by the 4 best ASR models combined?", "answer": " 2.2", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " How did using the ROVER combination on the hypothesis produced by the 4 best ASR models impact system performance?", "answer": " Significantly boost the performance of pipeline systems.", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " What was used to achieve WER 2.2 according to the text?", "answer": " The ROVER combination on the hypothesis produced by the 4 best ASR models.", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " What allocation supported the work using Bridges2 system at PSC and Delta system at NCSA?", "answer": " CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}, {"question": " Who was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739?", "answer": " Jessica Huynh", "ref_chunk": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}], "doc_text": "Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig- Pre-trained Model Pretrained Dataset Test WER (\u2193) STOP benchmark [5] Wav2Vec2 Hubert STOP STOP 4.45 4.26 Our ASR models Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP 3.8 3.3 3.1 2.7 2.4 2.3 2.3 Our System Combination 4 best models 2.2 Table 3: Word Error Rate (WER) on STOP dataset. ni\ufb01cantly better than all the E2E SLU models. Table 2 shows that \ufb01netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi\ufb01cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work. Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8. 5. ACKNOWLEDGEMENT This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re\ufb02ect those of that funding agency. 6. REFERENCES [1] D. Le, A. Shrivastava, P. D. Tomasello, et al., \u201cDeliberation model for on-device spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3468\u20133472. [2] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d in Arxiv preprint arXiv:2207.06670, 2022. [3] S. Arora, S. Dalmia, B. Yan, et al., \u201cToken-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,\u201d in Proc. EMNLP, 2022. [4] [5] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. P. Tomasello, A. Shrivastava, D. Lazar, et al., \u201cSTOP: A dataset for Spoken Task Oriented Semantic Parsing,\u201d in CoRR."}