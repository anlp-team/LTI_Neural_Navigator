{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_The_Hidden_Dance_of_Phonemes_and_Visage:_Unveiling_the_Enigmatic_Link_between_Phonemes_and_Facial_Features_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the relationship between the specific pattern, the mouth, and the nose?", "answer": " The specific pattern corresponds to a unique compositional unit of speech (phoneme), and when a speaker enunciates different phonemes, the vocal tract, mouth, nose, and other facial structures act in concert.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What is the purpose of the AM Estimator mentioned in the text?", "answer": " The AM Estimator is used to predict articulatory movements (AMs) from phonemes.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What type of problem is transforming each phoneme into a log mel spectrum considered to be?", "answer": " It is considered a classic regression problem.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What is the main goal of developing a modified version of the classical MNasNet model mentioned in the text?", "answer": " The main goal is to investigate the correlations between each phoneme and AM pair.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " How is the correlation between each phoneme-AM pair analyzed in the experiment described in the text?", "answer": " The segmented phonemes are transformed into log mel spectrum to capture information from the frequency domain, and an AM estimator is employed to predict each AM from phonemes. Hypothesis testing is then used to analyze the correlation.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What is the purpose of hypothesis testing for phoneme-AM predictability mentioned in the text?", "answer": " The purpose is to determine whether a specific phoneme can predict an AM.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What is the null hypothesis stated for hypothesis testing of AM predictability from phonemes?", "answer": " The null hypothesis is that the AM is not predictable from the phoneme.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " How is the chance level estimator calculated in the experiment described in the text?", "answer": " The chance level estimator is calculated as the mean of all instances of the phoneme in the training set.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What technique is used to test the hypothesis in the experiment?", "answer": " The one-sided paired-sample t-test is used to test the hypothesis.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}, {"question": " What is the significance level chosen for obtaining statistically significant results in the experiment?", "answer": " The significance level chosen is \u03b1 = 0.05.", "ref_chunk": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}], "doc_text": "specific pattern, the mouth and nose must form a corresponding shape. Each pattern corre- sponds to a unique compositional unit of speech, or a phoneme. When a speaker enunciates different phonemes, the vocal tract, mouse, nose, and other related facial structures act in concert. And each phoneme, therefore, carries some information about all these related features. 3. Methods 3.1. Overview 3.3. AM Estimator We leverage an AM estimator Eij to predict the j-th AM from the i-th phoneme m(i) = Eij(p(j) as an estimator that maps the j-th phoneme to the i-th AM. To begin, we transform each phoneme into a log mel spectrum, which is essentially an image. This is a classic regression problem, and therefore, we need a model with strong feature extraction capabilities to extract in- formation from the image. We develop a modified version of the classical MNasNet model developed by Google AI. It is de- signed to be efficient, lightweight, and highly accurate for tasks such as image classification and object detection. [21]. Our modification retains the original structure, but with a few modi- fications to the input and output layers. Specifically, we change the input Conv2d module to accept only 1 channel, and the out- put Linear module to produce only 1 value. In addition, since this part is model-independent, other models such as ResNet [22] are also capable of achieving the same function. We aim to investigate the correlations between each phoneme and AM pair. As shown in Fig 1, we first transform the seg- mented phonemes into log mel spectrum to better capture infor- mation from the frequency domain. After that, an AM estimator is employed to predict each AM from phonemes. Finally, we use hypothesis testing to analyze the correlation between each phoneme-AM pair. 3.2. Notations 3.4. Hypothesis Testing for Phoneme-AM Predictability Once AMs are predicted from different phonemes, the next step is to determine whether a specific phoneme can actually predict an AM. To do this, we use hypothesis testing for each AM- phoneme pair separately. Firstly, we write the null hypothe- sis and the alternative hypothesis for the i-th AM and the j-th phoneme as Our problem involves a set of paired voice recordings of phonemes and AMs, where we aim to predict each AM from different phonemes. We begin by segmenting the record- ings into phonemes, which can be represented as P = p(1), p(2), . . . , p(k), where k denotes the total number of dis- tinct phonemes. Similarly, the AMs can be represented as AM s = m(1), m(2), . . . , m(n), where n represents the num- ber of summarized AMs. We refer to the entire dataset as D. To simplify the training and evaluation process, we divide D into three subsets. The first subset is the training set Dt, which is used for estimator learning. The second subset is the validation set Dv1 , which is used for estimator selection. Fi- nally, the third subset is the validation set Dv2 , which is used for hypothesis testing and AM-phoneme pair selection. H0 : AM m(i) is not predictable from phoneme p(j) H1 : AM m(i) is predictable from phoneme p(j) To reject the null hypothesis H0, we need to compare our estimator Eij for the AM m(i) when using phoneme p(j) as input with a chance-level estimator Cij. If the performance of Eij is statistically significantly better than Cij, we can reject H0 and accept H1. To estimate the chance level for phoneme p(j) in our training set Dt, we use the mean m(i) of all instances of that phoneme in the set. Specifically, we calculate a constant m(i). We can value Cij as follows: Cij = 1 |Dt| express the hypotheses as: (cid:80) m(i)\u2208Dt H0 : \u00b5(\u03b5ij/\u03b5C H1 : \u00b5(\u03b5ij/\u03b5C ij) \u2a7e 1 ij) < 1 Here, \u00b5(\u00b7) represents the mean function, and \u03b5ij and \u03b5C ij are the mean squared errors (MSE) of the estimators Eij and Cij on the validation set Dv2 , respectively. We can compute them as follows: \u03b5ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 ( \u02c6m(i) \u2212 m(i))2 \u03b5C ij = 1 |Dv2 | (cid:88) m(i)\u2208Dv2 (Cij \u2212 m(i))2 To conduct repeated experiments, we need to train the es- timators multiple times. In each iteration, we randomly split the dataset into Dt, Dv1 , and Dv2 . We then use the one-sided paired-sample t-test to test the hypothesis. The confidence in- terval (CI) bounds are: CIl = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) \u2212 t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N CIu = \u00b5 (cid:32) \u03b5ij \u03b5C ij (cid:33) + t1\u2212\u03b1,\u03bd \u00b7 \u03c3(\u03b5ij/\u03b5C ij) \u221a N Here, \u03c3(\u00b7) represents the standard deviation function, N represents the number of experiments, \u03b1 represents the signifi- cance level, and \u03bd = N \u2212 1 represents the degree of freedom. For this project, we set N = 10, and we choose \u03b1 = 0.05 to obtain statistically significant results. We can read the value of t1\u2212\u03b1,\u03bd directly from the t-distribution table. To test the hypoth- esis, if the CI upper bound CIu < 1, we can infer that we suc- cessfully reject H0 and accept H1, meaning that the AM m(i) is predictable from phoneme p(j). On the contrary, if CIu \u2265 1, we cannot reject H0, indicating that the result is not statistically significant. 4. Experiments 4.1. Dataset We conducted experiments on a private audio-visual dataset D. The dataset contains 1,026 individuals\u2019 paired voice recordings and scanned 3D facial shapes. Each recording is a raw speech speaking out general phonemes and sentences with a length of 1-2 minutes. Each facial data consists of 6790 3D-coordinate points collected from one person. 4.2. Data Processing and Training Phoneme segmentation. To identify predictable AMs and their corresponding phonemes, the first step is to extract individual phonemes from the dataset. However, due to the large amount of data and the complexity of distinguishing phoneme intervals, manually segmenting phonemes can be laborious, difficult, and imprecise. To improve the accuracy of phoneme segmentation, we employ state-of-the-art"}