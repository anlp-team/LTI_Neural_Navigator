{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_New_Benchmark_of_Aphasia_Speech_Recognition_and_Detection_Based_on_E-Branchformer_and_Multi-task_Learning_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the overall accuracy of the E-Branchformer+WavLM model?", "answer": " 89.3", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " What is the sentence-level accuracy of the Conformer model from Table 3?", "answer": " 29.7", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " What type of archiectures are considered data-hungry in the text?", "answer": " Hybrid CTC/Attention architectures", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " According to Table 3, which type of Aphasia detectors have the best sentence-level detection accuracy?", "answer": " Tag-based Aphasia detectors", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " Why do tag-based detectors sometimes produce more false positives according to the text?", "answer": " They are too sensitive to dysfluency", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " What is the WER for the E-Branchformer+WavLM model +Tag-prepend experiment?", "answer": " 39.6", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " What metric is used for speech recognition in Table 4 for Dementia detection?", "answer": " Word Error Rate (WER)", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " Why is the model less effective at acoustic modeling in the DementiaBank evaluations?", "answer": " Audio is often noisy and has variable speaking volume", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " What is the conclusion of the paper mentioned in the text?", "answer": " Building an all-in-one Aphasia speech recognition and detection system, standardizing data processing, and suggesting potential improvements", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}, {"question": " What potential benefits are suggested by combining InterCTC and tag-prepending?", "answer": " More accurate tag predictions", "ref_chunk": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}], "doc_text": "WER WER Model Accuracy Sent Spk Model Patient Control Overall Accuracy Spk Sent SVM [4] 96.2 Conformer [38] Conformer [37] - - 29.7 25.5 - 91.7 E-Branchformer+WavLM +Tag-prepend +Tag-append +Tag-prepend/append +InterCTC-6 +InterCTC-9 +InterCTC-6/Tag-prepend 89.3 89.2 90.8 85.2 84.5 89.7 95.1 95.1 95.7 97.3 97.3 96.7 Table 3: Sentence-level (Sent) and speaker-level (Spk) detec- tion accuracy of proposed methods on AphasiaBank. [4] is reproduced using the of\ufb01cial code with oracle transcripts as the input. For +Tag-prepend/append and +InterCTC-6/Tag- prepend experiments, only the Tag-prepend output is reported since the difference is negligible. even against systems using an external language model. Despite this, they have a much higher WER for very severe Aphasia pa- tients. We believe this is because hybrid CTC/Attention archi- tectures are data-hungry, but the number of utterances and their average duration is much smaller for very severe patients. From Table 3, we can see that the tag-based Aphasia de- tectors have the best sentence-level Aphasia detection accu- racy. Interestingly, although the performance difference be- tween prepending and appending Aphasia tags is insigni\ufb01cant, inserting at both positions leads to slightly better sentence-level and speaker-level accuracy. Meanwhile, the InterCTC-based detector at layer 6 achieves state-of-the-art speaker-level ac- curacy (97.3%), surpassing the SVM baseline. However, its sentence-level accuracy is lower than those of tag-based detec- tors. This corresponds to previous studies showing that mid- dle encoder layers are more important to speaker-related tasks while the bottom layers are more relevant to ASR and related tasks [19, 30]. We also \ufb01nd that tag-based detectors produce signi\ufb01cantly more false positives for speakers who do not have Aphasia but are less \ufb02uent than others, thus having a lower speaker-level accuracy. This implies that tag-based detectors are sometimes too sensitive to dys\ufb02uency. E-Branchformer+WavLM 39.1 +Tag-prepend 39.6 +InterCTC-6 24.8 25.1 15.0 15.0 Table 4: Test result of proposed methods on DementiaBank. The metric for speech recognition is the word error rate (WER). The metrics for Dementia detection are sentence-level (Sent) and speaker-level (Spk) accuracy. Other studies [39\u201342] are not listed as their models are trained and tested on different data. Note that [37, 38] use a larger and cleaner training set. 65.6 61.3 83.3 77.1 Table 4 shows evaluation results for DementiaBank. Al- though the overall WER is much lower than those in previ- ous studies, Dementia detection accuracy is suboptimal. As we drew original recordings from the DementiaBank Pitt cor- pus, the audio is often noisy and has variable speaking volume. Consequently, the model is less effective at acoustic modeling, as seen by the decreased InterCTC detection accuracy. The re- sults also suggest that linguistic features are more important for Dementia detection than Aphasia. Furthermore, majority vot- ing for speaker-level predictions is less effective in this case as the number of sentences per speaker is typically between 5 to 20. Despite this, we believe our method has the potential to be adapted to other disordered speech in future studies. 4. Conclusion In this paper, we build an all-in-one Aphasia speech recognition and detection system and test its performance using Aphasia- Bank and DementiaBank. We also standardize the data process- ing and model evaluation process to establish a public bench- mark. Future studies are required to improve the recognition performance for severe Aphasia patients and the detection per- formance on DementiaBank. We can also further investigate the impact of joint learning and combining detector methods, and explore the potential bene\ufb01ts of \ufb01ne-tuning a pre-trained healthy ASR system using disordered speech. Finally, more accurate tag-based predictions can be ob- tained by combining InterCTC and tag-prepending. This sug- gests that tag predictions are re\ufb01ned based on prior InterCTC predictions. A similar result is discovered in a previous study where the language identity predictions are more accurate by incorporating an InterCTC auxiliary task [22]. In addition, the combined model has higher sentence-level accuracy and lower speaker-level accuracy compared to its InterCTC counterpart, which demands future investigation. 5. Acknowledgements This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cy- berinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] 6. References [1] M. Danly and B. Shapiro, \u201cSpeech prosody in broca\u2019s aphasia,\u201d Brain and Language, vol. 16, no. 2, pp. 171\u2013190, 1982. [25] [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- speech representations,\u201d in Proc. supervised learning of NeurIPS, 2020. [4] G. Chatzoudis et al., \u201cZero-shot cross-lingual aphasia detection using automatic speech recognition,\u201d in Proc. Interspeech, 2022. \u00b4Alvarez, \u201cImproving apha- I. G. Torre, M. Romero, and A. sic speech recognition by using novel semi-supervised learning methods on aphasiabank for english and spanish,\u201d Applied Sci- ences, vol. 11, no. 19, 2021. [5] [6] D. Le and E. Provost, \u201cImproving automatic recognition of aphasic speech with aphasiabank,\u201d in Proc. Interspeech, 2016, pp. 2681\u20132685. [7] M. Perez, Z. Aldeneh, and E. Provost, \u201cAphasic speech recog- nition using a mixture of speech intelligibility experts,\u201d in Proc. Interspeech, 2020, pp. 4986\u20134990. [8] D. Le, K. Licata, and E. Provost, \u201cAutomatic quantitative anal- ysis of spontaneous aphasic speech,\u201d Speech Communication, vol. 100, pp. 1\u201312, 2018. [9] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic assessment of speech impairment in cantonese-speaking people with aphasia,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 2, pp. 331\u2013345, 2020. [10] Y. Qin et al., \u201cAn end-to-end approach to automatic speech as- sessment for cantonese-speaking people with aphasia,\u201d Journal of Signal Processing Systems, vol. 92, pp. 819\u2013830, 2019. [11] A. Balagopalan et al., \u201cCross-language aphasia detection using optimal transport domain adaptation,\u201d in NeurIPS, 2019. [12] Y. Qin, T. Lee, and A. Kong, \u201cAutomatic speech assessment for aphasic patients based on syllable-level embedding and supra- segmental duration features,\u201d in Proc. ICASSP, 2018, pp. 5994\u2013 5998. [13] Y. Qin et al., \u201cAutomatic speech assessment for people with aphasia using TDNN-BLSTM with multi-task"}