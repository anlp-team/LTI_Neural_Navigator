{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the attention mechanism in the alignment model described in the text?,        answer: The purpose of the attention mechanism is to allow the model to examine local acoustics in order to refine predictions.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " How are streaming AED models different from general AED models?,        answer: Streaming AED models compute the context vector over a subset of frames to allow for streaming decoding, while general AED models sum over all input frames.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " How does the AED model terminate inference?,        answer: In the AED model, inference terminates when the model has output the end-of-chunk symbol in the final chunk.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " What is the Monotonic Chunkwise Attention (MoChA) model and how does it differ from the vanilla AED model?,        answer: The MoChA model allows the model to generate the next output using attention over a local window of frames to the left of the selected frame, while the vanilla AED model does not have this feature.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " What is the purpose of the monotonic infinite lookback (MILK) attention model?,        answer: The MILK attention model computes the context vector over all frames to the left of the selected frame at each step to enable streaming operation.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " How does the triggered attention approach enable streaming operation?,        answer: In the triggered attention approach, a CTC-network is used to control the activation of an AED model with a limited decoder delay, allowing for streaming operation.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " What do segmental attention models discussed in the text focus on?,        answer: Segmental attention models focus on the relevance of segment length modeling and how it leads to improved generalization towards long sequences.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " What is the Neural Transducer (NT) model described in the text?,        answer: The NT model explicitly partitions input encoder frames into non-overlapping chunks and processes a single chunk at a time, advancing to the next chunk when it outputs a special end-of-chunk symbol.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " How are E2E models different from classical ASR models?,        answer: E2E models introduce alternative alignment modeling approaches compared to classical ASR models.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}, {"question": " How does the text describe the role of transducer approaches in ASR?,        answer: The text mentions that transducer approaches handle the alignment problem in a way that can be interpreted to be similar to HMMs, including marginalization over alignments.    ", "ref_chunk": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}], "doc_text": "alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a \u2018local\u2019 relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference. AED models, which make no conditional (cid:3), (analogous to \u27e8eos\u27e9 in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks. An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, \u03c4 , is selected, then the model produces an output based on the local encoder frame, h\u03c4 . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame \u03c4 : h\u03c4 \u2212W +1, . . . , h\u03c4 . Thus, the MoChA model consists of a two-level process \u2013 identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] \u2013 the monotonic infinite lookback (MILK) attention model \u2013 computes the context vector over all frames to the left of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities. The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol 1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; H W W the NT model Relationship to Classical ASR In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 8 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69]. E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40]. is"}