{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Surveying_(Dis)Parities_and_Concerns_of_Compute_Hungry_NLP_Research_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What were the four categories into which the questions were divided?", "answer": " General information about participants, concerns about environmental impact, access to computational resources, and impact of compute-intensive experiments on the reviewing process", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " How many questions were asked in total?", "answer": " 19", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " Which questions were crafted as simple yes/no/unsure questions?", "answer": " Most of the questions were crafted as simple yes/no/unsure questions", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " What percentage of participants indicated they were junior members of the community?", "answer": " 53.5%", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " What were the possible responses for the question about current position that participants are holding?", "answer": " Student, Academic Postdoc, Academic PI, Researcher in large industry, Researcher in small industry, other", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " What was the largest group of participants based on their current position?", "answer": " Students (38.5%)", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources?", "answer": " Yes, No, Unsure", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " How often do you feel like your work would have been valued more by the community if you had access to more computational resources?", "answer": " Five point scale", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " How concerned are you by the environmental footprint of the field of NLP?", "answer": " Five point scale", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}, {"question": " Would your work benefit from smaller versions of pretrained models released alongside larger ones?", "answer": " Yes, No, Not sure", "ref_chunk": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}], "doc_text": "The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (\u00a72.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (\u00a73) and their access to computational resources (\u00a74). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (\u00a75). To keep a low-effort for our participants, we crafted most of the (\ud835\udc44)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). \ud835\udc444 (available compute resources) and \ud835\udc4411 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, \ud835\udc449, \ud835\udc4418, and \ud835\udc4419 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (\ud835\udc444, \ud835\udc449, \ud835\udc4411, \ud835\udc4412, \ud835\udc4414, \ud835\udc4419) were optional. All questions are provided in Table 1. 2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2). Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (\ud835\udc441). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career. Job Sector. We further asked our participants about the current position they are holding (\ud835\udc442). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l) 1https://www.aclweb.org/portal/ content/efficient-nlp-survey Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania. Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale. Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional). Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper\u2019s research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other) Q19. Any other thoughts or suggestions? Answer: Free text (optional). Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants. industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with \u201cother\u201d from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of \u201cother\u201d into the most fitting group in the survey (one"}