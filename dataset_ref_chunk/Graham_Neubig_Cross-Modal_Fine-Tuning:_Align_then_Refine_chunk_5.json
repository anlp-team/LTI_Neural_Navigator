{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of performing clustering on data labels in ORCA for dense prediction tasks with continuous labels?", "answer": " To generate pseudo-labels", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " How does ORCA compare to hand-designed, AutoML-searched, and general-purpose architectures across a breadth of modalities?", "answer": " ORCA outperforms them", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " Why is full fine-tuning generally better than tuning only the layer norms?", "answer": " Recent observations suggest that full fine-tuning generally outperforms tuning only the layer norms", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " What are the two modalities in which in-depth analyses are performed to show that ORCA is competitive with expert-designed task-specific models?", "answer": " PDE solving and tabular classification", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " What are the representatives of the most studied language and vision modalities used in the experiments with ORCA?", "answer": " RoBERTa and Swin Transformers", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " What is the percentage of the fine-tuning time taken for data alignment in the ORCA model?", "answer": " 11%", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " What are the 3 factors crucial to exploiting the learned knowledge in the ORCA model?", "answer": " Data alignment, full fine-tuning, pretraining modality selection", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " How does ORCA perform compared to hand-designed architectures on all tasks in the experiments?", "answer": " ORCA outperforms hand-designed architectures on all tasks", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " Why does data alignment using ORCA facilitate model adaptation according to the text?", "answer": " Closing the gap between the target and pretraining modalities can facilitate model adaptation", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}, {"question": " What is the key innovation that distinguishes ORCA from naive fine-tuning?", "answer": " Data alignment process", "ref_chunk": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}], "doc_text": "in the clas- si\ufb01cation datasets. For dense prediction tasks with continuous labels, we \ufb01rst perform clustering on the data labels to generate pseudo-labels. 4. Experiments Having introduced how ORCA tackles cross-modal \ufb01ne-tuning, we proceed with showing its empirical ef\ufb01cacy via three thematic groups of experiments: (1) we evaluate ORCA across a breadth of modalities and show that it outperforms hand-designed, AutoML-searched, and general-purpose architectures; we study its key compo- nents to understand the mechanism behind cross-modal \ufb01ne-tuning and exemplify how it bene\ufb01ts limited-data (2) we perform in-depth analyses in two modalities; . Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 3: Prediction errors (\u2193) of ORCA, naive \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider adapting all parameters (full setting) vs. only the layer norms (FPT setting).ORCA is better in both settings. The fact that full \ufb01ne-tuning generally outperforms tuning only the layer norms is also consistent with recent observations (Rothermel et al., 2021). See Appendix A.4.3 for the error bars. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite DeepSEA Train-from-scratch 50.87 76.67 8.0E-2 5.09 0.50 9.96 0.75 0.42 12.38 0.39 Fine-tuning ORCA 7.67 6.53 55.26 29.85 7.34E-3 7.28E-3 1.92 1.91 0.17 0.152 8.35 7.54 0.63 0.56 0.44 0.28 13.86 11.59 0.51 0.29 Fine-tuning (layernorm) ORCA (layernorm) 10.11 7.99 76.38 42.45 2.11E-2 2.21E-2 4.66 4.97 0.233 0.227 15.69 15.99 0.67 0.64 0.50 0.47 20.83 20.54 0.37 0.36 modalities, PDE solving and tabular classi\ufb01cation, to show that ORCA is competitive with expert-designed task-speci\ufb01c models; (3) we compare ORCA with previous ad-hoc cross-modal learning techniques to show that we strike a balance between generality and effectiveness. Experiment Protocol. While our work\ufb02ow accepts a wide range of pretrained transformers as model bodies, we use RoBERTa (Liu et al., 2019c) and Swin Transformers (Liu et al., 2021b), which are representatives of the most stud- ied language and vision modalities, to exemplify ORCA\u2019s ef\ufb01cacy. We implement the base models using the Hugging Face library (Wolf et al., 2019) and choose CoNLL-2003 and CIFAR-10 as the proxy datasets, respectively. For each task, we \ufb01rst perform hyperparameter tuning in the standard \ufb01ne-tuning setting to identify the optimal target sequence length, batch size, and optimizer con\ufb01guration. Experiments are performed on a single NVIDIA V100 GPU and managed using the Determined AI platform. Results are averaged over 5 trails. For other details, see Appendix A.2. 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities? FPT DASH 3 ORCA 25 6 Hand-designed 5 75 100-Suboptimal Tasks (%) 1 Perceiver IO 50 2 0 NAS-Bench-360 4 Figure 3: Aggregating Table 2 results using performance pro- \ufb01les (Dolan & Mor\u00b4e, 2002). Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. ORCA being in the top left corner means it is often the best. beats all AutoML baselines on all tasks except DeepSEA and NinaPro, where it ranks second and third, respectively. The improvements from the embedder learning stage of ORCA come at a small computational overhead\u2014Table 11 in the Appendix shows that the time needed for data alignment is only a small portion (11%) of the \ufb01ne-tuning time. Our results validate the \ufb01nding in prior cross-modal work that pretrained transformers learn knowledge transferable to seemingly unrelated tasks. In the following, we dissect the success of ORCA via multiple ablations and identify 3 factors crucial to exploiting the learned knowledge: data alignment, full \ufb01ne-tuning, pretraining modality selection. In this section, we highlight important ob- servation of this work: cross-modal \ufb01ne-tuning with data alignment can solve diverse tasks effectively and ef\ufb01ciently. To show this, we test ORCA on 10 tasks from NAS-Bench-3604 covering diverse 1D/2D problems such as protein folding, cardiac disease prediction, and cosmic-ray detection. Following Table 1, we consider 3 classes of baselines: (1) hand-designed, task-speci\ufb01c models identi\ufb01ed by Tu et al. (2022); (2) general-purpose models represented by Perceiver IO (Jaegle et al., 2022); (3) AutoML methods, including the leading algorithm on NAS-Bench-360, DASH (Shen et al., 2022). the most KEY 1: ALIGNING FEATURE DISTRIBUTIONS To understand whether the good performance of ORCA is indeed attributed to the data alignment process, which is our key innovation, we compare it with naive \ufb01ne-tuning that does not align the data (Table 3, middle rows). We see that ORCA consistently outperforms naive \ufb01ne-tuning. Moreover, we show in Appendix A.4.4 that ORCA with different alignment metrics all obtain better performance than \ufb01ne-tuning. Thus, closing the gap between the target and pretraining modalities can facilitate model adaptation. We report the prediction error for each method on each task in Table 2 and visualize the aggregate performance in Figure 3. ORCA achieves the lowest error rates on 7 of 10 tasks and the best aggregate performance. Speci\ufb01cally, it outperforms hand-designed architectures on all tasks. It 4NAS-Bench-360 is designed for testing how well ML algo- rithms generalize and is a core component of the 2022 AutoML Decathlon competition. See Appendix A.4.1 for the task summary. To further isolate the impact of data alignment, we com- pare ORCA with a train-from-scratch baseline (Table 3, \ufb01rst row) which trains RoBERTa and Swin using only the tar- get data. We observe training from scratch is worse than ORCA but better than \ufb01ne-tuning on ECG, Satellite, and DeepSea. We conjecture that this is because when the target modality differs signi\ufb01cantly from the pretraining modality, naive \ufb01ne-tuning may harm transfer, but aligning the feature distribution using ORCA can resolve this issue and bene\ufb01t Cross-Modal Fine-Tuning: Align then Re\ufb01ne 0.64 0.66 0.14 92.5 91.0 86 Embedder Learning EpochsEmb. OTDD (1e2)Fine-tuning Score 0.16 1.2 0.15 0204060 0.40 1.3 0.70 88 NinaPro Satellite 1.1 85 87 0.45 92.0 DeepSEA 0.50 010203040 0.68 91.5 0.35 010203040 ORCA 5Num Target Data (log10) 3 75Accuracy (%) 4 25 Fine-tuning 50 Satellite Figure 4: Left: Final accuracy and embedding distribution distance vs. embedder learning epochs on three NAS-Bench-360 tasks. As we learn to map the target data to the source modality better (smaller OTDD), we obtain models with better downstream performance. This shows an empirical correlation between \ufb01ne-tuning accuracy and alignment"}