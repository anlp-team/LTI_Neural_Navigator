{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Comparative_Study_on_E-Branchformer_vs_Conformer_in_Speech_Recognition,_Translation,_and_Understanding_Tasks_2024-02-27_01-10-56_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two branches merged in the architecture of cgMLP?,answer: Two branches are merged by a concatenation operation, a 1-D depth-wise convolution, and a linear projection.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " How is the output Yebf defined for an input X in cgMLP?,answer: The output Yebf for input X is defined as Yebf = LayerNorm(X3 ebf).", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " What type of information does cgMLP leverage in its architecture?,answer: cgMLP leverages depth-wise convolution and linear gating to extract local contextual information.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " How is the output X2,mlp derived in cgMLP?,answer: The output X2,mlp is derived as Z = GeLU(LayerNorm(X1 ebf)U).", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " What datasets are used in the speech recognition experiments mentioned?,answer: A total of 15 public ASR datasets are utilized, covering various languages, recording environments, speech types, and sizes.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " What optimizer is employed in the training process of the models?,answer: The Adam optimizer with warmup learning rate schedule is employed.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " What does Table 1 summarize in the results section?,answer: Table 1 summarizes the ASR results of AED models with joint CTC.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " How does E-Branchformer compare to Conformer baselines in the ASR results?,answer: E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " What type of models are mainly used in the experiments mentioned?,answer: The experiments mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding.", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}, {"question": " What speech recognition metric is used as an evaluation criterion in the experiments?,answer: The evaluation metric is character error rate (CER) or word error rate (WER).", "ref_chunk": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}], "doc_text": "(cgMLP) [29]. Two branches are merged by a concate- nation operation, a 1-D depth-wise convolution and a linear pro- jection, which is more effective than the simple concatenation followed by a linear projection used in Branchformer. For input X \u2208 RT \u00d7d, the output Yebf \u2208 RT \u00d7d is de\ufb01ned as follows: [13] X1 ebf = X + 1 2 FFN1(X), (6) X2,mha ebf , X2,mlp ebf = MHA(X1 X2 ebf), cgMLP(X1 ebf), , X2,mlp ebf ebf = X1 ebf + Merge(X2,mha ebf ), (7) (8) X3 ebf = X2 ebf + 1 2 FFN2(X2 ebf), (9) Yebf = LayerNorm(X3 ebf), (10) ebf \u2208 RT \u00d7d are intermedi- where X1 ate outputs. Figure 2 shows the architecture of cgMLP [29], which leverages depth-wise convolution and linear gating to ex- ebf \u2208 RT \u00d7d, the tract local contextual information. For input X1 output X2,mlp is derived as follows: ebf, X2,mha , X2,mlp ebf , X2 ebf, X3 ebf ebf Z = GeLU(LayerNorm(X1 ebf)U) \u2208 RT \u00d7dmlp , (11) A, B = Split(Z) \u2208 RT \u00d7 1 2 dmlp , \u02dcZ = A (cid:12) DwConv(LayerNorm(B)) \u2208 RT \u00d7 1 (12) 2 dmlp , (13) ebf = Dropout( \u02dcZV) \u2208 RT \u00d7d, where U \u2208 Rd\u00d7dmlp and V \u2208 R 1 tions. (cid:12) denotes element-wise product. X2,mlp (14) 2 dmlp\u00d7d are two linear projec- Layer Norm Split DepthwiseConv Dropout Linear Layer Norm GeLU Linear Figure 2: Architecture of cgMLP [29]. 3. Speech recognition experiments 3.1. Setups Data. A total of 15 public ASR datasets are utilized, covering various languages (English, Chinese, Spanish, Japanese, Italian, or even multilingual with 102 languages), recording environ- ments (clean, noisy, far-\ufb01eld), speech types (spontaneous, read), and sizes (10 to 10k hours). Disordered speech from Aphasia- Bank [30] is also evaluated. The evaluation metric is character error rate (CER) or word error rate (WER). The total model size and encoder\u2019s multiply-accumulate operations (MACs) for a 10-second audio are also reported. Models. We mainly use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. The en- coder is either Conformer or E-Branchformer, while the de- coder is a 6-layer Transformer. Log Mel \ufb01lterbanks are ex- tracted by default, except that FLEURS uses an SSL frontend as in [33]. FLEURS also exploits intermediate CTC [34] and self-condition CTC [35] in the encoder. We also conduct experi- ments using pure CTC or RNN-T models in a subset of datasets. Training. We follow the ESPnet2 recipes2 for data preparation, model training and decoding. Most recipes perform speed per- turbation with ratios {0.9, 1.0, 1.1} and SpecAugment [36]. A medium-sized model with hidden size d = 256 is employed by default. For FLEURS, GigaSpeech and LibriSpeech 960h, a larger model with d = 512 is trained instead. The Adam opti- mizer [37] with warmup learning rate schedule [7] is employed. Training hyperparameters such as the learning rate, weight de- cay and warmup steps are from existing baselines. We will re- lease our detailed setups to ensure reproducibility. 3.2. Results Table 1 summarizes the ASR results of AED models with joint CTC, which is the most widely used setup in ESPnet. Compared to those well-established Conformer baselines, E-Branchformer achieves comparable or superior results with a similar model size and computational complexity in almost all benchmarks. We have only observed a slight degradation in one set among 39 evaluation sets across 15 corpora. The improvements are especially remarkable in AphasiaBank, CHiME4, Fisher- Callhome, FLEURS, JSUT, MuST-C and TEDLIUM2, indicat- ing that E-Branchformer has strong modeling capacities for var- ious speech types. Conformer can vary across different datasets, with some datasets bene\ufb01ting from deeper networks while others may bene\ufb01t from wider networks. Table 2 com- pares E-Branchformer with two Conformer baselines in four con\ufb01gurations 2https://github.com/espnet/espnet/tree/ master/egs2 Table 1: CER or WER (%) on speech recognition benchmarks using attention-based encoder-decoder (AED) with joint CTC. The total number of parameters (\u00d7106) and encoder\u2019s multiply-accumulate (MAC) operations (\u00d7109) are also reported. \u2020 means a frozen SSL frontend is used but not counted. \u2021 means a language model is used with shallow fusion following existing ESPnet2 recipes. Dataset Token Metric Evaluation Sets Conformer E-Branchformer Params MACs Results \u2193 Params MACs Results \u2193 AIDATATANG [38] Char CER Char CER Char WER Char WER AISHELL [39] AphasiaBank [30] CHiME4 [40] dev / test dev / test patients / control {dt05,et05} {simu,real} Fisher-Callhome [41] BPE WER dev / dev2 / test / devtest / evltest BPE CER BPE WER Char CER LibriSpeech 100h [14] BPE WER LibriSpeech 960h [14] BPE WER BPE WER BPE WER BPE WER Char CER Char WER FLEURS [42] GigaSpeech [43] JSUT [44] MuST-C [45] Switchboard [46] TEDLIUM2 [47] VoxForge [48] WSJ [49] dev / test dev / test dev / eval1 {dev,test} {clean,other} {dev,test} {clean,other} tst-{COMMON, HE}.en-de eval2000 (callhm / swbd) dev / test dt it / et it dev93 / eval92 46.0 46.3 44.2 30.4 43.8 \u2020126.6 48.8 20.0 116.2 11.6 45.1 10.3 39.0 42.5 147.8 12.0 46.1 10.3 36.7 10.3 35.5 13.2 35.2 13.2 35.2 14.7 15.3 30.1 8.8 11.6 20.7 / 20.9 / 19.4 / 38.3 / 38.8 \u2021 3.6 / 4.3 4.3 / 4.6 40.3 / 35.3 \u2021 7.8 / 9.5 / 12.5 / 14.8 \u2021 10.1 / 10.4 10.9 / 10.8 \u2021 12.3 / 13.6 6.3 / 17.0 / 6.6 / 17.2 \u2021 1.72 / 3.65 / 1.85 / 3.95 7.7 / 6.7 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 \u2021 6.5 / 4.1 45.4 45.7 45.7 30.8 43.2 \u2020127.4 50.1 26.1 148.9 12.1 44.2 9.9 38.5 42.7 148.9 9.9 37.7 9.9 36.2 9.9 35.0 12.6 34.7 12.6 34.7 15.5 15.5 32.0 8.8 12.1 20.5 / 20.2 / 18.7 / 37.8 / 37.6 \u2021 3.4 / 4.1 4.2 / 4.4 36.2 / 31.2 \u2021 6.8 / 8.4 / 10.8 / 13.0 \u2021 9.3 / 9.2 10.6 / 10.5 \u2021 11.8 / 13.0 6.1 / 16.7 / 6.3 / 17.0 \u2021 1.67 / 3.64 / 1.85 / 3.71 7.3 / 6.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0"}