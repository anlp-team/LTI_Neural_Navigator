{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Efficient_Sequence_Transduction_by_Jointly_Predicting_Tokens_and_Durations_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the TDT model learn the alignment between?", "answer": " The TDT model learns the alignment between an input sequence and the corresponding target sequence.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " What is the formula for computing the alignment P (At,u|x)?", "answer": " The formula for computing the alignment P (At,u|x) is \u03b1(t, u)\u03b2(t, u).", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " What does the variable Nd denote in the experiments?", "answer": " The variable Nd denotes the number of duration tokens.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " How is the alignment affected as Nd becomes larger in the experiments?", "answer": " As Nd becomes larger in the experiments, the alignment contains longer-duration tokens.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " What is the purpose of the FastEmit regularization method?", "answer": " The purpose of the FastEmit regularization method is to reduce the delay of token emission.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " What effect does a large value of \u03bb have on the FastEmit regularization method?", "answer": " A large value of \u03bb in the FastEmit regularization method can cause the model to diverge.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " How does the FastEmit regularization method scale the gradients?", "answer": " The FastEmit regularization method scales the gradients to token probabilities by 1 + \u03bb and keeps the gradients to blank probabilities unchanged.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " What is the ratio of sequence lengths mentioned in the experiments?", "answer": " The ratio of sequence lengths mentioned in the experiments is T U = 7 : 1.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " What observation is made regarding token emissions in Figures 7 and 8?", "answer": " It is observed that the first few token emissions occur rapidly with a duration set Nd = 1 but are delayed with Nd = 8.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}, {"question": " How does the training strength of \u03bb affect the delay between token emissions?", "answer": " The training strength of \u03bb affects the delay between token emissions by reducing the delay as \u03bb increases.", "ref_chunk": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}], "doc_text": "under-normalization as, \u2202LTDT \u2202hv t,u = P (v|t, u)\u03b1(t, u) P \u2032(y|x) (cid:20) \u03b2(t, u) \u2212 1 exp(\u03c3) \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 (cid:80) (cid:80) 0, d\u2208D \u03b2(t + d, u + 1)PD(d|t, u), d\u2208D\\{0} \u03b2(t + d, u)PD(d|t, u), v = yu+1 v = \u00d8 otherwise (cid:21) 18 (cid:35) (cid:125) (cid:35) (cid:125) (49) (50) (51) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Figure 7. Alignment (P (At,u|x) as a function of the duration (D) supported by the TDT model. Simulated joint (JS) trained with a larger number of duration tokens (Nd) possess align- ments with correspondingly longer gaps between time steps (t \u2208 T ) for each token prediction (u \u2208 U ) Figure 8. Alignment (P (At,u|x) as a function of the FastEmit regularization strength (\u03bb). Simulated joint (JS) trained with varying \u03bb possess alignments with a correspondingly shorter delay between each token prediction (u \u2208 U ) as compared to the baseline of \u03bb = 0 D. Analysis of Token-and-Duration Transducer Alignments TDT models are capable of learning the alignment P (At,u|x) between an input sequence (S) and the corresponding target sequence (S\u2032). This quantity represents the total probability mass that goes through the state (t, u) in the lattice. We use the definition of alignment P (At,u|x) from (Yu et al., 2021), computed as, P (At,u|x) = \u03b1(t, u)\u03b2(t, u) In the following section, we construct a set of force-alignment experiments to determine the effect of input-output sequence interactions and hyper-parameter choices. Unless stated otherwise, all following experiments are with a simulated joint tensor (JS \u2208 RT \u00d7U \u00d7(V +1+Nd)) for some input sequence (S) sampled from the normal distribution (N (0, 1)) and a target sequence (S \u2208 ZU ) generated from the discrete uniform distribution (U {1, . . . , V }), where Nd refers the the the number of duration tokens and V is the size of the vocabulary of the token set. We use the PyTorch (Paszke et al., 2019) to optimize JS using S\u2032 as the target sequence. We minimize he TDT loss defined in Eq. 9 using the Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.1 for 100 update steps. Once JS is optimized, we compute P (At,u|x) and plot the T \u00d7 U alignment matrix. We use a fixed random seed so as to reproduce the same JS and S\u2032 when T , U and V are kept constant and chose a fixed value of T = 70, U = 10, V = 5 (chosen simply to speed up convergence), number of duration tokens (Nd = 8), \u03c3 = 0.05, \u03c9 = 0.0 and fastemit \u03bb = 0.0 for the experiments unless explicitly mentioned. D.1. Effect of Durations on TDT Alignments In a TDT model, one head emits the token while another predicts the duration of the token (say D \u2208 {0, 1, . . . , (Nd) \u2212 1}) where Nd is the number of duration tokens. In the following set of experiments, we attempt to optimize JS while modifying only the duration set. Given that T >> U , and \u03c3 > 0, we expect the alignment to intuitively contain longer-duration tokens as Nd becomes larger. In Fig. 7, we see that the learned alignment precisely matches our expectations. As the Nd grows larger, the model selects longer durations, significantly reducing the number of decoding steps required. A natural effect of selecting longer tokens is that token emissions are significantly delayed compared to the baseline of D \u2208 {0, 1} which can be considered an 19 (52) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations approximation of conventional Transducer alignment with single duration step per token emitted. Note, that longer duration tokens are enabled by the large difference in the input and target sequence lengths (T = 70; U = 10), reducing to a T U = 7 : 1 ratio. This ratio of sequence lengths is slightly larger than the observed ratio of acoustic sequence length versus the corresponding sub-word encoded target text tokens in Librispeech with a sufficiently large sub-word encoding vocabulary, close to TLS \u2248 5.5 : 1. We expect that target token encoding schemes such as character-based ULS encoding will diminish this ratio of sequence lengths to approximately T U \u2248 2 : 1, thereby preventing long-duration tokens from being emitted frequently. D.2. Effect of FastEmit on Alignments One reduce the delay of token emission using FastEmit method proposed in (Yu et al., 2021). As can be seen in Section D.1, when the number of duration tokens (Nd) is large, the emission of tokens (u \u2208 U ) is delayed significantly which may hinder latency-sensitive applications. In the following section, we discuss the utilization of FastEmit (Yu et al., 2021) as a strong regularization scheme in order to prevent the model from deferring token emission to such a degree. FastEmit introduces a hyperparameter \u03bb and scales the gradients to token probabilities by 1 + \u03bb and keeping the gradients to blank probabilities unchanged. We attempt to simply train the same simulated joint (JS) optimized with different strengths of the \u03bb scaling factor for FastEmit. In Fig. 8, we find that the effect of FastEmit regularization strength constant (\u03bb) has a substantial effect on reducing the delay between token emissions. It must be noted that \u03bb > 1e-2 is not realistically applicable when training on non-synthetic data, as the strength of the regularization term will cause the model to diverge, but in this simulated setting, it has been done in order to explicitly show the effect of FastEmit on token emission delay. An important observation is that when comparing the alignment of Figure 7 (1st row) and Figure 8 (5th row), the first 5 \u223c 6 token emissions occur rapidly with the duration set Nd = 1 but are delayed by a significant number of steps for Nd = 8. FastEmit does improve the token emission of the first few tokens, however since each token presents a"}