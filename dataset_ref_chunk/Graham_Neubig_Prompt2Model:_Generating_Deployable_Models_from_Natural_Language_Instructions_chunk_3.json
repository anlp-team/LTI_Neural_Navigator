{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Prompt2Model:_Generating_Deployable_Models_from_Natural_Language_Instructions_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What strategy was used to generate a diverse dataset in the dataset generator?", "answer": " High-Diversity Few-Shot Prompting", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " How did the dataset generator prevent generating duplicate examples?", "answer": " By augmenting user-provided examples with a random sample of previously generated examples", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " What is the purpose of adjusting the sampling temperature in the dataset generator?", "answer": " To encourage diverse exploration and preserve output quality", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " What method is used to select pseudo-labels for ensuring the uniqueness of generated examples?", "answer": " Self-Consistency Decoding", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " How are API requests parallelized in the dataset generator?", "answer": " Using zeno-build", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " What type of architectures are used for model selection in the Model Retriever section?", "answer": " Encoder-decoder architectures on Hugging Face", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " How is the problem of selecting a pretrained model framed?", "answer": " As a search problem", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " What algorithm is used to compute query-model similarity scores in the model selection process?", "answer": " BM25", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " What is the default threshold for filtering out models based on size in the model selection process?", "answer": " 3GB", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}, {"question": " What tool is used to create a graphical user interface in the Web App Creation section?", "answer": " Gradio", "ref_chunk": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}], "doc_text": "state that none are a good fit for their task. We then ask the user to identify the appropriate columns for input and output from the dataset\u2019s schema. 3.3 Dataset Generator We carefully engineered our dataset generator to enable speed-optimized generation at a low-cost while creating diverse and high-quality examples. Our strategy comprises the following components: High-Diversity Few-Shot Prompting We use automated prompt engineering to generate a diverse dataset. We augment the user-provided demonstra- tion examples with a random sample of previously generated examples to promote diversity and avoid generating duplicate examples. Without this strat- egy, 120 out of 200 generated QA examples were duplicates; with it, only 25 were duplicates. Temperature Annealing We adjust the sampling temperature from low (favoring deterministic out- puts) to high (encouraging diverse exploration) pro- portionally to the number of examples already gen- erated. This modulation helps preserve output qual- ity while gradually encouraging diversity. 2https://www.deepl.com/en/docs-api Self-Consistency Decoding Given that LLM may generate non-unique or incorrect outputs for the same inputs, we use self-consistency filtering (Wang et al., 2022) to select pseudo-labels. Specifi- cally, we create a consensus output for each unique input by selecting the most frequent answer; in the case of ties, we heuristically select the shortest answer. This promotes accuracy of the generated dataset while ensuring unique examples. Asynchronous Batching API requests are par- allelized using zeno-build (Neubig and He, 2023). We use additional mechanisms, such as dynamic batch size and throttling, to optimize API usage. 3.4 Model Retriever We need to select an appropriate model to finetune. To support many tasks with a unified model interface, we presently limit ourselves to encoder- decoder architectures on Hugging Face (Wolf et al., 2020), following recent work that shows that encoder-decoder models are more data-efficient for model distillation (Calderon et al., 2023). This re- striction still leaves a large set of pretrained models to choose from, e.g. Salesforce/codet5-base for coding-related tasks (Wang et al., 2021b) or MaryaAI/opus-mt-ar-en-finetuned-ar-to-en for Arabic-to-English translation (Tiedemann and Thottingal, 2020). We frame the problem of select- ing a pretrained model as a search problem. Using the user\u2019s instruction as a query, we search against all textual descriptions of models on Hugging Face. This search task is challenging because Hug- ging Face model descriptions are sparse and con- tain lots of templatic text, often with only a few words that signify the content of the model. To address this, we follow the HyDE framework (Gao et al., 2023) and first use gpt-3.5-turbo to create a hypothetical model description given the user\u2019s instructions. We show an example of a hypotheti- cal document generated for a question-answering instruction in Figure 3. Using this description as an expanded query, we then apply the BM25 al- gorithm to compute query-model similarity scores (Robertson et al., 1995). To ensure the ease of de- ployment of the resulting model, we filter out mod- els whose size (in bytes) exceeds a user-specified threshold (set to 3GB by default). Using the in- tuition that highly-downloaded models are more likely to be high in quality, we choose the top model after ranking by: BM 25(query, model) \u00b7 log(# of Downloads + 1). Hypothetical Document Embedding---language: enlicense: apache-2.0tags:- question-answering- nlp- transformersdatasets:- natural-questions- squad--## Model DescriptionThis model is a fine-tuned version of a BERT model for question-answering tasks. It can generate answers to natural questions given context. Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. LLM Figure 3: For our model retriever, we first construct a hypothetical model description for a query, then com- pute similarity scores between that hypothetical model description and the descriptions of real models. 3.5 Training Dataset Processing We train the model by lever- aging two datasets- one generated and one re- trieved. To sidestep the challenge of making schema-specific modeling decisions (e.g. construct- ing specialized architectures for classification or generation tasks), we treat all datasets as \u201ctext-to- text\u201d problems (Raffel et al., 2020). We textualize the input columns of each dataset and prepend the user\u2019s instructions to the input to guide the model. Finetuning We concatenate the retrieved and generated datasets and shuffle them before train- ing the student model. We use the same default hyperparameters for all tasks.3 We train with the AdamW optimizer with lr = 5e-5 for 3 epochs, which takes roughly one hour for all tasks. 3.6 Evaluation Our Model Evaluator automatically evaluates mod- els for all tasks using three general-purpose met- rics: Exact Match, ChrF++ (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2019). ChrF++ bal- ances precision and recall to assess text genera- tion quality. Exact Match measures how often the model output perfectly matches the exact refer- ence. BERTScore captures semantic similarities de- spite different wordings or phrasings by comparing 3We empirically find that these default hyperparameters are effective, but we plan on implementing hyperparameter selection using generated validation data in the future. the model output and reference in the embedding space. We use XLM-R (Conneau et al., 2020) as the encoder for BERTScore to support multilingual evaluation. 3.7 Web App Creation in We finally Prompt2Model a graphical user interface that allows downstream users to interact with the trained model. This web application, built using Gradio (Abid et al., 2019), can then be easily deployed publicly on a server. provide to optional automatically an step create 4 Experimental Setup Tasks As a proof-of-concept, we test our sys- tem\u2019s ability to learn a model for three tasks: \u2022 Machine Reading Question Answering: We first consider a common use case where pretrained models and training datasets are plentiful. We use SQuAD (Rajpurkar et al., 2016) as ground truth to evaluate this setting. Japanese NL-to-Code: Code generation from Japanese-language queries is a challenging sce- nario where prior work exists but no annotated data or pretrained models are available. We use MCoNaLa (Wang et al., 2023) for evaluation. \u2022 Temporal Expression Normalization: We finally consider a task where there are no pretrained models or"}