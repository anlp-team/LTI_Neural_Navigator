{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Justine_Cassell_How_About_Kind_of_Generating_Hedges_using_End-to-End_Neural_Models?_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main difference between BERTScore and traditional metrics in evaluating text generation?", "answer": " BERTScore computes similarity across semantic space and has a strong correlation with human judgment at the segment level.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " How does BARTScore approach text generation evaluation?", "answer": " BARTScore formulates text generation evaluation as a task from pretrained language models in an unsupervised fashion.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " What does Perplexity measure in the context of language models?", "answer": " Perplexity quantifies the level of uncertainty when a language model generates a new token.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " Which libraries were used for training control in the implementation of the models?", "answer": " The Transformer library and Pytorch-Lightning library.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " What optimizer was applied to the models during training?", "answer": " AdamW with a learning rate of 10e-5.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " How many epochs were the models trained for, and what mechanism was used to prevent overfitting?", "answer": " The models were trained for 10 epochs with an Early-stopping mechanism on validation loss.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " What language models were used in the study?", "answer": " The base version of the BART model, the small version of BlenderBot, and the small version of DialoGPT.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " What decoding strategy was used for reranking in the study?", "answer": " Beam search.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " How was repetition prevented during generation, and what penalty was applied to repeated 2-gram units?", "answer": " Repetition was prevented by allowing 2 grams to occur only once, and a repetition penalty of 1.2 was applied.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}, {"question": " What GPU was used for fine-tuning the models?", "answer": " Nvidia Quadro RTX 8000 GPU.", "ref_chunk": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}], "doc_text": "wise cosine similarity for each generated word vec- tor and each word in the reference, then the recall of the generated sequences is calculated. BERTScore is distinct from the previous two metrics in that it computes similarity across semantic space and has been shown to have a strong correlation with human judgment at the segment level. BARTScore (Yuan et al., 2021) formulates the text generation evaluation as a text generation task from pretrained language models in an unsuper- vised fashion. When the generated text is better, the training model will get a higher score by convert- ing the generated text to reference or source text. BART score can be applied to different evaluations (e.g., informativeness, coherence, and factuality). Perplexity (Chen et al., 1998) calculates lan- guage model perplexity. Perplexity quantifies the level of uncertainty when an LM generates a new token. C Implementation Details The implementation of all models was based on the Transformer library3, in addition, the Pytorch- Lightning4 library was used for training control. We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e\u22125. All the models are trained with 10 epochs but with an Early-stopping mechanism on validation loss, which means when the validation loss remains for 2 epochs, the training will stop to prevent overfit- ting. We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. For the reranking method, we use beam search as our decoding strategy. To prevent repetition, we allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning 50 70Percentage of labels (%) DialoGPT IDE 60 DialoGPT (Reranking) BART BART (Reranking) 10 0 30 BlenderBot (Reranking) Percentage of labels per category IDS Human IDA IDQ 40 BlenderBot 20 Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human. IDA: Apologizer; IDE: Extender; IDQ: Propositional hedges; IDS: Sub- jectivizer (as defined in Section 2.1) cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete con- figuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decod- ing strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation. D Figures Figure 4: Hedge subcategories distribution in mod- els\u2019 outputs compared with human."}