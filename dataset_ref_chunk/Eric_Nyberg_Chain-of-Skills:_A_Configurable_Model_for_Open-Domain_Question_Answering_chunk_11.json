{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Chain-of-Skills:_A_Configurable_Model_for_Open-Domain_Question_Answering_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the abbreviation for the model by Singh et al. in 2021?", "answer": " EMDR2", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " How many first-hop passages are kept in the HotpotQA process after reranking the combined set?", "answer": " 30", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " What model was the FiE model re-implemented from according to the text?", "answer": " Electra-large", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " What is the learning rate used for training the NQ model?", "answer": " 5e-5", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " How many steps is the model trained for in the OTT-QA setup?", "answer": " 5000", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " What are the objectives for training the path reranker in HotpotQA?", "answer": " passage ranking, supporting sentence prediction, answer span extraction", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " How many positive and negative passages are sampled during training from the top 100 chains returned by COS?", "answer": " 0-2 positive passages and 0-2 negative passages", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " What is the max sequence length used during training of the reader model?", "answer": " 512", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " What model is used as initialization for both the path reranker and reader models?", "answer": " Electra-large", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}, {"question": " How does the model rank top 100 passage chains during inference time?", "answer": " Structure preserved, sum scores of two passages, subtract dynamic threshold score, sort based on final score", "ref_chunk": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}], "doc_text": "2020) EMDR2 (Singh et al., 2021) YONO (Lee et al., 2021) UnitedQA (Cheng et al., 2021) R2-D2 (Fajcik et al., 2021) FiE (Kedia et al., 2022) FiE (ours implementation) COS + FiE #Params EM 770M 51.4 330M 51.8 770M 54.4 440M 52.5 440M 53.2 54.7 1.87B 1.29B 55.9 330M 58.4 330M 56.3 330M 56.4 For HotpotQA, single retrieval and linking are used jointly to find the first-hop passages where we keep top 200 passages from single retrieval and top 5 passage from each linked question entity. The combined set is then reranked to keep the top 30 first-hop passages. Then expanded query retrieval and passage entity linking are applied to these 30 passages, where we keep top 50 passages from ex- panded query retrieval and top 2 passages from every linked passage entity. Next, another round of reranking is performed on the newly collected pas- sages and then we sort the evidence passage chains based on the final aggregated score and keep top 100 chains. Since all of the baselines on HotpotQA adopt a large passage path reranker, we also trained such a model following (Zhu et al., 2021) (dis- cussed in Appendix C) to rank the top 100 passage Table A2: End-to-end QA Exact Match score on NQ chains to get the top 1 prediction. The hyperparameters for OTT-QA and Hot- potQA inference are selected such that the total number of evidence chains are comparable to previ- ous works (Ma et al., 2022a; Xiong et al., 2021b). C Question Answering Results C.1 Training Details We follow descriptions in (Kedia et al., 2022) for re-implementation of FiE model and the model is initialized from Electra-large (Clark et al., 2020). For NQ, we train the model for 5,000 steps with the effective batch size of 64, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 15, the max question length of 28, the max sequence length of 250, and 10 global tokens. Note that although Kedia et al. (2022) reports that training with 15,000 steps leads to better performance, we actually found it to be the same as 5,000 steps. Thus we train with fewer steps to save computation. For OTT-QA, we used the same set-up of hyperparameters except that the max sequence length is changed to 500. For HotpotQA path reranker and reader, we pre- pare the input sequence as follows: \"[CLS] Q [SEP] yes no [P] P1 [P] P2 [SEP] \", where [P] is a special token to denotes the start of a passage. Then the input sequence is encoded by the model and we extract passage start tokens rep- resentations p1, ...pm and averaged sentence em- beddings for every sentence in the input s1, ...sn to represent passages and sentences respectively. The path reranker is trained with three objectives: passage ranking, supporting sentence prediction and answer span extraction, as we found the latter two objectives also aid the passage ranking training. For answer extraction, the model is trained to pre- dict the start and end token indices as commonly done in recent literature (Xiong et al., 2021b; Zhu et al., 2021). For both passage ranking and support- ing sentence prediction, the model is trained with the ListMLE loss (Xia et al., 2008). In particular, every positive passage in the sequence is assigned a label of 1, and every negative passage is assigned 0. To learn a dynamic threshold, we also use the [CLS] token p0 to represent a pseudo passage and assign a label of 0.5. Finally, the loss is computed as follows: Lp = \u2212 m (cid:88) i=0 log (cid:80) exp(piWp) p\u2032\u2208P\u222a{pi} exp(p\u2032Wp) . where P contains all passages representations that have labels smaller than pi. Wp \u2208 Rd are learnable weights and d is the hidden size. In other words, the model learns to assign scores such that positive passages > thresholds > negative passages. The supporting sentence prediction is also trained us- ing Equation 9. Overall, use the following loss weighting: Lpath = Lp + La + 0.5 \u00d7 Ls where La is the answer extraction loss and Ls is the supporting sentence prediction loss. (9) (10) During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not pre- served and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure pre- served. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence predic- tion. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5 \u00d7 Ls The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model di- rectly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model\u2019s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on"}