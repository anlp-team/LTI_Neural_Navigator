{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many hours of data are in the resulting dataset for training?", "answer": " 896 hours ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " What is the range of utterance duration in the dataset?", "answer": " 5 to 15 seconds ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " How many utterances from different speakers were selected for human evaluation?", "answer": " 40 utterances ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " What optimizer is used for training the quantizer?", "answer": " Adam optimizer ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " How many GPUs are used to train all models?", "answer": " 4 RTX A6000 GPUs ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " What sampling technique is used for inference?", "answer": " Nucleus sampling ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " What does ASR Error Rate stand for?", "answer": " Automatic Speech Recognition Error Rate ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " What platform was used for human evaluation?", "answer": " Amazon Mechanical Turk (MTurk) ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " What metric is used for the evaluation of prosody diversity in the text-to-speech system?", "answer": " Prosody FID Score (P-FID) ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}, {"question": " How many utterances are randomly sampled to generate syntheses for Speaker Similarity Score evaluation?", "answer": " 50k utterances ", "ref_chunk": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}], "doc_text": "YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets. 4.2 Training and Model Con\ufb01guration We use \u03b3 = 0.25 in Equation 4 and \u03bb = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with \u03b21 = 0.5, \u03b22 = 0.9. For the transformer, we use Adam op- timizer with \u03b21 = 0.9, \u03b22 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 \u00d7 10\u22124 to 0. All models are trained using 4 RTX A6000 GPUs in b\ufb02oat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8. Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the of\ufb01cial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modi\ufb01ed from the of\ufb01cial imple- mentation and changed their \ufb01xed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution. Model Variants. To test the ef\ufb01cacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently 3https://github.com/jik876/hi\ufb01-gan 4https://github.com/NVIDIA/DeepLearningExamples for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS. 4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better re\ufb02ects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses. Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech\u2019s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% con\ufb01dence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr\u00b4echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, re\ufb02ecting both naturalness and diversity. A pretrained classi\ufb01er is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insuf\ufb01cient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classi\ufb01er released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lot\ufb01an and Busso 2019). We use the input before the \ufb01nal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shuf\ufb02ed speaker references. Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine 5https://zenodo.org/record/4630406#.YoT0Ji-B1QI Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN. Method Code size (#groups) RCER (%) MOS-Q (95% CI) MOS-N (95% CI) GT n/a n/a 3.66(.06) 3.81(.05) HF-GAN n/a 12.8 3.47(.06) 3.62(.06) MQTTS Quant. 1024 (1) 65536 (1) 160 (4) 160 (8) 56.5 59.9 19.7 14.2 3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06) 3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06) similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID. Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021)"}