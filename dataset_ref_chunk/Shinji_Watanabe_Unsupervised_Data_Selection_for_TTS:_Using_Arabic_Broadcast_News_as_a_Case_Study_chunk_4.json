{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Unsupervised_Data_Selection_for_TTS:_Using_Arabic_Broadcast_News_as_a_Case_Study_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What model was used in the study for TTS training?,        answer: FastSpeech2 with the Transformer-TTS as the teacher model    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What was the impact of vowelization on speech generation according to Table 3?,        answer: Vowelization improved results considerably    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " How did the model with a larger reduction factor affect training convergence and attention alignment?,        answer: It achieved faster training convergence and easier attention alignment    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " Which models outperformed others without pre-training, according to the results?,        answer: 5V and 7V    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What does the low WER for the ground-truth indicate?,        answer: The anchor speaker speaks clearly and the recording setup is good    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What type of labeling slightly outperformed the automatic labeling based on the results?,        answer: Manual labeling    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What is the quality comparison between male and female speaker datasets according to Table 5?,        answer: Male speaker dataset has higher quality    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What efficiency improvement was observed with the use of PWG in the subjective evaluation?,        answer: Applying the neural vocoder brought a significant improvement in naturalness and intelligibility    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What was the rating for naturalness and intelligibility with model 7V and PWG?,        answer: Naturalness: 4.9 \u00b1 0.05, Intelligibility: 4.4 \u00b1 0.06    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}, {"question": " What was the MOS score for intelligibility and naturalness achieved in the study?,        answer: Intelligibility: 4.4/5, Naturalness: 4.2/5    ", "ref_chunk": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}], "doc_text": "9.6 \u00b1 1.5 9.0 \u00b1 1.0 9.2 \u00b1 1.1 N/A X 14.0 29.8 14.1 27.2 9.7 21.6 5.5 9.6 4.4 7.0 4.4 9.6 3.9 32.3 5.7 14.7 1.3 34.0 54.4 35.3 51.1 20.4 45.2 15.4 19.9 9.5 22.8 10.8 26.1 9.1 53.9 14.1 34.0 3.0 X X X X X X X N/A Table 3. The comparison of detailed CER for the male speaker between w/ or w/o vowelization. The model was FastSpeech2 with the Transformer-TTS as the teacher model and the reduction factor was set to 1. ID 7V 7 Model w/ vowelization w/o vowelization Sub. 11 160 Ins. Del. CER [%] 2 45 32 170 3.9 32.3 4.2. Results and Analysis 4.2.1. Objective Evaluation For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22]. The objective evaluation in Table 2 shows that vowelization im- proved results considerably. Table 3 shows the impact of voweliza- tion for the FastSpeech2 models. The generated speech in the un- vowelized text compared to the vowelized text includes a lot of dele- tion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier at- tention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This re- sult indicated that the amount of training data required for non-AR models is much smaller than AR models. We notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good. To check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling pro- vided by the annotators, we trained a FastSpeech2 model with the \ufb01ne-tuned Transformers as a teacher model. Table 4 shows a com- parison between the ef\ufb01ciency of the four models trained on differ- ent samples, which shows that the manual labeling slightly outper- formed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection. For checking the ef\ufb01ciency of the data selection and the perfor- mance of the mechanism presented, we trained a FastSpeech2 model with the \ufb01ne-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality. Table 4. Comparison between the manual and the automatic labeling re- sults for the male speaker. The model was FastSpeech2 with the Transformers as the teacher model. The reduction factor was set to 1 and vowelization was performed. ID 10 10V 11 7V Model MOSNet wv-MOS DNSMOS w/ manual labels WER [%] CER [%] MCD [dB] 12.6 \u00b1 1.1 10.0 10.9 \u00b1 1.0 7.0 11.2 \u00b1 0.65 4.0 8.8 \u00b1 0.9 3.9 22.0 18.0 11.0 9.13 Table 5. Objective evlaution using FastSpeech2 with the Tacotron2 as the teacher model with vowelization. ID 5V 11V Dataset WER [%] CER [%] MCD [dB] 8.3 \u00b1 1.6 7.9 \u00b1 1.3 Male Female 9.5 13.0 4.4 7.0 Table 6. Subjective evaluation results with 95% con\ufb01dence interval, where reduction factor \u201cR\u201d, intelligibility \u201cInt.\u201d and naturalness \u201cNat.\u201d. ID 7V 7 8V 12 - Model FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) FastSpeech2 (Trans.) w/ PWG Ground-truth R 1 1 3 1 N/A Vowel. X Int. 4.1 \u00b1 0.06 3.5 \u00b1 0.08 3.9 \u00b1 0.07 4.4 \u00b1 0.06 4.9 \u00b1 0.05 Nat. 4.0 \u00b1 0.06 3.2 \u00b1 0.08 3.8 \u00b1 0.07 4.2 \u00b1 0.06 4.9 \u00b1 0.05 X X N/A 4.2.2. Subjective Evaluation Finally, we conducted the subjective evaluation using MOS on nat- uralness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 sam- ples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten. Table 6 shows that with reduction factor 1, vowelization signif- icantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can con\ufb01rm that a larger reduc- tion factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that ap- plying the neural vocoder brought a signi\ufb01cant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very famil- iar with the tone of the speakers in the broadcast news. 5. CONCLUSION This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/\ufb01ne-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our ap- proach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1. 6. REFERENCES [1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Lib- rispeech for text-to-speech,\u201d Proc. Interspeech 2019. [2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,"}