{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_PWESuite:_Phonetic_Word_Embeddings_and_Tasks_They_Facilitate_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of using the last hidden state of an LSTM-based model in generating word embeddings?,answer: To produce a vector for each input word.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What input is used in generating word embeddings?,answer: Characters, IPA symbols, and articulatory feature vectors.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What is the function that produces a vector for each input word not yet trained to encode phonetic information?,answer: Function \u0192.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What is the purpose of the differentiable loss defined in the text?,answer: To force the embeddings to be spaced in the same way as the articulatory distance.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What loss function is employed to relax the constraint on the spacing of embeddings while preserving the structure?,answer: Triplet margin loss.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What does the triplet margin loss consider in its formulation?,answer: It considers an anchor, a positive example, and a negative example.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " How does the model focus on phonemes that are potentially more useful in summarizing phonetic information?,answer: By applying attention to all hidden states extracted from the last layer of the LSTM encoder.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What common approach is mentioned in the text for learning word embeddings?,answer: Training on the masked language model objective popularized by BERT.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What is the primary contribution of the evaluation suite introduced in the text?,answer: The embedding evaluation metrics.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}, {"question": " What is the purpose of intrinsic evaluation in the context of phonetic word embeddings?,answer: To efficiently compute and measure the usefulness of the embeddings for a particular task.", "ref_chunk": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}], "doc_text": "capturing the intuition that /p\u00e6t/ and /b\u00e6t/ are pho- netically closer than /p\u00e6t/ and /h\u00e6t/, for example. 3.3.2. Metric Learning As one means of generating word embeddings, we use the last hidden state of an LSTM-based model. We use characters \uef23C, IPA symbols \uef23P (Section 2) and articulatory feature vectors as the input. We discuss these choices and especially their effect on performance and transferability in Section 5.3. We now have a function \u0192 that produces a vector for each input word. However, it is not yet trained to produce vectors encoding phonetic information. (12) ) (13) (14) We, therefore, define the following differentiable loss where A is the articulatory distance. Ldist. = 1 |W| (cid:88) \uebf8\uebe1\u2208W \uebf8b\u223cW (cid:128) ||\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8b)||2 \u2212 A(\uebf8\uebe1, \uebf8b)(cid:138)2 This forces the embeddings to be spaced in the same way as the articulatory distance (A, Sec- tion 3.3.1) would space them. Metric learning (learning a function to space output vectors sim- ilarly to some other metric) has been employed previously (Yang and Jin, 2006; Bellet et al., 2015; Kaya and Bilge, 2019) and was used to train acous- tic embeddings by Yang and Hirschberg (2019). 3.3.3. Triplet Margin loss While the previous approach forces the embed- dings to be spaced exactly as by the articulatory distance function A, we may relax the constraint so only the structure (ordering) is preserved. This is realized by triplet margin loss: Ltriplet = m\uebe1x \uf8f1 \uf8f2 \uf8f3 0 \u03b1 + |\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)| \u2212|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)| We consider all possible ordered triplets that of A(\uebf8\uebe1, \uebf8p) < A(\uebf8\uebe1, \uebf8n). We refer to \uebf8\uebe1 as the anchor, \uebf8p as the positive example, and \uebf8n as the negative example. We then minimize Ltriplet over all valid triplets. This allows us to learn \u03b8 for an embedding function \u0192\u03b8 that preserves the local neighbourhoods of words defined by A(\uebf8, \uebf8\u2032). In addition, we modify the function \u0192\u03b8 by applying attention to all hidden states extracted from the last layer of the LSTM encoder. This allows our model to focus on phonemes that are potentially more useful when trying to summarize the phonetic information in a word. A related approach was used by Yang and Hirschberg (2019) to learn acoustic word embed- dings. Although contrastive learning is a more intuitive approach, it yielded only negative results: (cid:0)exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8p)|2)(cid:1)/(cid:0)(cid:80) exp(|\u0192\u03b8(\uebf8\uebe1) \u2212 \u0192\u03b8(\uebf8n)|2)(cid:1). Though metric learning and triplet margin loss have been applied previously to similar applica- tions, we are the first to apply them using articula- tory features and articulatory distance. (\uebf8\uebe1, \uebf8p, \uebf8n) distinct words such 3.4. Phonetic Language Modeling To shed more light into the true landscape of pho- netic word embedding models, we describe here a model which did not perform well on our suite of tasks (in contrast to other models). A common (15) (16) way of learning word embeddings now is to train on the masked language model objective, popularized by BERT (Devlin et al., 2019). We input articula- tory features from PanPhon into several successive Transformer (Vaswani et al., 2017) encoder layers and a final linear layer that predicts the masked phone. Positional encoding is added to each in- put. We prepend and append [CLS] and [SEP] tokens, respectively, to the phonetic transcriptions of each word, before we look up each phone\u2019s Pan- Phon features. We use [CLS] pooling\u2013taking the output of the Transformer corresponding to the first token\u2013to extract a word-level representation. Un- like BERT, we do not train on the next sentence prediction objective. In addition, we do not add an embedding layer because we are not interested in learning individual phone embeddings but rather wish to learn a word-level embedding. 4. Evaluation Suite (key contribution) We now introduce the embedding evaluation met- rics of our suite, the primary contribution of this paper. We draw inspiration from evaluating seman- tic word embeddings (Bakarov, 2018) and work on phonetic word embeddings (Parrish, 2017). In some cases, the distinction between intrinsic and extrinsic evaluations is tenuous (e.g., retrieval and analogies). The main characteristic of intrinsic eval- uation is that they are efficiently computed and are In contrast, not part of any specific application. extrinsic evaluation metrics directly measure the usefulness of the embeddings for a particular task. We evaluate with 9 phonologically diverse lan- guages: Amharic,\u2217 Bengali,\u2217 English, French, Ger- man, Polish, Spanish, Swahili, and Uzbek. Lan- guages marked with \u2217 use non-Latin script. The non-English data (200k tokens each) is from CC- 100 (Wenzek et al., 2020; Conneau et al., 2020), while the English data (125k tokens) is from the CMU Pronouncing Dictionary (Group, 2014). 4.1. Intrinsic Evaluation 4.1.1. Articulatory Distance The unifying desideratum for phonetic embeddings is that they should capture the concept of sound similarity. Recall from Section 2 that phonetic word embeddings are a function \u0192 : \uef23\u2217 \u2192 Rd. In the vector space of Rd, there are two widely used notions of similarity S. The first is the negative L2 distance and the other is the cosine similarity. Consider three words \uebf8, \uebf8\u2032 and \uebf8\u2032\u2032. Using either metric, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) yields the embedding simi- larity between \uebf8 and \uebf8\u2032. On the other hand, since we have prior notions of similarity SP between the words, e.g., based on a rule-based function, we can use this to represent the similarity between the words: SP(\uebf8, \uebf8\u2032). We want to have embeddings \u0192 such that S\u25e6\u0192 produces results close to SP. There are at least two ways to verify that the similarity results are close. First is exact equality. For exam- ple, if SP(\uebf8, \uebf8\u2032) = 0.5, SP(\uebf8, \uebf8\u2032\u2032) = 0.1, we want S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) = 0.5, S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)) = 0.1. We can measure this using Pearson\u2019s correlation coef- ficient between S\u25e6\u0192 and SP. On the other hand, we may consider only the relative similarity values. Fol- lowing the previous example, we would only care that S(\u0192 (\uebf8), \u0192 (\uebf8\u2032)) > S(\u0192 (\uebf8), \u0192 (\uebf8\u2032\u2032)). In this case we use Spearman\u2019s correlation coefficient between S \u25e6 \u0192 and SP."}