{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the visual prefix in the caption generation task?,        answer: The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " What does the captioning loss Lc measure?,        answer: The captioning loss Lc measures the log likelihood of a caption conditioned on its image.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " How is the InfoNCE loss computed for image-text retrieval?,        answer: The InfoNCE loss for image-text retrieval is computed by calculating the normalized cosine similarity between the image and text embeddings using linear mappings Wt and Wi.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " What is the purpose of concatenating distinct examples together in image captioning?,        answer: The purpose of concatenating distinct examples together in image captioning is to encourage the model to attend more explicitly to images and learn to focus on the correct image within a sequence.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " What language model is used in the architecture described?,        answer: The publicly available OPT model with 6.7B parameters is used as the language model in the architecture.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " What is the visual model used for producing strong visual representations?,        answer: A pretrained CLIP ViT-L/14 model is used as the visual model for producing strong visual representations.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " How are the models implemented in the architecture?,        answer: All models are implemented in PyTorch v1.12 and trained mixed-precision with bfloat16 to ensure memory and compute efficiency.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " What optimizer is used during training?,        answer: The Adam optimizer with a learning rate of 0.0003 and warmup of 100 steps is used during training.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " What are the hyperparameters \u03bbc and \u03bbr used for?,        answer: The hyperparameters \u03bbc and \u03bbr represent the captioning and retrieval loss weights respectively in the architecture.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}, {"question": " How are the linear mappings and [RET] embedding vector updated during training?,        answer: During training, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates as \u03b8 and \u03d5 are frozen.    ", "ref_chunk": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}], "doc_text": "captioning as the task of generating text tokens con- ditioned on a visual prefix. The visual prefix is the output of the image-to-text mapping layer, v\u03d5(y)T Wc, which is prepended to the caption. The log likelihood of caption x (tokenized as (s1, . . . , sT )) conditioned on its image y is: lc(x, y) = T (cid:88) log p\u03b8(st|v\u03d5(y)T Wc, s1, . . . , st\u22121) t=1 3.2. Translating Between Image-and-Text To integrate vision and language, we learn translation param- eters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. The captioning loss Lc is then the negative log likelihood of all samples in a batch of N image-text pairs: Lc = \u2212 1 N N (cid:88) lc(xi, yi) i=1 Mapping image-to-text. We learn a linear mapping Wc \u2208 Rm\u00d7kd which maps visual embeddings v\u03d5(y) from the visual model for image y as v\u03d5(y)T Wc \u2208 Rk\u00d7d (af- ter reshaping kd to k \u00d7 d). This represents a sequence of Image-text retrieval. In addition to image captioning, we train our model to retrieve images conditioned on text (and Image-text retrieval has been used to learn vice versa). 3 (1) Grounding Language Models to Images for Multimodal Inputs and Outputs [RET] silhouette of a plane against the sunset <pad> cute cat sitting on a scooter scooter <img1> a Combined Groundtruth Caption sunset Caption #2 [RET] Image Captioning Input Caption Tokenizer Tokenizer Generated Text(next token prediction) Generated Text(next token prediction) LLM Output Embeddings(seq_len, 4096) of of cute cat sitting on a scooter <img2> Image #2 ... Cross Entropy Loss Visual Encoder Visual Encoder Visual Encoder LLM silhouette of a plane flying into the sun <pad> cat on a motorcycle Input Image the sunset InfoNCE Loss Image and Caption Inputs cute silhouette of a plane against Frozen Model ... ... silhouette of a plane against the sunset Input Embeddings(seq_len, 4096) Image #1 Cross Entropy Loss silhouette silhouette [RET] silhouette of a plane against the Loss Caption #1 Linear Layer Image-Text Retrieval Figure 2. Overview of the FROMAGe architecture. FROMAGe is a model trained on image-text pairs for image captioning and image-text retrieval. It is capable of processing arbitrarily interleaved image and text inputs, and producing interleaved images and text as outputs. joint visual and language representations (Jia et al., 2021; Radford et al., 2021), enabling cross-modality search from text descriptions. Underlying the approach is contrastive learning (Chopra et al., 2005) with the InfoNCE loss (Oord et al., 2018). Given a caption x and its paired image y, we extract the output of the last hidden layer of the LLM for the [RET] token, h\u03b8(x), and the output of the visual encoder for the image, v\u03d5(y). The normalized cosine similarity for the image and text embeddings can be computed with the learnt linear mappings Wt, and Wi (described in Sec. 3.2): sim(x, y) = (h\u03b8(x)T Wt)(v\u03d5(y)T Wi)T \u2225h\u03b8(x)T Wt\u2225\u2225v\u03d5(y)T Wi)T \u2225 We minimize the InfoNCE loss for text-to-image (t2i) and image-to-text (i2t) retrieval over a batch of N text-image pairs (xi, yi), where each example is treated as a positive pair, and other in-batch examples as negatives: Lt2i = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(xi, yi)/\u03c4 ) j=1 exp(sim(xi, yj)/\u03c4 ) (cid:80)N (cid:33) Li2t = \u2212 1 N N (cid:88) i=1 (cid:32) log exp(sim(yi, xi)/\u03c4 ) j=1 exp(sim(yi, xj)/\u03c4 ) (cid:80)N (cid:33) Similar to previous work (Jia et al., 2021; Radford et al., 2021), \u03c4 is a learnable temperature parameter. The final training loss is a weighted sum of the captioning loss (Eq. 1) and the retrieval losses (Eq. 2 and 3): (2) (3) lion image-text pairs.3 To encourage the model to attend more explicitly to images, we randomly concatenate distinct examples together (with probability of 0.5 to concatenate) for the image captioning task (Fig. 2, left). We found this helpful in training the model to attend to the correct image within a sequence (detailed analysis in the appendix). We use the publicly available OPT model (Zhang et al., 2022) with 6.7B parameters as our LLM. Past work indi- cates that findings at the 6.7B scale are likely to generalize to larger model sizes (Dettmers et al., 2022), and large enough to exhibit the few-shot and in-context learning abilities that we are interested in (Radford et al., 2019). For the visual model, we use a pretrained CLIP ViT-L/14 model (Radford et al., 2021) for its ability to produce strong visual represen- tations for vision-and-language tasks (Merullo et al., 2022). All models are implemented in PyTorch (Paszke et al., 2019) v1.12 and trained mixed-precision with bfloat16 (Abadi et al., 2016). As most of the model parameters (97.0%) are frozen, our method is memory and compute efficient: we backpropagate through the frozen LLM and visual model, but only compute gradient updates for the 3 trainable linear mappings and [RET] embedding (see Sec. 3.3). Our mod- els are trained with a batch size of 180 for 1 epoch (18000 it- erations) on 1 A6000 GPU (clock time of 24 hours). We use the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 0.0003 and warmup of 100 steps. The loss weights \u03bbc and \u03bbr are set to 1 and we use a visual prefix length of k = 1 and retrieval embedding dimension q = 256, and em- bedding dimension d = 4096 (inherited from OPT-6.7B). L = \u03bbcLc + \u03bbr(Lt2i + Li2t) \u03bbc and \u03bbr are hyperparameters representing captioning and retrieval loss weights respectively. Since \u03b8 and \u03d5 are frozen, only the linear mappings Wc, Wt, and Wi, and the [RET] embedding vector receive gradient updates. 3.4. Data and Implementation Details 4. Experiments The most interesting capabilities of FROMAGe emerge in situations with both image-and-text inputs and image-and- text outputs, such as multimodal dialogue (Fig. 1). As there does not exist comprehensive benchmarks for these specific tasks, we focus evaluation on image retrieval and image- We (CC3M) dataset (Sharma et al., 2018) consisting of 3.3 mil- train on the Conceptual"}