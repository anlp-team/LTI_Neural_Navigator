{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploring_the_Integration_of_Speech_Separation_and_Recognition_with_Self-Supervised_Learning_Representation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What impact did joint fine-tuning have on the Word Error Rates (WERs) in both anechoic and reverberant conditions?", "answer": " Joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " Why was the degradation less severe for the MVDR beamforming compared to TF-GridNet-based complex spectral mapping?", "answer": " The degradation was less severe for the MVDR beamforming as its output is constrained to be distortion-less.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What were the results of the multi-channel TF-GridNet in the anechoic case without fine-tuning?", "answer": " The multi-channel TF-GridNet achieved an SDR of 26.43 dB and a WER of 3.2% without fine-tuning.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What happened to the separation performance of the multi-channel TF-GridNet after joint fine-tuning?", "answer": " The separation performance dropped to 15.28 dB after joint fine-tuning.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What datasets were used for the experimental results presented in Table 2?", "answer": " The experimental results presented in Table 2 were based on the WHAMR! dataset.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " Which model outperformed the mask-based MVDR beamformer on the spatialized WSJ0-2mix?", "answer": " The monaural TF-GridNet outperformed the mask-based MVDR beamformer.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What was the best model overall before fine-tuning in the noisy/reverberant condition?", "answer": " The multi-channel TF-GridNet reached the best signal-level metrics before fine-tuning.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What were the performance metrics of the monaural TF-GridNet in the noisy/reverberant conditions?", "answer": " SDR of 9.07 dB and WER of 18.3% in noisy anechoic condition, SDR of 9.07 dB and WER of 18.3% in noisy reverberant condition.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What was the main observation regarding the ASR model after fine-tuning for speech separation?", "answer": " The ASR model improved its performance with fine-tuning but did not reach the WER of the joint fine-tuning model.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}, {"question": " What does the experiment in the paper focus on and what were the main challenges?", "answer": " The paper investigates the integration of speech separation, SSLR extraction, and ASR under various conditions. The main challenges include joint fine-tuning and balancing separation performance with WER.", "ref_chunk": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}], "doc_text": "interesting finding, joint fine-tuning further reduced the WERs in both anechoic and reverberant conditions while degrad- ing the separation performance. This degradation was less se- vere for the MVDR beamforming as its output is constrained to be distortion-less. Meanwhile, TF-GridNet-based unconstrained com- plex spectral mapping faced severe performance degradation, de- spite the better WER. In the anechoic case, the multi-channel TF- GridNet can achieve an SDR of 26.43 dB and a WER of 3.2% with- out fine-tuning. However, the separation performance dropped to 15.28 dB after joint fine-tuning. In detail, we observed buzzy arti- facts in the intermediate separated signals3. 3.4. Results on Noisy Multi-channel Speech Separation In this section, we present our experimental results of the WHAMR! dataset, which are summarized in Table 2. In the top panel, we re- port the performance of monaural TF-GridNet on both noisy ane- choic and reverberant conditions. As with the results on the spa- tialized WSJ0-2mix, the monaural TF-GridNet outperformed the mask-based MVDR beamformer integrated with weighted predic- tion error dereverberation [44]. The difference is even more sig- nificant due to the limitation of the number of microphones and noisy/reverberant characteristics of the data. The best model overall is again the multi-channel TF-GridNet, which reached the best signal-level metrics before fine-tuning. Af- ter joint fine-tuning, the SDR decreased significantly, but the WER 3Examples of spectrograms and audio signals are available online: https://yoshikimas.github.io/mimo-iris. October 22-25, 2023, New Paltz, NY Table 2: Separation and WER results on WHAMR!. Noisy/Anechoic Noisy/Reverberant SDR [dB] WER (%) SDR [dB] WER (%) Monaural TF-GridNet\u22c6 9.27 14.5 9.07 18.3 Two-channel MIMO-Speech [40] Time-domain [45] MVDR (proposed) TF-GridNet (proposed) - ASR-only fine-tuning - w/o fine-tuning - -1.42 9.11 13.12 - 42.2 2.3 4.4 6.5 2.27 - -1.30 7.84 11.05 28.9 20.9 44.4 2.5 6.5 10.5 \u22c6 The monaural TF-GridNet was not jointly fine-tuned. improved by over 400% relative factor in the noisy/reverberant con- dition. The performance is outstanding with WERs of 2.3% and 2.5% in anechoic and reverberant conditions, respectively, which are close to the performance achieved on the clean WSJ dataset. We also fine-tuned the ASR model while freezing the separation model, and its results are in the second bottom row of Table 2. While it outperformed the model without fine-tuning, its WER did not reach that of the joint fine-tuning model. This result confirms the advan- tage of the joint fine-tuning of both front-end and back-end. We emphasize that the ASR performance without fine-tuning still out- performed the previous MIMO-Speech [44] and the cascade combi- nation of the time-domain speech separation and ASR models [45]. 4. CONCLUSION In this paper, we investigated the integration of speech sepa- ration, SSLR extraction, and ASR with well-established beam- forming techniques as well as the latest SotA techniques includ- ing TF-GridNet. Our experiments were perfromed under ane- choic/reverberant and clean/noisy conditions using the spatialized WSJ0-2mix and WHAMR! datasets. In detail, we explored how both separation performance and WER are affected by joint fine- tuning. Our experimental results show that the purely DNN- based speech separation method, TF-GridNet-based complex spec- tral mapping, can considerably outperform the mask-based MVDR beamforming preferred as an ASR front-end. Joint fine-tuning de- graded the separation performance while significantly improving the WER, which is inconsistent with the tendency reported in a speech enhancement paper [32]. Our future work should focus on how this degradation can be prevented, e.g. by using continual learning strategies. Overall our best system, based on multi-channel TF-GridNet, WavLM, and E2E ASR, was able to reach performance on par with the one achieved on clean, single-speaker WSJ [33]. 5. ACKNOWLEDGEMENTS Y. Masuyama was partially supported by JSPS KAKENHI Grant Numbers JP21J21371 and JST CREST Grant Number JP- MJCR19A3. X. Chang, Z.-Q. Wang, and W. Zhang used the Bridges2 system at PSC and Delta system at NCSA through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coordina- tion Ecosystem: Services & Support (ACCESS) program. S. Cor- nell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 6. REFERENCES [1] D. Raj and et al., \u201cIntegration of speech separation, diariza- tion, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in Proc. SLT, 2021, pp. 897\u2013904. [2] B. Li et al., \u201cAcoustic modeling for google home,\u201d Proc. In- terspeech, pp. 399\u2013403, 2017. [3] Y.-J. Lu et al., \u201cESPnet-SE++: Speech enhancement for ro- bust speech recognition, translation, and understanding,\u201d in Proc. Interspeech, 2022, pp. 5458\u20135462. J. R. Hershey et al., \u201cDeep clustering: Discriminative em- beddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [4] [5] D. Yu et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [6] Z. Q. Wang et al., \u201cMulti-channel deep clustering: Dis- criminative spectral and spatial embeddings for speaker- independent speech separation,\u201d in Proc. ICASSP, 2018, pp. 1\u20135. [7] D. Wang and J. Chen, \u201cSupervised speech separation based on deep learning: An overview,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702\u20131726, 2018. [8] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256\u20131266, 2019. [9] Y. Luo et al., \u201cDual-path RNN: Efficient long sequence mod- eling for time-domain single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 46\u201350. [10] C. Subakan et al., \u201cAttention is all you need in speech sepa- ration,\u201d in Proc. ICASSP, 2021, pp. 21\u201325. [11] L. Yang et al., \u201cTFPSNet: Time-frequency domain path scanning network for speech separation,\u201d in Proc. ICASSP, 2022, pp. 6842\u20136846. [12] K. Tan et al., \u201cNeural spectrospatial filtering,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605\u2013621, 2022. [13] Z. Q. Wang et al., \u201cTF-GridNet: Integrating full- and sub- band modeling for speech separation,\u201d arXiv:2211.12433, 2022. [14] M. Maciejewski et al., \u201cWHAMR!: Noisy and reverberant single-channel speech separation,\u201d in Proc. ICASSP, 2020, pp. 696\u2013700. [15] M. L. Seltzer et al., \u201cLikelihood-maximizing beamform- ing for robust hands-free"}