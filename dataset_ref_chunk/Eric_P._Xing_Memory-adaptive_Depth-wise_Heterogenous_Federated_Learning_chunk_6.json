{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Memory-adaptive_Depth-wise_Heterogenous_Federated_Learning_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to the text, why do small sub-networks slightly influence the global performance in Federated Learning?", "answer": " Small sub-networks influence global performance due to limited knowledge of the global model in the aggregation phase.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " What is the average performance degradation observed in SplitMix due to the weaker base learner?", "answer": " SplitMix has an average performance degradation of 5.55% \u00b1 0.81%.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " How do FEDEPTH and m-FEDEPTH algorithms perform against insufficient memory budget?", "answer": " FEDEPTH shows 1.73% \u00b1 0.34% degradation, while m-FEDEPTH shows 2.74% \u00b1 0.97% degradation.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " What scenario was reproduced to show the Convergence of FEDEPTH family on Cifar10?", "answer": " The predefined scenario was Figure 6, where new clients with resources r = 2 joined and replaced clients with r = 1.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " What alternative way is suggested to handle dynamic partition, device upgrade, and memory constraints in federated learning?", "answer": " The alternative way suggested is to train a new large base model from scratch and discard previously trained neural networks, then perform Model Knowledge Distillation (MKD).", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " How do FEDEPTH and m-FEDEPTH compare in terms of prediction accuracy for CIFAR-100?", "answer": " m-FEDEPTH always outperforms FEDEPTH for CIFAR-100.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " What is the potential benefit of replacing zero paddings with other modules in the design of FEDEPTH?", "answer": " Replacing zero paddings with other modules may result in a better model, but it may come at the cost of extra memory due to new activations and parameters.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " How do HeteroFL and SplitMix perform on unbalanced Dirichlet partitions compared to FedAvg?", "answer": " HeteroFL and SplitMix perform worse than FedAvg on unbalanced Dirichlet partitions, with SplitMix even performing worse than training with the smallest models.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " What improvements do FEDEPTH and m-FEDEPTH achieve on CIFAR-10 and CIFAR-100 compared to FedAvg?", "answer": " FEDEPTH and m-FEDEPTH achieve substantial improvements of 6.15% and 6.53% on CIFAR-10, and 12.24% and 11.57% on CIFAR-100 compared to FedAvg.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}, {"question": " What learning rate and training epoch are chosen for fine-tuning in the text?", "answer": " A learning rate of 5 x 10^4 and a training epoch of 100 are chosen for fine-tuning.", "ref_chunk": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}], "doc_text": "the previous section and Figure 2 that small sub-networks slightly influence the global per- formance because the small number of model parameters provides limited knowledge of the global model in the aggregation phase of FL. In contrast, SplitMix has an apparent performance degrada- tion of an average of 5.55 \u00b1 0.81% due to the weaker base learner. The FEDEPTH and m-FEDEPTH are relatively stable algorithms against insufficient memory budget, showing 1.73 \u00b1 0.34% and 2.74 \u00b1 0.97% degradation, respectively. 1https://github.com/sksq96/pytorch-summary 2We reproduced this algorithm to conform to our predefined Figure 6: Convergence of FEDEPTH family on Cifar10. Results in the Surplus Budget scenario. We let the new clients with rich resources r = 2 join in FL and replace the clients with r = 1. Prior works, like HeteroFL and SplitMix, did not con- sider such dynamics, and the clients with more memory budgets still train \u00d71 neural networks. An alternative way is to train a new large base model from scratch and discard previously trained \u00d71 neural networks, hence wasting computing resources. From Table 2, we can observe that MKD indeed makes sense for improving the performance of the global model (still \u00d71- width). Furthermore, we note that combining depth-wise training and MKD is a flexible solution to simultaneously solve dynamic partition, device upgrade, and memory constraints. For example, when a new client with r = 7 6 enters into the federated learning, the client can locally learn two models via regular and depth-wise se- quential training, respectively, and then perform MKD while main- taining an original-size model for aggregation. Comparison between FEDEPTH and its variant. As shown in Table 2, for CIFAR-10, FEDEPTH and m-FEDEPTH can achieve similar prediction accuracy. However, for CIFAR-100, m- FEDEPTH always outperforms FEDEPTH. It is worth recalling the design of FEDEPTH, which introduces zero paddings to match the dimension between two skip-connected blocks. This may inject negligible noise for the training on more complex data. Replacing the zero paddings with other modules, such as convolutions, may result in a better model. However, this usually comes at the cost of extra memory because of the new activations and parameters, which is usually intolerable to resource-constrained devices. memory budgets, rather than the original fixed-depth allocation. (2) 60 0 100Top-1 Accuracy (1.0) (0.3) 20 40 80 (5) FeDepth (1.0) 0 100Top-1 Accuracy 80 FedAvg (\u00d71) 20 40 FedAvg m-FeDepth (2) 60 (5) (0.3) Figure 7: Fine-tuning ViT-T/16 on CIFAR-10 (left) and CIFAR-100 (right) under balanced non-IID data partitions with FedAvg FEDEPTH, and m-FEDEPTH. FedAvg (\u00d71) assumes each client can afford to train the full-size model with 12 identical encoder blocks, while FedAvg (\u00d7 1 6 -width model, whose memory consumption is equal to train two encoder blocks. 6 ) assumes each client trains a 1 Influence of Unbalanced Non-IID Distribution Table 3 shows the prediction results on distributed datasets in FL from the unbalanced Dirichlet partition (Fair Budget). We note that HeteroFL and SplitMix were not evaluated on such an unbalanced distribution. Overall, the higher skewed distribution leads to worse performance for FL, which can be observed by comparing results on Table 2 and Table 3. Since the number of samples per class in CIFAR-100 is lim- ited (there are 500 samples for each class), \u03b1u(\u03bb) and \u03b1(\u03bb) will output similar statistical distribution according to the number of samples on each client in FL. Therefore, we obtain similar CIFAR- 100 results on both balanced and unbalanced non-IID data parti- tions. Specifically, \u03b1(\u03bb) always outputs 400 training samples per client on average. For CIFAR-100, \u03b1u(0.3) outputs 399.40\u00b134.53 training samples per client, \u03b1u(1.0) outputs 399.34 \u00b1 17.74. For CIFAR-10, \u03b1u(0.3) outputs 399.44 \u00b1 150.60 training samples per client, \u03b1u(1.0) outputs 399.39 \u00b1 77.37. CIFAR-10 CIFAR-100 Method \u03b1u(0.3) \u03b1u(1.0) \u03b1u(0.3) \u03b1u(1.0) FedAvg (\u00d7 1 6 ) HeteroFL SplitMix DepthFL FeDepth m-FeDepth 46.46 46.14 31.23 47.13 52.61 51.58 52.02 52.20 44.70 55.49 58.55 57.91 15.62 16.02 22.68 23.19 23.25 27.86 17.99 18.36 25.28 26.02 26.16 29.56 Table 3: Experimental results on unbalanced Dirichlet par- titions. Because of the relatively limited number of samples per class in CIFAR-100, an unbalanced Dirichlet partition outputs similar statistical distribution according to the num- ber of local data examples. Regarding CIFAR-10 results, we observe that HeteroFL and SplitMix cannot achieve comparable predictions or generalization ability compared to FedAvg. SplitMix even performs worse than training with the smallest models in FL. This result indicates that SplitMix is not robust to unbalanced distribution. One reason for this phenomenon is that small base models cannot capture repre- sentative features due to the significant weight divergence between local clients stemming from a highly skewed distribution (Frankle and Carbin 2019; Li et al. 2022). For HeteroFL, as mentioned in the case study in Section , the full-size neural networks on resource- sufficient devices provide the fundamental ability but small sub- networks trained with unbalanced distribution indeed affect the global performance. In contrast to HeteroFL and SplitMix, our pro- posed FEDEPTH and m-FEDEPTH gain substantial improvements of 6.15% and 6.53% on CIFAR-10, and of 12.24% and 11.57% on CIFAR-100 compared to FedAvg. r = { 1 setting of PreResNet-20 in the scenario of Fair Budget. 6 , 1 3 , 1 2 , 1} are uniformly allocated to 100 clients as the same For fine-tuning, we choose a learning rate of 5 \u00d7 104 and a training epoch of 100. Figure 7 shows the test results of ViT- T/16 (Qu et al. 2022) under balanced Dirichlet data partitions, on which we observe that exploiting FEDEPTH and m-FEDEPTH can produce good global models. Specifically, FEDEPTH-ViT signifi- cantly outperforms FEDEPTH-PreResNet-20 with 36.06 \u00b1 12.79% and 30.64 \u00b1 4.24% improvements on CIFAR-10 and CIFAR- 100 on average, respectively. m-FEDEPTH-ViT significantly out- performs m-FEDEPTH-PreResNet-20 with 36.66 \u00b1 12.88% and 27.41 \u00b1 3.13% improvements on CIFAR-10 and CIFAR-100 on average, respectively. We also observe that although local ViTs are fine-tuned on varying distribution data, we obtain global models with similar performance. It indicates that ViT is more robust to distribution shifts and hence improves FL over"}