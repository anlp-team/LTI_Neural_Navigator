{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Squeeze,_Recover_and_Relabel:_Dataset_Condensation_at_ImageNet_Scale_From_A_New_Perspective_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What accuracy did the proposed approach achieve on the ImageNet-1K dataset with 224x224 resolution and IPC 50?", "answer": " The proposed approach achieved an accuracy of 60.8%.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " How does the proposed approach compare to previous methods in terms of accuracy on the ImageNet-1K dataset?", "answer": " The proposed approach outperformed all previous methods by a large margin.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " What are the three stages involved in the new framework for dataset condensation proposed in the text?", "answer": " The three stages involved are squeezing, recovery, and relabeling.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " What is the ultimate goal of the data condensation task described in the text?", "answer": " The ultimate goal is to synthesize data in a way that achieves a certain/minimal performance gap on the original evaluation set when models are trained on the synthetic data.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " How is the objective of the data condensation task formulated in the text?", "answer": " The objective is formulated as achieving \u03f5-approximate performance on the synthetic data compared to the original full dataset.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " What is the primary drawback associated with conventional solutions for dataset condensation mentioned in the text?", "answer": " The primary drawback is the computational burden due to the unrolling of the inner loop during each outer-loop update, coupled with bias transference from real to synthetic data.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " What is the main objective of the proposed decoupling approach in the text?", "answer": " The main objective is to disassociate model training and synthetic data optimization to enhance efficiency and avoid bias transference.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " What is the purpose of the Decoupling Outer-loop and Inner-loop Training stage in the proposed framework?", "answer": " The purpose is to extract pertinent information from the original dataset, encapsulate it within deep neural networks, and evaluate the impact of various data augmentation techniques and training strategies.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " How does the proposed decoupling approach aim to reformulate the traditional bilevel optimization inherent to dataset condensation?", "answer": " The proposed approach reformulates it into a unilevel learning procedure by extracting information from the original dataset and encapsulating it within deep neural networks.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}, {"question": " What is the core idea behind the training procedure of the proposed approach to dataset condensation?", "answer": " The core idea is to adequately train and preserve pivotal information within a dataset within a deep neural network.", "ref_chunk": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}], "doc_text": "ImageNet-1K datasets. On ImageNet-1K with 224\u00d7224 resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community\u2019s confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs. 2 Methods Condensed Arch Time Cost (ms) Peak GPU Memory Usage (GB) DM [10] MTT [1] ConvNet-4 ConvNet-4 18.11 12.64 10.7 48.9 SRe2L (Ours) ConvNet-4 ResNet-18 ResNet-50 0.24 0.75 2.75 4.2 7.6 33.8 Table 1: Synthesis Time and Memory Consumption on Tiny-ImageNet (64\u00d764 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes). Contributions. We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated excep- tional efficacy, efficiency and robustness in both the data synthesis and model training phases. We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain. To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\u00d7224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin. 2 Approach Data Condensation/Distillation. The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have (cid:1)(cid:9), we aim to learn a small condensed dataset a large labeled dataset T = (cid:8)(x1, y1) , . . . , (cid:0)x|T |, y|T | (cid:1)(cid:9) (|C| \u226a |T |) that preserves the crucial information in original Csyn = (cid:8)((cid:101)x1, (cid:101)y1) , . . . , (cid:0) (cid:101)x|C|, (cid:101)y|C| T . The learning objective on condensed synthetic data is: \u03b8Csyn = arg min LC(\u03b8) \u03b8 (cid:105) , (cid:101)y is the soft label coresponding to the synthetic data (cid:101)x. The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets [15] and \u03f5- approximate [16], the objective of data condensation task can be formulated as achieving: (cid:104) ((cid:101)x, (cid:101)y)\u2208Csyn where LC(\u03b8) = E \u2113(\u03d5\u03b8Csyn((cid:101)x), (cid:101)y) sup (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:16) \u03d5\u03b8Csyn (x), y (cid:17)(cid:12) (cid:12) (cid:12) (cid:111) (x,y)\u223cT \u2264 \u03f5 where \u03f5 is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data Csyn through: (cid:18) (cid:19) (cid:17)(cid:12) (cid:12) (cid:12) (cid:110)(cid:12) (cid:12) (cid:12)\u2113 (\u03d5\u03b8T (x), y) \u2212 \u2113 (cid:111) (cid:16) \u03d5\u03b8Csyn (x), y sup arg min Csyn,|C| (x,y)\u223cT Then, we can learn <data, label>\u2208 Csyn with the associated number of condensed data in each class. Decoupling the condensed data optimization and the neural network training: Conventional solutions, such as FRePo [5], CAFE [6], DC [8], typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference 3 (1) (2) (3) from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section. 2.1 Decoupling Outer-loop and Inner-loop Training Inspired by recent advances in DeepDream [17], Inverting Image [18, 19] and data-free knowledge transfer [20], we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure. ): During this stage, our objective is to extract pertinent information from the Stage-1 Squeeze ( original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream [17] and Inverting Images [18, 19]. It\u2019s noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset T and the learnable small synthetic dataset C. The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe: \u03b8T = arg min LT (\u03b8) \u03b8 where LT (\u03b8) typically uses cross-entropy loss as LT (\u03b8) = E"}