{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title and publication date of the paper by K. Seki and team on text-to-speech synthesis?", "answer": " Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection, arXiv:2210.14850, 2022", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " Who authored the paper on natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions in 2018?", "answer": " J. Shen, R. Pang, R. J Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al.", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " What is the title of the paper by A. Vaswani and team presented at NeurIPS in 2017?", "answer": " Attention is all you need", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " Which institution released the CSTR VCTK corpus in 2017?", "answer": " University of Edinburgh. The Centre for Speech Technology Research (CSTR)", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " What is the title and publication date of the paper by C. Wang and colleagues focused on a large-scale multilingual speech corpus?", "answer": " VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation, ACL, 2021", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " What is the name of the toolkit introduced by S. Watanabe and team in 2018 for end-to-end speech processing?", "answer": " ESPnet", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " Who presented the paper on Beto, Bentz, Becas in 2019?", "answer": " S. Wu and M. Dredze", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " In the study, what were the different types of text data used in pretraining Dtext?", "answer": " Spoken texts from VoxPopuli, M-AILABS, and CSS10; written texts from ParaCrawl", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " How many different cases of data usage were investigated for multilingual TTS in the study?", "answer": " Three cases: Spoken Text only, Written Text only, and Spoken+Written Text combined", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}, {"question": " What was the main comparison made regarding the bottleneck layer architecture in the study?", "answer": " Comparison between Residual layer and Transformer encoder for the seen and unseen languages", "ref_chunk": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}], "doc_text": "preprint arXiv:2210.15447, 2022. [Seki et al., 2022] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850, 2022. [Shen et al., 2018] J. Shen, R. Pang, R. J Weiss, M. Schus- ter, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by condi- tioning WaveNet on mel spectrogram predictions. In Proc. ICASSP, pages 4779\u20134783, 2018. [Snyder et al., 2018] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn em- beddings for speaker recognition. In Proc. ICASSP, pages 5329\u20135333, 2018. [Staib et al., 2020] M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech syn- thesis. In Proc. Interspeech, pages 2942\u20132946, 2020. [Tachibana et al., 2018] H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided atten- tion. In Proc. ICASSP, pages 4784\u20134788, 2018. [Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Proc. NeurIPS, vol- ume 30, 2017. [Veaux et al., 2017] C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2017. [Wang et al., 2021] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learn- In Proc. ACL, pages 993\u20131003, ing and interpretation. 2021. [Watanabe et al., 2018] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N.-E. Y. Soplin, J. Hey- mann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech, pages 2207\u2013 2211, 2018. [Wu and Dredze, 2019] S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP, pages 833\u2013844, 2019. [Xu et al., 2021] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP, pages 6079\u20136083, 2021. [Zen et al., 2012] H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Sta- tistical parametric speech synthesis based on speaker and language factorization. TASLP, 20(6):1713\u20131724, 2012. [Zen et al., 2019] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J Weiss, Y. Jia, Z. Chen, and Y. Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. In- terspeech, pages 1526\u20131530, 2019. [Zhang and Lin, 2020] H. Zhang and Y. Lin. Unsupervised learning for sequence-to-sequence text-to-speech for low- resource languages. Proc. Interspeech, pages 3161\u20133165, 2020. [Zhang et al., 2022] G. Zhang, K. Song, X. Tan, D. Tan, Y. Yan, Y. Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech, pages 456\u2013460, 2022. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text Written Text Spoken+Written Text 5.65 5.81 5.54 3.79 4.55 3.72 6.48 6.94 6.34 7.15 9.10 7.51 7.38 7.61 7.07 10.62 21.24 15.33 4.99 5.22 4.96 5.28 12.73 5.44 9.05 9.50 8.82 18.27 18.44 17.48 6.46 6.76 6.35 9.58 12.52 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer Transformer encoder 5.65 5.77 3.79 4.63 6.48 6.36 7.15 6.61 7.38 7.17 10.62 11.25 4.99 4.90 5.28 6.50 9.05 8.89 18.27 14.15 6.46 6.44 9.58 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtext on the performance of our method. As described in \u00a7 3.1, we used spoken texts from VoxPop- uli, M-AILABS, and CSS10 in the evaluations presented in \u00a7 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effective- ness of using written texts for our multilingual TTS, we used ParaCrawl [Ba\u02dcn\u00b4on et al., 2020], a web-crawled text dataset built for machine translation, for Dtext during the unsuper- vised pretraining described in \u00a7 2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the fol- lowing three different cases. 1) Spoken Text: Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text: Only using the text data from ParaCrawl, and 3) Spoken+Written Text: We combined the text data in Spo- ken Text and Written Text. We used the byte-based proposed model presented in \u00a7 3.2 and \u00a7 3.3. Table 6 lists the results. encoder as the bottleneck layer (referred to as Transformer- encoder), comparing it with the original residual layer de- tailed in \u00a7 2.4 (referred to as Residual layer). Table 7 lists the results. For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depend- ing on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can im- prove the generalizability of the proposed model. Neverthe- less, the overall performance of both models remains compa- rable in terms of average metrics. C Effect of excluding some"}