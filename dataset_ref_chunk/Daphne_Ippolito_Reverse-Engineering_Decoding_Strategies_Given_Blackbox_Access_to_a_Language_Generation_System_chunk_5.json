{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Reverse-Engineering_Decoding_Strategies_Given_Blackbox_Access_to_a_Language_Generation_System_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the goal of the adversary in estimating the value of k?", "answer": " The goal of the adversary is to estimate k while keeping n as small as possible.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " What is the ideal prompt m for estimating k?", "answer": " The ideal prompt m gives responses that are perfectly uniform over the entire vocabulary V.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " How is the lower bound for k trivially estimated?", "answer": " The number of unique items in a set of responses can be observed by calling Gen(m) n number of times, each time keeping the first token of the output.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " What is the main challenge in engineering a prompt that yields a perfectly uniform distribution?", "answer": " It is exceedingly difficult to engineer a prompt that gives perfectly uniform distributions over the entire vocabulary.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " What is the key building block for estimating k?", "answer": " The construction of a prompt m that distributes substantial probability mass onto a subset Vm of the token space is the key building block.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " How does the algorithm for estimating p work?", "answer": " The algorithm estimates p by using two prompts m1 and m2 that return responses from known distributions, calculating the average of the estimated p values from m1 and m2.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " What does the top-p reverse engineering problem reduce to when given a known distribution?", "answer": " The top-p reverse engineering problem reduces to top-k when given a known distribution.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " Why is it important to estimate p using the sum of the k largest in-vocabulary probabilities?", "answer": " This approach ensures robustness against an imperfectly guessed distribution, especially when prompts including exemplars create biases in the returned tokens.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " What is the advantage of using prompts that return distributions close to known distributions for estimating p?", "answer": " Using such prompts reduces the error in estimating p, which is determined by how close the guessed distribution is to the true underlying one.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}, {"question": " How does the algorithm ensure consistency in distribution matching for the tokens returned?", "answer": " The algorithm uses the actual distributions over tokens for distribution matching by summing the k largest in-vocabulary probabilities, ensuring consistency.", "ref_chunk": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}], "doc_text": "that takes an input prompt m and outputs a single response token r, let m1 and m2 be two prompts that with high probability return responses from large (\u226bk) sets of different sizes\u2014e.g. m1 returns random nouns and m2 returns random adverbs. function ESTIMATEK(Prompts m1,m2;Gen:m(cid:55)\u2192r) samples1\u2190 while max number iterations not reached do [],samples2\u2190 [] (cid:91) (cid:91) // we set max iterations=32 for {1..100} do // at most 3200 samples generated samples1.insert(Gen(m1)) samples2.insert(Gen(m2)) end for k1 \u2190 k2 \u2190 minSamples\u2190 # unique items in samples1 # unique items in samples2 (cid:91) (cid:91) (samples1(k1) >1 and (cid:91) samples2(k2) >1) if k1 =k2 and minSamples then // Boolean testing all items appear twice break end if end while return \u230a(k1+k2)/2\u230b // guesses average if convergence not reached end function As we mentioned in the main paper text, suppose, for a given prompt m, we call Gen(m), n number of times, each time keeping just the first token of the output. We can trivially lower bound k by observing the number of unique items in a set of responses. As n approaches \u221e, all k allowed responses will be observed. Since this is infeasible, the adversary\u2019s goal is to estimate k while keeping n as small as possible. It is easy to see that the ideal prompt m is one that gives responses that are perfectly uniform over the entire vocabulary V. In the uniform case, we are left with the standard coupon collector problem (P\u00f3lya, 1930). We would recover the exact value of k with probability at least 1\u2212 1 k by setting n>2klogk. Unfortunately, such a prompt is exceedingly difficult to engineer (see Appendix D). It turns out we can do almost as well without needing full uniformity. The key building block for our attack is the construction of an m that distributes substantial probability mass onto a subset Vm \u2286V of the token space. We require that for any k <|Vm|, we have Prob(cid:0)Gen(m)=x(k)(cid:1)\u2265 1 ck , for some small constant c. Put in plain language, we want to ensure that for any number of tokens k the distribution might be truncated at, the least likely token that can be generated is no more than c times less likely to appear than if the distribution were truly uniform. If n \u2265 2cklog(ck), then with probability at least 1\u2212 1 ck , our prediction is exactly correct. This result is far from tight, but follows easily from coupon collector on a uniformly random set of size ck. In practice, we use Algorithm 1, which repeatedly estimates a lower bound for k using two different prompts m1 and m2 for increasing numbers of trials until (1) the two estimates match and (2) the x(k) token appears at least twice in both generations (to prevent spurious matching). In such a case, the expected number of trials n is approximately bounded above by 2cklog(ck) via coupon collector6. B Algorithm for Estimating p Our goal is to estimate p to within a factor of \u03f5. This would be trivial to do if we could construct a prompt m that is uniform over a subset Vm \u2286V of size at least 1 \u03f5 . Then estimating p would be equivalent to estimating top-k for k \u2248 p \u03f5 because each unique token seen implies a probability mass of \u03f5. It is impossible to design a prompt which yields a perfectly uniform distribution. However, although uniformity is desirable, for top-p estimation, it is more important that the distribution of Gen(m) is known, i.e., we have access to the underlying language model f\u03b8. If k distinct tokens appear in the p-truncated distribution, 6Aside: the constant 2 that appears in the expected number of trials is due to requiring that the kth most likely token appears at least twice. However, it is unrelated to the constant 2 that appears in the bound in the previous paragraph, which is chosen to ensure the 1 ck failure probability. Algorithm 2 Algorithm for estimating p. Consider a language model f\u03b8 :(m,r)\u2192R that scores a prompt/response pair and a system Gen:m(cid:55)\u2192r that takes an input prompt m and outputs a single response token r using f\u03b8 and top-p sampling. Let m1 and m2 be two prompts that return responses from known distributions over relatively small sets (|Vm|around 10-40)\u2014e.g. m1 returns random months and m2 returns random dates within the month of March. function ESTIMATEP(Prompts m1,m2;Gen:m(cid:55)\u2192r,f\u03b8) HELPER(m1,Gen,f\u03b8) HELPER(m2,Gen,f\u03b8) p1 \u2190 p2 \u2190 return (p1+p2)/2 (cid:91) (cid:91) end function function HELPER(Prompt m;Gen,known LM f\u03b8 ) baseProbs\u2190 [] for v \u2208Vm do (cid:91) baseProbs.insert(Pf\u03b8 (r =v|m)) // Will store known probability distribution // Vm is the subset of tokens we consider // Probabilities using full random sampling end for Sort baseProbs from largest to smallest. baseProbs.insert((cid:80) samples\u2190 for {1..N} do Probf\u03b8 (r =v|m) v\u2208V\u2212Vm [] (cid:91) // Summed probabilities of all out-of-set tokens samples.insert(Gen(m)) end for l \u2190 num unique items in samples (cid:80)l p\u2190 return p end function (cid:91) i=1baseProbs[i] (cid:91) then (using the same notation as above), we can bound p as: k\u22121 (cid:88) Probf\u03b8(x(l))<p\u2264 k (cid:88) Probf\u03b8(x(l)). l=1 l=1 Thus, given a known distribution, the top-p reverse engineering problem reduces to top-k. Even if we do not know exactly the underlying model for a blackbox system, we can construct prompts that appear to often return distributions close to a family of known distributions. Then the error in estimating p is just determined by how far off our guess of distribution is from the true underlying one. Note that to ensure robustness against an imperfectly guessed distribution, we estimate p using the sum of the k largest in-vocabulary probabilities, rather than trying to actually match the probabilities for the unique items sampled. This turns out to be important when prompts including exemplars are used, as the exemplars often create a bias in the tokens returned, but the overall drop-off in probabilities of most to least likely tokens tends to be more consistent. However, for distribution matching, we use the actual distributions over tokens. In"}