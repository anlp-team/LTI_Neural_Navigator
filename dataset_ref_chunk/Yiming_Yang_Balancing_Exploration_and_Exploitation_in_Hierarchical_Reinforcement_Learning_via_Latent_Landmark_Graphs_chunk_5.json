{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Balancing_Exploration_and_Exploitation_in_Hierarchical_Reinforcement_Learning_via_Latent_Landmark_Graphs_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the color gradient used to indicate the trend of an episode in the Ant Maze task?", "answer": " From purple to yellow", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " What does purple represent in the color gradient used to indicate the trend of an episode in the Ant Maze task?", "answer": " Start position", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " What does yellow represent in the color gradient used to indicate the trend of an episode in the Ant Maze task?", "answer": " End position", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " How are adjacency states along an episode represented in the latent space?", "answer": " Clustered to similar latent representations", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " In the ablation studies on various components of the HILL framework, what does Basic GCHRL solely rely on as the base RL optimizer?", "answer": " SAC (Soft Actor-Critic)", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " What does the proposed negative-power contrastive representation learning objective aim to demonstrate in the Ant Maze task?", "answer": " Effectiveness and efficiency", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " What does the use of HER (Hindsight Experience Replay) alleviate in the exploration process?", "answer": " Non-stationary issues", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " What does HILL (B+CSRL+HER+G-N-U) take into consideration to achieve effective exploration-exploitation trade-offs?", "answer": " The novelty and utility measures on latent landmark graphs", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " How does the stable ratio k affect well-learned representations according to the ablation studies?", "answer": " Helps to stably constrain changes", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}, {"question": " What does the Representation Dimension d determine in the HILL framework?", "answer": " The degree of information compression", "ref_chunk": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}], "doc_text": "estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually 2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity 0.2 1.0Success 4.0Steps \u00d71e6 0.0 0.0 1.5 3.0 2.5 1.0 0.3 0.5 2.0 0.4 0.4 3.5 0.8 0.6 0.5 0.8 0.2 3 1.0 0.0 1.5 0.6 2 2.0 5 3.0Steps \u00d71e6 0.5 2.5 0.0 0.4 0.8 0.2 1.0Success (a) Stable Ratio k (b) Representation Dimension d 1.0Success 0.2 n=1 \u03b2=0.1 0.8 0.0 0.0 n=1 \u03b2=10 n=2 \u03b2=0.1 n=2 \u03b2=1 n=1 \u03b2=1 1.5 0.6 3.0Steps \u00d71e6 n=2 \u03b2=10 2.5 1.0 0.4 2.0 0.5 0.0 0.2 c=50 c=10 0.0 2.5 3.0Steps \u00d71e6 0.6 2.0 1.0 c=100 1.5 c=20 1.0Success 0.8 0.5 0.4 (c) Scaling Factor \u03b2, Power n (d) Subgoal Selection Interval c 1.5 3.0 3.5 2.5 0.6 0.0 0.8 0.0 0.4 0.2 1.0Success 4.0Steps \u00d71e6 FPS w/o FPS 2.0 0.5 1.0 200 0.5 3.0Steps \u00d71e6 1.0Success 0.2 0.0 300 1.5 0.0 500 2.0 400 1.0 2.5 0.6 100 0.8 0.4 (e) Landmark Sampling Strategy (f) Landmark Number Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task. introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance. E. Ablation Study on HyperParameters We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters. 1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks. 2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by \u03d5 is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks. 3) Scaling Factor \u03b2, Power n: \u03b2 and n both affect the performance of subgoal representation learning. A smaller n or a larger \u03b2 pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when \u03b2 is small and achieves better performance when n = 1, \u03b2 = 0.1. Therefore, we set n = 1, \u03b2 = 0.1 for all tasks. 4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments. 5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS\u2019s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent\u2019s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks. 6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components. REFERENCES [1] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 5, 1992. [2] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artificial intelligence, vol. 112, no. 1-2, pp. 181\u2013211, 1999. [3] A. G. Barto and"}