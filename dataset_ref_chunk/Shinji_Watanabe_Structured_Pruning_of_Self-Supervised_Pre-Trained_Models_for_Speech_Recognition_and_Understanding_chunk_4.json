{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Structured_Pruning_of_Self-Supervised_Pre-Trained_Models_for_Speech_Recognition_and_Understanding_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What comparison is made between HJ-Pruning-SepSize and HJ-Pruning-MAC?,answer: HJ-Pruning-SepSize is comparable with HJ-Pruning-MAC but requires a grid search over two target sparsities.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What percentage of higher compression rate can be achieved without loss in accuracy compared to ASR?,answer: Over 55%.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What do the blue Pareto frontiers in Fig. 3 represent?,answer: The Pareto frontiers represent the results of a grid search over Transformer and CNN sparsities.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What test set are the models evaluated on in Fig. 4 for robustness analysis?,answer: The out-of-domain Tedlium test set.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What components are shown in Fig. 5 after HJ-Pruning-MAC?,answer: Remaining CNN channels, attention heads, and FFN intermediate sizes.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What is the main observation regarding the pruning of upper layers in Fig. 5?,answer: The upper layers are pruned the most, indicating that upper layers are more redundant.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What other compressed models are included for comparison in Fig. 6?,answer: DistilHuBERT and LightHuBERT.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What does the paper propose in the conclusion section regarding HJ-Pruning?,answer: HJ-Pruning jointly prunes heterogeneous components of SSL speech models to achieve strong performance-efficiency tradeoffs.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " What percentage of MACs can be saved by HJ-Pruning depending on the task?,answer: 40% or 50% MACs.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}, {"question": " According to the references, what is SUPERB?,answer: SUPERB is a benchmark for speech processing universal performance.", "ref_chunk": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}], "doc_text": "the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because ) \u2193 ( e t a R r o r r E d r o W % 7 6.5 6 5.5 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (a) Word Error Rates (%) on LibriSpeech test-clean. 87 ) \u2191 ( 86 y c a r u c c A % 85 84 Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points 30 40 50 MACs (\u00d7109) 60 70 (b) Intent Classi\ufb01cation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2. 26 ) \u2193 ( e t a R r o r r E d r o W % 24 22 20 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 18 20 30 40 50 MACs (\u00d7109) 60 70 Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1. the classi\ufb01cation task is easier and thus requires less information than the sequence-to-sequence task. 4.3. Robustness of structured pruning To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust. 4.4. Architectures of pruned models Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The \ufb01rst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN, 10% 20% 30% 40% s l e n n a h C 500 400 300 200 s d a e H 12 8 4 0 1 2 3 4 CNN Layer 5 6 7 1 2 3 4 5 6 7 8 9 10 11 MHA Layer s e z i S . m r e t n I 3,000 2,000 1,000 1 2 3 4 5 6 7 8 9 10 11 FFN Layer Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. ) \u2193 ( e t a R r o r r E d r o W % 13 11 9 7 5.73 Unpruned HuBERT-base DistilHuBERT\u2217 LightHuBERT-small\u2217 LightHuBERT-base\u2217 HJ-Pruning-MAC 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. \u2217 WERs from SUPERB [1]. See Sec. 4.5 for discussions. the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23]. 4.5. Comparison with other compression methods As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and \ufb01ne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then \ufb01ne-tuned on the 100h labeled data, but our task- speci\ufb01c pruning only utilizes the 100h data. This comparison shows that our task-speci\ufb01c pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-ef\ufb01ciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 12 12 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. In- terspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., \u201cSelf- Supervised Speech Representation Learning: A Review,\u201d arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., \u201cAn exploration of self- supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., \u201cInvestigating Self-Supervised Learning for Speech Enhancement and Sep- aration,\u201d in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., \u201cSLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,\u201d in Proc."}