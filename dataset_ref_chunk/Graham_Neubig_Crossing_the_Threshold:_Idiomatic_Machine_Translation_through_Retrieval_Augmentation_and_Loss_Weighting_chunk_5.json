{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Crossing_the_Threshold:_Idiomatic_Machine_Translation_through_Retrieval_Augmentation_and_Loss_Weighting_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method led to the greatest increase in automatic metrics on all three test sets?", "answer": " Combining sentence upweighting and kNN-MT.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " By how many BLEU points did the combination of sentence upweighting and kNN-MT increase automatic metrics on the idiomatic test set?", "answer": " 3.08 BLEU points.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " Which test set saw the highest increase in automatic metrics due to the combination of sentence upweighting and kNN-MT?", "answer": " The random test set (fr) with 5.75 points.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " What did the one-tailed permutation test evaluate?", "answer": " The statistical significance of the results.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " For which language were the achieved values considered borderline in terms of statistical significance?", "answer": " Japanese.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " How does the size of the training data for the Japanese model compare to that of the French and Finnish models?", "answer": " The Japanese model was trained on roughly 1/10th of the data of the French and Finnish models.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " What percentage increase in accuracy was observed in French and Finnish for idiomatic sentences when using sentence upweighting and kNN?", "answer": " Roughly 13%.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " Did the combination of sentence upweighting and kNN-MT improve accuracy on literal sentences?", "answer": " Yes, it either slightly increased or slightly decreased accuracy.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " By what percentage did accuracy increase on the random set with the combination of sentence upweighting and kNN-MT?", "answer": " Around 7%.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}, {"question": " In which test set did performance decrease in Japanese when using the combination of sentence upweighting and kNN-MT?", "answer": " The random test set.", "ref_chunk": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}], "doc_text": "given otherwise. Exact evaluation standards are in Appendix E. 7 Results 7.1 Automatic and Human Evaluation In most cases, as reported in Figure 3, using a com- bination of sentence upweighting and kNN-MT led to the greatest increase in automatic metrics on all three test sets, of up to 3.08 BLEU points on the idiomatic test set (fr), 2.69 BLEU points on the literal test set (fi), and 5.75 points on the random test set (fr). In all cases except ja-rand, us- ing one or more of these methods improved over Rand-outFR - BLEU Idioms Idioms 0.50 0.09 0.20 0.92 Knn 0.64 0.92 0.21 0.94 0.94 0.94 0.92 0.20 0.20 Rand-outFI - Meteor 0.923 0.94 Rand-outFR - BertScore 0.39 0.39 0.32 0.92 0.922 0.08 Rand-outFI - BertScore Idioms 0.57 Rand-outFR - RougeL-sum 0.32 Rand-in Literal Rand-in 0.10 0.23 0.23 0.41 0.47 0.90 0.90 0.61 0.920 Knn + upweight 0.45 0.45 0.09 0.10 0.10 FI - BLEU Rand-in 0.51 0.51 0.22 0.41 0.90 0.35 0.46 0.90 0.53 0.940 0.95 0.90 0.93 0.29 0.29 0.48 0.48 0.57 0.20 0.92 0.92 0.54 0.30 Rand-outFR - Meteor 0.13 0.58 0.58 0.28 0.60 0.60 0.30 0.54 Rand-outJP - BertScore Literal Literal Literal 0.55 0.55 0.55 Literal 0.40 0.15 0.15 Rand-out 0.59 0.30 0.26 0.40 0.40 0.19 0.40 0.36 0.28 0.15 0.52 0.49 0.49 0.54 0.32 Idioms 0.91 0.27 0.27 0.27 0.33 0.33 0.43 Idioms 0.32 0.32 0.32 0.24 0.91 0.91 0.91 0.33 0.932 0.900 0.52 0.52 0.52 0.40 0.40 0.40 0.27 0.48 0.932 0.58 0.58 0.24 0.24 0.43 0.28 0.49 0.40 0.25 0.31 0.59 Literal 0.30 0.30 0.40 0.40 0.56 0.40 0.40 0.36 0.31 0.16 0.55 0.55 0.56 0.35 0.30 Literal Normal 0.59 0.30 Upweight Rand-outJP - RougeL-sum 0.36 0.56 0.13 0.17 0.60 0.16 Literal Literal Literal 0.48 0.14 0.48 0.12 0.905 0.65 Idioms 0.57 0.19 0.14 Idioms Idioms Idioms Idioms Idioms 0.48 0.48 Idioms 0.15 0.56 0.45 0.45 0.45 0.16 Rand-outFI - RougeL-sum 0.38 0.38 Rand-outJP - Meteor 0.54 0.38 0.26 0.26 0.26 0.33 0.14 Rand-in Literal Literal 0.39 0.39 0.39 0.41 0.10 0.90 0.937 0.42 0.42 0.42 0.936 0.936 0.93 0.50 0.50 0.09 0.22 0.22 0.93 0.93 0.93 0.936 0.21 0.92 0.50 0.18 0.18 0.902 0.93 0.93 0.935 0.28 0.50 0.92 0.34 0.34 0.34 0.44 0.44 0.53 Rand-in 0.53 0.21 0.21 0.95 Rand-in Rand-in Rand-in Rand-in 0.53 0.34 Rand-in 0.89 0.09 0.42 0.95 0.93 0.89 Rand-in Rand-in 0.940 Rand-outJP - BLEU 0.14 0.53 0.22 0.10 Figure 3: Results of automatic metrics. In most cases, combining loss weighting with KNN-MT improves automatic metrics the most on all three test sets, including the out-of-distribution (Random) test set. the baseline. Exact numerical results are in Ap- pendix J. We evaluate the statistical significance of the results through a one-tailed permutation test (Gra- ham et al., 2014). Further details are in Appendix F. Exact results are in Appendix G. For Finnish, sig- nificance is achieved for all three test sets, and for French, significance is achieved for the idiomatic and random test sets. For Japanese, values achieved are not significant, but are borderline. We note that the Japanese model was trained on roughly 1/10th of the data of the French and Finnish models, so its translations are not as high- quality. This also leads to the construction of a much smaller datastore, which may lead to weaker performance on the random set. base knn upweight upweight + knn fr-idioms fr-literal fr-rand-out 0.6177 0.7039 0.7526 0.6659 0.7303 0.8398 0.7010 0.7105 0.7477 0.7463 0.7434 0.8232 As our focus is on mitigating semantic errors, we mostly focus on the results of human evalua- tion, which are summarized in Table 4. Here, we also find that using both sentence upweighting and kNN is the best condition in most cases, increasing accuracy by roughly 13% in French and Finnish, and 4.5% in Japanese for idiomatic sentences. En- couragingly, this does not overly harm translation of literal sentences, as accuracy on the literal set either increases slightly (by roughly 4% in French and Finnish), or decreases very slightly (by roughly 0.4% in Japanese). For the random set, the combi- nation of sentence upweighting and kNN-MT by around 7% accuracy. However, in Japanese, perfor- mance on the random test set decreases by 4%. In all cases except ja-rand, one or more of these methods improves over the baseline. fi-idioms fi-literal fi-rand-out 0.4803 0.7692 0.7647 0.5562 0.8462 0.8235 0.5604 0.8205 0.7771 0.6194 0.8141 0.828 ja-idioms ja-literal ja-rand-out 0.4152 0.6475 0.6207 0.4286 0.6516 0.5560 0.4643 0.6557 0.5776 0.4598 0.6434 0.5862 Table 4: Human-judged accuracy on sentence-level se- mantics. 7.2 Error analysis We repeat the frequency analysis performed on commercial systems (\u00a74.2) for \u2206LM, and find that adding upweighting and kNN-MT generally im- proves translations at all frequency levels. These increases are not concentrated in low-frequency idioms, so more common idioms continue to be translated better.7 A representative example (for 7This trend is different in retrieval of long-tail facts in ques- tion answering, in which retrieval flattens out the difference French) is in Figure 4. A complete set of plots are in Appendix I. 0.4 0.4 0.4 0.25Metric Score 0.925 0.500Score BLEU 0.450 0.450 0.400 0.400 0.930 0.15 0.8Percentiles 0.8Percentiles base base 0.6 0.6 0.6 0.6 0.920 0.500Metric Score base 0.8Percentile of idioms in OS base base 0.8Percentile of idioms in OS 0.915 0.425 METEOR ROUGE 0.4 0.425 0.475 0.475 upweight+knn upweight+knn upweight+knn upweight+knn 0.20 0.2 0.2 BERTScore 0.2 0.2 0.935Score upweight+knn Figure 4: Automatic metrics for fr-idiom sentences, plotted by frequency, for base and upweight+knn. We examine the rate of severe and major errors made in the base model and the upweight+knn model in Table 5. In French and Finnish, the rate of critical errors decreased greatly, particularly in the idiomatic and random test sets. This is true to a lesser extent in Japanese. Major errors also decreased to a lesser extent. The only test set where errors increase is again the ja-rand test set. We note that it\u2019s possible for the rate of major errors to be higher in the upweight+knn model because some severe"}