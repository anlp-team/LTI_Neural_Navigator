{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_TAPLoss:_A_Temporal_Acoustic_Parameter_Loss_for_Speech_Enhancement_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the proposed method TAPLoss?,answer: The main focus of TAPLoss is to improve the perceptual quality of speech enhancement models.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " What are the four categories of low-level acoustic descriptors involved in the TAPLoss estimator?,answer: The four categories are frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " How does the TAPLoss different from prior work in speech enhancement?,answer: TAPLoss enables optimization at each time step for fine-grain speech characteristics, unlike prior work that looked at aggregated parameters or a small subset of categories.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " Why were acoustic parameters not previously incorporated in workflows with deep neural networks?,answer: Acoustic parameters were not incorporated because they required non-differentiable computations.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " What is the goal of speech enhancement according to the text?,answer: The goal is to enhance the quality and intelligibility of degraded speech signals.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " What types of approaches have been the core of state-of-the-art speech enhancement systems?,answer: Deep neural networks based approaches have been the core, particularly in single channel speech enhancement.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " What are some examples of frequency-related parameters considered in the text?,answer: Pitch, jitter, F1, F2, F3 frequency, and bandwidth are examples of frequency-related parameters.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " What limitations have been identified in the losses used in previous studies according to the text?,answer: Limitations include low correlations with speech quality, overemphasis on high-energy phonemes, and an inability to improve pitch.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " What is the purpose of the TAPLoss estimator in speech enhancement?,answer: The TAPLoss estimator minimizes the distance between estimated acoustics for clean and enhanced speech to improve perceptual quality and intelligibility.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}, {"question": " How does the proposed method demonstrate success empirically?,answer: The proposed method shows improvement in relative and absolute enhancement metrics for perceptual quality and intelligibility.", "ref_chunk": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}], "doc_text": "3 2 0 2 b e F 6 1 ] L C . s c [ 1 v 8 8 0 8 0 . 2 0 3 2 : v i X r a TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Yunyang Zeng1\u2020, Joseph Konan1\u2020, Shuo Han1\u2020, David Bick1\u2020, Muqiao Yang1\u2020, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acous- tic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recog- nition and paralinguistic analysis. We provide a differentiable es- timator for four categories of low-level acoustic descriptors involv- ing: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features. Un- like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic param- eter (TAP) loss enables auxiliary optimization and improvement of many \ufb01ne-grain speech characteristics in enhancement work\ufb02ows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time- frequency domain models can bene\ufb01t from our method. Index Terms\u2014 Speech, Enhancement, Acoustics, Perceptual Quality, Explainable Enhancement Evaluation, Interpretability [14], [15]; however, these have limited improvements because per- ceptual quality is only implicitly supervised. In this paper, we seek to address these issues by using fundamental speech features, which we refer to as acoustic parameters. The use of acoustic parameters has been shown to facilitate speaker classi\ufb01cation, emotion recognition, and other supervised tasks involving speech characteristics [16]\u2013[18]. Historically, these acoustic parameters were not incorporated in work\ufb02ows with deep neural networks because they required non-differentiable compu- tations. However, this does not re\ufb02ect their signi\ufb01cant correlation with voice quality in prior literature [19]\u2013[21]. Recently, some works have made progress in incorporating acoustic parameters for optimization of deep neural networks. Pitch, energy contour, and pitch contour were proposed to optimize perceptual quality in [22]. However, these three parameters are a small subset of the charac- teristics we consider, and evaluation was not performed on standard English datasets. A wide range of acoustic parameters was pro- posed in [23], which introduced a differentiable estimator of these parameters to create an auxiliary loss aimed at forcing models to retain acoustic parameters. This proved to improve the perceptual quality of speech. Unlike prior work, which used summary statistics of acoustic parameters per utterance, our current estimator allows optimization at each time step. The values of each parameter vary over time in the utterance, so statistics lose a signi\ufb01cant amount of information in the comparison of clean and enhanced speech. 1. INTRODUCTION Speech enhancement is aimed at enhancing the quality and intelli- gibility of degraded speech signals. The need for this arises in a variety of speech applications. While noise suppression or removal is an important part of the speech enhancement, retaining the per- ceptual quality of the speech signal is equally important. In recent years, deep neural networks based approaches have been the core of most state-of-the-art speech enhancement systems, in particular single channel speech enhancement [1]\u2013[4]. These are tradition- ally trained using point-wise differences in time-domain or time- frequency-domain. We look at 25 acoustic parameters \u2013 frequency related param- eters: pitch, jitter, F1, F2, F3 Frequency and bandwidth; energy or amplitude-related parameters: shimmer, loudness, harmonics-to- noise (HNR) ratio; spectral balance parameters: alpha ratio, Ham- marberg index, spectral slope, F1, F2, F3 relative energy, harmonic difference; and additional temporal parameters: rate of loudness peaks, mean and standard deviation of length of voiced/unvoiced re- gions, and continuous voiced regions per second. We use OpenSmile [24] to perform the ground-truth non-differentiable calculations, cre- ating a dataset to to train a differentiable estimator. However, many studies have shown limitations in these losses, including low correlations with speech quality [5] and [6]. Other studies have shown that they have overemphasis on high-energy phonemes [7] and an inability to improve pitch [8], resulting in speech that has artifacts or poor perceptual quality [9]. The insuf\ufb01- ciency of these losses has led to much work devoted to improving the perceptual quality of enhanced signals, which our work also aims to improve. Perceptual losses have often involved estimating the standard evaluation metrics such as Perceptual Evaluation of Speech Quality (PESQ) [10]. However, PESQ is non-differentiable, which forces dif\ufb01cult optimization [11] and often leads to limited improvements [12], [13]. Other approaches use deep feature losses Finally, we present our estimator for these 25 temporal acoustic parameters. Using the estimator, we de\ufb01ne an acoustic parameter loss, coined TAPLoss, LTAP, that minimizes the distance between estimated acoustics for clean and enhanced speech. Unlike previous work, we do not assume the user has access to ground-truth clean acoustics. We empirically demonstrate the success of our method, observing improvement in relative and absolute enhancement met- rics for perceptual quality and intelligibility. 2. METHODS 2.1. Background \u2020Equal Contribution (Random Order) In the time domain, let y denote a signal with discrete duration M such that y \u2208 RM . We de\ufb01ne clean speech signal s, noise signal n, Weight PESQ STOI 0.01 2.788 0.9697 Demucs LTAP \u03bb1 Ablation (\u03bb2 = 0) 0.03 2.841 0.9698 0.1 2.824 0.9689 0.3 2.834 0.9689 1 2.859 0.9694 0.01 2.899 0.9707 Demucs LTAP \u03bb2 Ablation (\u03bb1 = 1) 0.03 2.903 0.9712 0.1 2.926 0.9714 0.3 2.958 0.9722 1 2.958 0.9720 0.01 2.979 0.9654 FullSubNet LTAP \u03b3 Ablation 0.3 2.969 0.9648 0.03 2.981 0.9654 0.1 2.979 0.9654 1 2.965 0.9654 Table 1: LTAP ablation study of Demucs hyperparameters, \u03bb1 and \u03bb2, and FullSubNet hyperparamter \u03b3. Fig. 1: Percent Acoustic Improvement PAI on DNS-2020 Synthetic Test (No Reverb). Compared are baseline improvement over noisy (blue) PAI(s1, x), our improvement over noisy (red) PAI(s2, x), our improvement over baseline (green) PAI(s2, s1). Sorted"}