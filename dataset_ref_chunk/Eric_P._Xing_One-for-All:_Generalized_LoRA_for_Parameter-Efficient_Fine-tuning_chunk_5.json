{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_One-for-All:_Generalized_LoRA_for_Parameter-Efficient_Fine-tuning_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method consistently performs better across five datasets and a varying number of training examples per class?,        answer: GLoRA    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What type of tasks does GLoRA focus on with Large Language Models (LLMs)?,        answer: Few-shot generative language tasks    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What approach does GLoRA take when applied to LLMs, in contrast to vision tasks?,        answer: Tuning the attention layers only, not all linear layers    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " How does GLoRA perform compared to the pre-trained LLM and LoRA fine-tuned variants?,        answer: Consistently outperforms them    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " In few-shot learning for fine-grained visual recognition datasets, which method demonstrates superior performance across different shot scenarios?,        answer: GLoRA    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What kind of value does out-of-domain generalization hold for large-scale neural networks?,        answer: Significant value    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What is the accuracy percentage of the GLoRA model in the ImageNet-1K fine-tuning experiment on out-of-domain datasets?,        answer: 78.3%    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What is the performance gap between the fully-scaled ImageNet-1K fine-tuned model and GLoRA in a 16-shot setting?,        answer: 5.67%    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What is the key capability demonstrated by GLoRA in the out-of-domain generalization experiment?,        answer: Enhanced domain generalization aptitude    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}, {"question": " What kind of datasets were used to test the inference efficiency of GLoRA in comparison with existing methods?,        answer: Out-of-domain datasets    ", "ref_chunk": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}], "doc_text": "31.0 44.0 74.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0 76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6 76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3 6 e l E - B R O N s e g a r e v A 65 65 50 50 50 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 16Number of training samples per class 90Accuracy (%) 8 8 8 8 8 8 OxfordPets 60 60 60 60 60 FGVCAircraft 80 80 10 10 4 4 4 4 4 4 Food101 30 30 30 30 90 1 1 1 1 1 1 80Accuracy (%) 70Accuracy (%) 70Accuracy (%) Flowers102 95 GLoRA(0.28M) GLoRA(0.28M) Average 85 85 100Accuracy (%) StanfordCars 50Accuracy (%) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) LoRA(0.29M) GLoRA(0.27M) GLoRA(0.27M) GLoRA(0.27M) 2 2 2 2 2 2 40 40 40 40 NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) NOAH(0.36M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) VPT(0.64M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) Adapter(0.16M) 75 75 20 20 70 70 70 GLoRA(0.29M) Figure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of training examples per class. More comparisons are provided in Appendix H. 3.2 RESULTS ON LARGE LANGUAGE MODELS Table 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones. Model Dataset ARC (25-s) HellaSwag (10-s) MMLU (5-s) TruthfulQA (0-s) Average LLaMA-1-7B LoRA GLoRA LoRA GLoRA LLaMA-2-7B GLoRA Alpaca Alpaca ShareGPT ShareGPT - ShareGPT 51.0 53.5 52.9 51.7 53.2 53.1 53.7 77.8 77.3 78.1 77.9 77.4 78.5 78.5 35.7 33.8 34.5 36.1 36.2 46.9 46.5 34.3 34.8 37.8 39.2 43.9 38.8 45.1 49.7 49.8 50.8 51.2 52.7 54.3 56.1 We apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks where all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start from the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only GLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled from the 4 given datasets for model validation during the evolutions. We finally report the searched model\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms the pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy- perparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix. 3.3 FEW-SHOT LEARNING To extend the evaluation of GLoRA under conditions of limited data availability, we present the per- formance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar- ing it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in Figure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority of the few-shot learning datasets, consistently outperforming the performance of existing methods by a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth- ods yield similar accuracy levels, attributable to the already exceptional overall performance. On the Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we can observe that the average performance boost becomes more pronounced at higher shot scenarios, nevertheless, even at lower shot settings, the gains of our approach remain significant. 3.4 DOMAIN GENERALIZATION The capacity of out-of-domain generalization holds significant value for large-scale neural networks (Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener- alization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate the out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K 2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 7 Table 3: Results on domain generalization. GLoRA is significantly better than the existing works. Source Target ImageNet Sketch V2 A R Adapter Houlsby et al. (2019) VPT Jia et al. (2022) LoRA Hu et al. (2021) NOAH Zhang et al. (2022) GLoRA (0.29M) 70.5 70.5 70.8 71.5 78.3 16.4 18.3 20.0 24.8 30.6 59.1 58.0 59.3 66.1 67.5 5.5 4.6 6.9 11.9 13.3 22.1 23.2 23.3 28.5 31.0 Table 4: Inference efficiency comparison of GLoRA with existing methods. Throughput (imgs/sec) Method \u2191 #Param(M) \u2191 FLOPs(G) bs = 1 bs = 4 bs = 16 Full tuning 0 0 91.5 375.7 539.5 VPT Jia et al. (2022) Adapter Houlsby et al. (2019) AdaptFormer Chen et al. (2022) NOAH Zhang et al. (2022) 0.55 0.16 0.16 0.12 5.60 0.03 0.03 0.02 86.1 70.9 71.4 72.1 283.5 306.6 309.9 312.7 381.5 504.7 508.1 492.9 LoRA Hu et al. (2021) GLoRA 0 0 91.5 375.7 539.6 (Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets. Aligning with preceding research, we limit the number of training examples per class to 16 for this experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned model stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor- mance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on ImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial boost in comparison to existing methods. When"}