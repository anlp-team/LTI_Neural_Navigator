{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Paaploss:_A_Phonetic-Aligned_Acoustic_Parameter_Loss_for_Speech_Enhancement_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How is the acoustic improvement calculated in the analysis described in the text?,        answer: The acoustic improvement is calculated by taking the difference in acoustic parameters for clean and enhanced speech for each frame, and then averaging these differences per phoneme by the number of frames.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " What is the goal of the PAAP Loss mentioned in the text?,        answer: The goal of the PAAP Loss is to learn the relations between phonemes and acoustic parameters over time, and fine-tune enhancement models to account for this.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " What specific types of phonemes show the highest improvements in loudness in the analysis?,        answer: Plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/ show the highest improvements in loudness, with an average improvement of around 90%.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " How does the PAAP Loss contribute to the performance of speech enhancement models?,        answer: Experiments show that models fine-tuned with PAAP Loss produce speech with more improvement in acoustic parameters for the specific phonemes that are relevant for that particular parameter, leading to performance increases across all evaluation metrics.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " What is the overall acoustic improvement percentage for vowels compared to consonants?,        answer: The overall acoustic improvement of vowels is around 45%, which is higher than any group of consonants.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " Which specific consonants besides vowels showed high improvement in the analysis?,        answer: The nasals /N/ and /M/, as well as liquid consonants /L/ and /R/, showed high improvement in acoustic parameters.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " What is the main purpose of the phonetically-aligned acoustic parameter loss (PAAP) described in the text?,        answer: The PAAP loss minimizes the differences between important temporal acoustic parameters that are weighted by phoneme types in speech enhancement.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " What types of phonemes are used as classification standards for consonants in the analysis?,        answer: The place of articulation, including dorsals, labials, coronoals, and /HH/ for larynx articulation, are used as classification standards for consonants.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " How are the per-phoneme acoustic parameter improvements represented in the analysis?,        answer: The per-phoneme acoustic parameter improvements for loudness and F1 frequency are represented in plots where each phoneme is one point, with colors/shapes indicating different phoneme categories.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}, {"question": " What is the significance of the clustering of improvement for F1 frequency in the analysis?,        answer: The clustering of improvement for F1 frequency shows that nearly all the highest improvements are seen with vowels, indicating that formant structure is more important for vowels than consonants.    ", "ref_chunk": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}], "doc_text": "acoustic parameter. Now we break down the analysis further by showing the improvement in each acoustic parameter segmented by phoneme. The acoustic improvement is calculated by \ufb01rst creating phoneme alignments with the phonetic aligner on the clean speech. Then for each frame, we take the difference in acoustic parameters for clean and enhanced speech, and add this difference to the running total of the corresponding aligned phoneme. At the end, we average the differences per phoneme by the number of frames. We connect this analysis with the acoustic-phonetic properties mentioned in the introduction. Recall that plosives have very char- acteristic behavior with amplitude features. Also recall that vowels and nasals have speci\ufb01c formant characteristics. We include plots of per-phoneme acoustic parameter improvement for loudness and F1 frequency to represent the amplitude and formant characteristics, respectively. We plot the phoneme-dependent improvement for loudness and formant-1 (F1) frequency in Fig. 2. Each phoneme represents one point, where the colors/shapes indicate different phoneme cate- gories. We separate out vowels, and then use the place of articulation as the classi\ufb01cation standard of consonants. This includes dorsals, labials and coronals, which correspond to consonants where the articulation is performed with tongue dorsum, lips, and tongue front respectively. We also separate /HH/ as the only consonant in English with the place of articulation in the larynx. Therefore, we use \ufb01ve different colors/shapes in total to represent phoneme categories in the \ufb01gure. With this knowledge, we can see that our phonetically-aligned acoustic parameter loss results in the expected improvements given the above domain knowledge. The highest improvements in loud- ness are in plosives such as /B/, /P/, /K/, /G/, /D/, and /DH/, where the average improvement is around 90%. The goal of the PAAP Loss was to learn the relations between phonemes and acoustic pa- rameters over time, and \ufb01ne-tune enhancement models to account for this. Now we observe models \ufb01ne-tuned with PAAP Loss pro- duce speech with more improvement in acoustic parameters for the speci\ufb01c phonemes that are relevant for that particular parameter. (2) (3) Fig. 2: Reduction in error of loudness / F1 frequency vs. average value of acoustic parameter for each phoneme. We also see the expected clustering of improvement for F1 fre- quency. Nearly all the highest improvements are seen with vow- els, as formant structure is more important for vowels than conso- nants. The overall acoustic improvement of vowels is around 45%, higher than any group of consonants. The nasals /N/ and /M/, also mentioned in the introduction for their formant structure, showed similar improvements to many vowels. The other consonants that showed high improvement, /L/ and /R/ are liquid consonants, which are known to be more similar to vowels than other consonants. 5. CONCLUSION In this work, we propose a novel auxiliary objective for speech en- hancement, the phonetic-aligned acoustic parameter (PAAP) loss, which minimizes the differences between important temporal acous- tic parameters that are weighted by phoneme types. We \ufb01ne-tune competitive speech enhancement models with the addition of PAAP Loss, and experiments show that performance increases across all evaluation metrics, including measures of perceptual quality, and WER from competitive ASR models. We provide a detailed analysis of the phoneme-dependent acoustic improvement to show that the acoustic parameters improve most in expected phoneme categories. 6. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [37], which is supported by National Science Foundation grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [38], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 7. REFERENCES [1] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, \u201cConvolutional-recurrent neural networks for speech enhancement,\u201d in Proc. ICASSP, IEEE, 2018, pp. 2401\u20132405. [2] F. Weninger, F. Eyben, and B. Schuller, \u201cSingle-channel speech sep- aration with memory-enhanced recurrent neural networks,\u201d in Proc. ICASSP, 2014, pp. 3709\u20133713. [3] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015. [4] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in LVA/ICA, 2015. [5] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPhonetic feedback for speech enhancement with and without parallel speech data,\u201d in Proc. ICASSP, IEEE, 2020, pp. 6679\u20136683. [6] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe inter- speech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech, 2020. [7] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in Proc. ICASSP, IEEE, 2022, pp. 9271\u20139275. [8] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d Proc. Interspeech, pp. 1816\u20131820, 2019. [9] J. Turian and M. Henry, \u201cI\u2019m sorry for your loss: Spectrally-based audio distances are bad at pitch,\u201d arXiv preprint arXiv:2012.04572, 2020. [10] P. Plantinga, D. Bagchi, and E. Fosler-Lussier, \u201cPerceptual loss with recognition model for single-channel enhancement and robust ASR,\u201d arXiv preprint arXiv:2112.06068, 2021. [11] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan, G. J. Mysore, and Z. Jin, \u201cA differentiable perceptual audio metric learned from just noticeable differences,\u201d Proc. Interspeech, 2020. [12] S.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao, \u201cMetricgan+: An improved version of metricgan for speech enhancement,\u201d Proc. Interspeech, 2021. [13] J. M. Martin-Do\u02dcnas, A. M. Gomez, J. A. Gonzalez, and A. M. Peinado, \u201cA deep learning loss function based on the perceptual evaluation of the speech quality,\u201d IEEE Signal Processing Letters, vol. 25, no. 11, pp. 1680\u20131684, 2018. [14] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, \u201cDNN- based source enhancement self-optimized"}