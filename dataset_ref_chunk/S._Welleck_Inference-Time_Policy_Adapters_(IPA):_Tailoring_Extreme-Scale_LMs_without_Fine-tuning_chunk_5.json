{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Inference-Time_Policy_Adapters_(IPA):_Tailoring_Extreme-Scale_LMs_without_Fine-tuning_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the study mentioned in the text?", "answer": " The main focus is on evaluating IPA for controlling the safety of a dialogue model.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " What is the DI-ASAFETY dataset and how is it used in the study?", "answer": " DI-ASAFETY is a dataset containing 54K context-sensitive unsafe examples. It is used to train a dialogue safety classifier by providing human-written safe and unsafe responses.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " How is the safety of dialogue responses evaluated in the study?", "answer": " The safety of dialogue responses is evaluated using a classifier score as an automatic measure of safety, as well as a human evaluation of safety and coherence on 200 examples through Amazon Mechanical Turk.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " Which dialogue models are compared with IPA in the study?", "answer": " Blenderbot-3B-distill, DialoGPT, GODEL, and ChatGPT are compared with IPA.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " What is the reward chosen to control the response safety while preserving dialogue quality in the study?", "answer": " The reward chosen is the product of safety score from the dialogue safety classifier, coherence score, and engagingness scores from UniEval-Dialogue.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " How does IPA perform compared to Blenderbot-3B-distill and other dialogue models in terms of safety and coherence?", "answer": " IPA significantly improves dialogue safety and coherence compared to Blenderbot-3B-distill, surpassing other dialogue models.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " What issue does the study aim to address in knowledge-grounded dialogue systems?", "answer": " The study aims to address the issue of models generating hallucinations containing unverifiable information.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " What dataset is used to evaluate faithfulness of responses in the knowledge-grounded dialogue experiment?", "answer": " The Wizard of Wikipedia data and the FaithDial dataset are used to evaluate faithfulness of responses.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " What models are used as baselines in the knowledge-grounded dialogue experiment?", "answer": " Models such as GPT-2, DialoGPT, DoHA, T5, T5-CTRL, and T5-LossTruncation are used as baselines.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}, {"question": " How does IPA perform in generating faithful dialogue responses compared to supervised models in the knowledge-grounded dialogue experiment?", "answer": " IPA performs better in generating faithful dialogue responses compared to supervised models that struggle due to poor data quality of their supervision dataset.", "ref_chunk": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}], "doc_text": "Our promising results once again showcase the effectiveness and efficiency of IPA. \u00d7 4.4 Dialogue Safety Control Existing dialogue systems often fail to respond safely to potentially unsafe user utterances (Kim et al., 2022), limiting their deployment in real- world applications. Here, we aim to evaluate IPA for controlling the safety of a dialogue model. Datasets and Metrics. We experiment on DI- ASAFETY (Sun et al., 2022), a challenging dataset containing 54K context-sensitive unsafe examples. The task is to generate a coherent response to a po- tentially unsafe utterance while avoiding offensive, harmful, toxic or biased language. DIASAFETY contains human-written safe and unsafe responses which we use to train a dialogue safety classifier. We use the classifier score as an automatic mea- sure of safety. In addition, we conduct a human evaluation of safety and coherence (3-point Likert scale) on 200 examples through Amazon Mechani- cal Turk; see Appendix E Figure 4 for details. Setup and Baselines. We apply IPA to tailor the Blenderbot family models (Roller et al., 2021), which are pretrained dialogue agents. Specifically, we use Blenderbot-3B-distill as the frozen base pol- icy, a samller Blenderbot-1B-distill as the approxi- mate policy and initialize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm for adapter training. To preserve the dialogue quality while controlling the response safety, we choose our reward to be the product of the safety score from our dialogue safety classifier, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022).9 i.e., We compare IPA with its base policy, Blenderbot-3B-distill, and other off-the-shelf di- alogue models including DialoGPT (Zhang et al., 2020), GODEL (Peng et al., 2022) as well as Chat- GPT (OpenAI, 2022). ChatGPT is known to have safeguards through content filtering and is consid- ered a strong baseline. Results. As shown in Table 5, IPA significantly improves dialogue safety and coherence compared to its base policy Blenderbot-3B-distill, surpass- ing other dialogue models including DialoGPT and GODEL. In comparison with ChatGPT, IPA achieves comparable performance on safety based 7Average pairwise agreements are 0.88 and 0.82 with GPT2-XL and GPT-3, respectively. 8Human pairwise agreements are 0.84 and 0.87 for safety and coherence. 9https://github.com/maszhongming/UniEval on both automatic and human evaluation while showcasing improved coherence. Upon further in- vestigation, we found that ChatGPT often generates canned responses like \"I\u2019m a language model; I\u2019m not allowed...\" as hard safeguards, which hurts the coherence and naturalness of the dialogue flow. On the other hand, Blenderbot tailored by IPA can gen- erate safe responses that are coherent, natural, and human-like. Our results demonstrate the potential of IPA to enhance controllability in various NLP applications beyond conditional text generation. 4.5 Knowledge-grounded Dialogue Ideally, knowledge-grounded dialogue systems should generate responses that are faithful to the given knowledge. However, models tend to gen- erate hallucination containing unverifiable infor- mation (Dziri et al., 2022a; Rashkin et al., 2021a; Dziri et al., 2022c). To address this undesirable behavior, we use IPA to tailor dialogue model to- wards generating more faithful content. Given the knowledge K and the conversation history H, the task is to generate a response r that\u2019s faithful to K and coherent with H. Dataset and Metrics We evaluate on the Wizard of Wikipedia (WoW) data. WoW (Dinan et al.) involves a Wizard and an Apprentice engaging in a conversation. The Wizard\u2019s role is to provide information on a specific topic, while the Appren- tice\u2019s task is to seek further details. WoW has been shown to suffer from hallucinations (Dziri et al., 2022b), in more than 60% of the turns, making it a valuable dataset for studying hallucination issues. FaithDial (Dziri et al., 2022a) is a hallucination- free benchmark created by modifying the halluci- nated responses within the WoW dataset. We use the FaithDial test data at test time to evaluate the faithfulness of responses and compare them against the knowledge snippets and gold responses. To measure faithfulness, we use the critic model (Dziri et al., 2022a), which returns the probability of an given utterance being identified as faithful. Additionally, we use BERTScore to measure the semantic similarity between the generated response r and the knowledge K, and the token-level F1 score to rate the lexical overlap between r and K. To measure coherence and engagingness, we use the UniEval model (Zhong et al., 2022). Setup and Baselines Similar to the dialogue safety experiment, we use the Blenderbot-{3, 1}B- distill model (Roller et al., 2021) as our base policy Dialogue Model Critic BERTScore F1 Coherence Engaging supervised baseline GPT-2 DIALOGPT DOHA T5 T5-CTRL T5-LT off-the-shelf dialogue model BlenderBot 39.9 40.6 46.8 53.5 54.8 58.6 0.29 0.34 0.32 0.41 0.45 0.43 47.7 53.5 56.1 61.7 65.2 65.0 0.77 0.83 0.88 0.86 0.83 0.83 1.26 1.32 1.33 1.28 1.21 1.21 10.3 0.12 9.8 0.92 1.21 IPA- (BlenderBot) 76.6 0.68 80.1 0.91 1.34 Table 6: Evaluation results for Knowledge-Grouded Dialogue generations on Faithdial. We use off-the-shelf Blenderbot as the base policy to tailor. and approximate policy respectively, and initial- ize the policy adapter with a Blenderbot-1B-distill model. We use QUARK as the RL algorithm. To preserve coherence and engagingness while ensur- ing the faithfulness of a dialogue response, we choose our reward to be the product of the faithful- ness score from the critic model described above, as well as coherence and engagingness scores from UniEval-Dialogue (Zhong et al., 2022). We compare to previously baselines from Dziri et al. (2022a), supervised models fine-tuned on WoW, including GPT2, DialoGPT (Zhang et al., 2020), DoHA (Prabhumoye et al., 2021) T5 (Raffel et al., 2020), T5-CTRL (Rashkin et al., 2021b), and T5-LossTruncation (Kang and Hashimoto, 2020). We also compare against the base policy, off-the- shelf BlenderBot model (Roller et al., 2021). Results As shown in Table 6, supervised mod- els struggle to generate faithful dialogue response grounded on the given knowledge. This is mainly because of the poor data quality of their supervi- sion dataset: WoW has been shown to suffer from hallucinations in more than 60% of the turns (Dziri et al.,"}