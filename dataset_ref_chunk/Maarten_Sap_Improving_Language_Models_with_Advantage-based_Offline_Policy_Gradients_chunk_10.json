{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Improving_Language_Models_with_Advantage-based_Offline_Policy_Gradients_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the definition of \u03c0\u03b8old in the given text?", "answer": " The ratio of current policy probability and old policy probability", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " How does PPO update its objective during training?", "answer": " Using stochastic gradient descent on the rollout of actions collected using \u03c0\u03b8old", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " In the context of A-LOL, what does A\u03c0ref represent?", "answer": " The advantage estimate of the reference policy for taking an action at given the state st", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " What method is used to implement A-LOL and its variants?", "answer": " The huggingface Transformers library", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " How many training instances are there in the Helpful and Harmless Assistant task?", "answer": " 143,000", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " What learning rate is selected for all offline RL experiments?", "answer": " 0.0002", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " How many steps are reduced for A-LOL methods due to identified negative advantages?", "answer": " 6,093 steps", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " What method-specific hyperparameters are used in DPO?", "answer": " \u03b2 = 0.1 as the KL weight", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " What batch size is used for all methods during training?", "answer": " 16", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}, {"question": " What is used to encourage diversity during online exploration in PPO?", "answer": " Top-p sampling with p = 0.95", "ref_chunk": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}], "doc_text": "\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and \u02c6At is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective. Specifically, \u03c0old is replaced with \u03c0ref and the \u02c6At is swapped with A\u03c0ref . Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training. B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS B.1 IMPLEMENTATION DETAILS We implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, around 46K instances were identified with negative advantages and were discarded from training. Therefore, we reduce the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 3,000 PPO update steps, where each step consists of a rollout of batch size 16 that is trained for 4 internal PPO epochs (\u2248192K internal training steps). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We also reduced the learning rate to 0.00002 for PPO, as higher 16 Preprint under review. Figure 4: Test set distribution of Rewards achieved by Reference policy and top performing offline RL methods. Figure 5: Reddit Response Generation Task: Comparing A-LOL seq. and reference policy test reward distribution for every scoring function. A-LOL seq. trained on downvoted comments almost matches the distribution with A-LOL seq. trained on upvoted comments on all scoring functions except Probability of downvote. learning rates were leading to more instability. Including the cost of generating the responses, PPO takes almost 3\u00d7 the time taken for offline RL training. In total, we run PPO with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO updates. The validation reward plots of each seed are show in Figure 3. B.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP For a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details. Conversation: <the user query> Response A: <A model response> Response B: <B model response> FIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" 17 Preprint under review. Figure 6: A-LOL Ablations with different clip values (\u03f5) and priority vs random sampling. or \"neither\" to indicate which response is more helpful and safer. Your response should use the format: Comparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> #################### Examples of Safety and Helpful human annotation setup are given in Figure 7. B.3 A-LOL ABLATION EXPERIMENTS We study the effects of priority sampling in A-LOL. In our main experiments, we prioritize high- advantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority sampling, we compare it against random sampling from the entire dataset. We clamp the negative advantage instances within randomly sampled batches to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.16 B.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES We manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test the last two turns of the"}