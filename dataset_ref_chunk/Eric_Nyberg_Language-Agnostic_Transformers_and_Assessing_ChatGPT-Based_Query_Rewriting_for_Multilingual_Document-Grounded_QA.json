{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Language-Agnostic_Transformers_and_Assessing_ChatGPT-Based_Query_Rewriting_for_Multilingual_Document-Grounded_QA.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two significant challenges posed by the development of multilingual dialogue systems?", "answer": " The two significant challenges are understanding queries in any language and retrieving relevant passages from a collection of documents in multiple languages, and generating appropriate responses in the same language.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " Which model is used for document retrieval in the multilingual question-answering model discussed?", "answer": " The multilingual DPR (mDPR) model is used for document retrieval.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What task presents new challenges in passage retrieval compared to conventional retrieval tasks?", "answer": " Passage retrieval in conversational question answering (QA) presents new challenges as each question must be interpreted within the context of the ongoing dialogue.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " How does the mDPR model identify relevant passages for a given question?", "answer": " The mDPR model employs a bi-encoder architecture, utilizing a pre-trained multilingual model to encode the questions and passages independently. The encoded representations are then compared using a maximum inner product search to identify relevant passages for a given question.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What two paradigms for multilingual pre-trained transformer models are evaluated in the study?", "answer": " The two paradigms evaluated in the study are a language-agnostic paradigm and a language-aware paradigm.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What is the language-agnostic model used for multilingual sentence embedding in the study?", "answer": " The Language-Agnostic BERT Sentence Embedding (LaBSE) model is used for multilingual sentence embedding.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What is the impact of query rewriting techniques on multilingual document-grounded question-answering systems according to the study?", "answer": " The study found that, for the examples examined, query rewriting does not enhance performance compared to the original queries.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What are the limitations mentioned in the conclusion of the study?", "answer": " The limitations mentioned include budget and credit constraints affecting the query rewriting observations and the limited generalizability of findings due to the sample size.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What are the potential future works suggested by the study?", "answer": " Potential future works suggested include testing the performance of query rewriting using in-context examples and testing query rewriting using open-source models and advanced fine-tuning methods.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}, {"question": " What is the effect of Fusion-in-Decoder (FiD) on reader performance in the study?", "answer": " The study observed an improvement of approximately 4.2%, 3.51%, and 2.80% in F1, BLEU, and Rouge-L scores, respectively when including FiD along with mT5 in the case of both Vietnamese (vi) and French (fr) languages.", "ref_chunk": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}], "doc_text": "1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023\u2014w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-\nLanguage-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj\u2217,SohamDineshTiwari\u2217,MitaliPotnis\u2217,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated\n101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101\u2013108 July 13, 2023 \u00a92023 Association for Computational Linguistics\n\n\nagnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument\n2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM\u2019sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-\n102\n\n\n0.89\n3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy\u2019soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.\nFinetuned\n0.92\n0.80\n0.72\n0.95\n0.90\n0.90\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n0.76\n0.87\n0.87\n0.57\n0.65\n0.65\n0.83\n0.82\n0.82\nModel\n0.78\nfr\nfr\n0.84\nLaBSE\nLaBSE\nLaBSE\nR@1\nR@10\nXLMR\nXLMR\nXLMR\n0.55\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\nzh+en\n0.86\nR@20\n0.45\nvi\nvi\nEvaluated\n0.75\n0.75\n0.67\nPretrained\nR@5\nTable1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments\n103\n\n\n0.89\nTable2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations\u2019lastturns,andChatGPT\u2019stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof\"halluci-nations\"inChatGPT\u2019sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.\nvi(ChatGPT)\n0.65\n0.38\n0.33\n0.26\n0.78\n0.70\nEvalOn\n0.49\n0.84\n0.84\n0.58\nfr(ChatGPT)\nMultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.\nTrainedon\nfr(Raw)\n0.77\nR@1\nR@10\n0.16\n0.46\nR@20\nen(ChatGPT)\nen(ChatGPT)\n0.45\n0.51\nen(Raw)\nen(Raw)\nR@5\nvi(Raw)\n104\n\n\nFigure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation\nFigure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.\n105\n\n\nReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91\u201399,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547\u2013564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454\u2013470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878\u2013891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162\u20136176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874\u2013880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769\u20136781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045\u20133059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582\u20134597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1\u201367.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?\n106\n\n\nJulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000\u201310014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.\n107\n\n\n62.96\n64.73\n65.34\nTable3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.\nfr+vi\nfr+vi\nfr+vi\nfr+vi\n63.03\n42.22\n59.89\n58.55\n40.87\nBLEU\nModel\n41.47\nfr\nfr\nfr\nfr\n45.93\n45.72\n39.42\n64.83\nvi\nvi\nvi\nvi\nEvaluated\n65.61\nmT5+FiD\nmT5+FiD\nmT5+FiD\nmT5\nmT5\nmT5\n60.00\n68.22\nROUGE-L\n56.76\nF1\nPre-trained\nAAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.\n62.43\n108"}