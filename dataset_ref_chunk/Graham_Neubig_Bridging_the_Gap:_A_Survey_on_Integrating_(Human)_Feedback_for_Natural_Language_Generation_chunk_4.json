{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Bridging_the_Gap:_A_Survey_on_Integrating_(Human)_Feedback_for_Natural_Language_Generation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the alignment objective that has been extensively studied in the AI safety and alignment literature?,        answer: The alignment objective that has been extensively studied in the AI safety and alignment literature is to ensure that models are aligned with the intended objectives and produce desired outcomes.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " Why is extra care given in commercial machine translation to ensure that models do not mistranslate business-critical information?,        answer: Extra care is given in commercial machine translation to ensure that models do not mistranslate business-critical information because it is important for the accuracy and reliability of the translations.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " According to the text, what does the alignment objective of generating summaries entail?,        answer: The alignment objective of generating summaries entails including all core information, even if it means the summaries are slightly longer.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " What is an example of a harmlessness objective discussed in the text?,        answer: An example of a harmlessness objective discussed in the text is not producing toxic text or violating certain norms.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " How do most works in machine translation leverage feedback to improve the quality of translation?,        answer: Most works in machine translation leverage feedback related to the quality of translation to improve aspects such as relevance, consistency, and accuracy.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " What role does human feedback play in optimizing model parameters directly?,        answer: Human feedback can be used to optimize model parameters directly by formulating it as an optimization problem based on which an improved model can be obtained.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " Define the feedback-based imitation learning approach mentioned in the text.,        answer: The feedback-based imitation learning approach involves using human feedback to optimize the model through supervised learning with a dataset of positively-labeled generations and corresponding inputs.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " What is one downside of approaches that disregard generations without positive feedback, as mentioned in the text?,        answer: One downside of approaches that disregard generations without positive feedback is that they may miss out on useful information contained in those generations that could help optimize the model.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " How do Thoppilan et al. (2022b) use feedback to fine-tune their model?,        answer: Thoppilan et al. (2022b) collect feedback on whether their model violates a set of safety objectives and use it to fine-tune the model.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}, {"question": " What are the three main categories of techniques suggested to optimize model parameters using human feedback?,        answer: The three main categories of techniques suggested to optimize model parameters using human feedback are feedback-based imitation learning, joint-feedback modeling, and reinforcement learning.    ", "ref_chunk": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}], "doc_text": "to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective. Bai et al. (2022a) explicitly divided the prob- lem of \u201caligning\u201d a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2 Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021). 2We mostly ignore the proposed honesty aspect, as none of these works tackle this directly. 5 Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness. 4 Directly Leveraging Human Feedback In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures. 4.1 Optimizing for Human Feedback Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \u201coptimizable\u201d, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem: \u03b8\u22c6 = arg max Ex\u223cD[h(x, M\u03b8(x))]. \u03b8 Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, \u03b8, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+. Preprint This can be achieved by minimizing the loss: \u03b8\u22c6 = arg min |D+| (cid:88) L(i)(\u03b8) \u03b8 L(i)(\u03b8) = \u2212 log p\u03b8 i=1 (cid:16) y(i) | x(i)(cid:17) An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model\u2019s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI\u2019s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model. (2) On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural"}