{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Squeeze,_Recover_and_Relabel:_Dataset_Condensation_at_ImageNet_Scale_From_A_New_Perspective_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main difference between distribution matching and BN layer matching in the context of ViT recovery process?,answer: Distribution matching aligns feature distributions of original and synthetic training data, while BN layer matching is solely performed on Batch Normalization layers using statistical properties.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " How are BN statistics calculated in the context of BN-matching?,answer: Referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " What is BN-ViT and how does it broaden the applicability of the data condensation approach?,answer: BN-ViT replaces LayerNorm by BN layers and adds additional BN layers in-between two linear layers, broadening the applicability of data condensation architecture from ConvNets to ViTs.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " What is the objective of Stage-2 Recover in the ViT recovery process?,answer: Stage-2 Recover involves reconstructing retained information back into the image space using class labels, regularization terms, and BN trajectory alignment.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " What regularization terms are discussed and used in Stage-2 Recover of the ViT recovery process?,answer: The regularization terms discussed and used in Stage-2 Recover are image prior regularizers of \u21132 and total variation (TV).", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " How does DeepInversion utilize the feature distribution regularization term to improve the quality of generated images?,answer: DeepInversion utilizes the feature distribution regularization term as a recovering loss term to improve the quality of generated images.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " What is the purpose of multi-crop optimization in the process of image synthesis?,answer: Multi-crop optimization aims to enhance the informational content of synthesized images by randomly cropping and resizing regions during each iteration.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " What is the objective of Stage-3 Relabel in the ViT recovery process?,answer: Stage-3 Relabel aims to match the multi-crop optimization strategy and reflect soft labels for the recovered data.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " How does the proposed approach reduce compute and memory consumption?,answer: The proposed approach decouples real and synthetic data during training, requiring minimal memory and reducing computational overhead typically incurred by existing solutions.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}, {"question": " What is the significance of the two-stage process of squeezing and recovering in the proposed approach?,answer: The two-stage process of squeezing and recovering helps reduce computational and memory overhead, allowing for scaling up training strategies for larger datasets and models.", "ref_chunk": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}], "doc_text": "Enabling BN Layers in ViT for Recovering Process: In contrast to distribution matching [10] that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer [20]. Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in [21]. This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well. (x,y)\u2208T [y log (p (x))]. Stage-2 Recover ( ): This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows: arg min Csyn,|C| \u2113 (\u03d5\u03b8T ((cid:101)xsyn), y) + Rreg where Rreg is the regularization term. \u03d5\u03b8T is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the (cid:101)xsyn as a single-level training process. Following [18, 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \u21132 and total variation (TV) proposed in [18]: Rprior ((cid:101)xsyn) = \u03b1tvRTV((cid:101)xsyn) + \u03b1\u21132R\u21132 ((cid:101)xsyn) (6) 4 (4) (5) where R\u21132 = \u2225(cid:101)xsyn\u22252, this regularizer encourages image to stay within a target interval instead ((cid:101)xi,j+1 \u2212 (cid:101)xij)2 + ((cid:101)xi+1,j \u2212 (cid:101)xij)2(cid:17) \u03b2 of diverging. RTV = (cid:80) where \u03b2 is used to balance the sharpness of the image with the removal of \u201cspikes\u201d and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery. (cid:16) 2 i,j Learning Condensed Data with BN Consistency: DeepInversion [20] utilizes the feature distribu- tion regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as: (cid:88) (cid:13) (cid:13)\u03c32 (cid:88) l | T (cid:1)(cid:13) (cid:13)2 l ((cid:101)x) \u2212 E (cid:0)\u03c32 \u2225\u00b5l((cid:101)x) \u2212 E (\u00b5l | T )\u22252 + RBN((cid:101)x) = \u2248 l (cid:88) (cid:13) (cid:13)\u00b5l((cid:101)x) \u2212 BNRM l (cid:13) (cid:13)2 + (cid:88) l (cid:13) (cid:13)\u03c32 l ((cid:101)x) \u2212 BNRV l (cid:13) (cid:13)2 l l l ((cid:101)x) are mean and variance. BNRM and BNRV where l is the index of BN layer, \u00b5l((cid:101)x) and \u03c32 are running mean and running variance in the pre-trained model at l-th layer, which are globally counted. l l Multi-crop Optimization: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224 \u00d7 224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ): To match our multi-crop optimization strategy, also to reflect the true soft (cid:101)yi = \u03d5\u03b8T ((cid:101)xRi ) (8) where (cid:101)xRi is the i-th crop in the synthetic image and (cid:101)yi is the corresponding soft label. Finally, we can train the model \u03d5\u03b8Csyn on the synthetic data using the following objective: Lsyn = \u2212 (cid:88) (cid:101)yi log \u03d5\u03b8Csyn ((cid:101)xRi) i We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage. 3 Experiments In this section, we evaluate the performance of our proposed SRe2L over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of SRe2L in large-scale datasets, cross- architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with"}