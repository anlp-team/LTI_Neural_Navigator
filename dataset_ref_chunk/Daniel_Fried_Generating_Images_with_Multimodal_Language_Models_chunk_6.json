{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Generating_Images_with_Multimodal_Language_Models_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one of the compelling applications of GILL mentioned in the text?", "answer": " Its ability to generalize to many different tasks due to LLM pretraining and freezing.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " How does GILL outperform retrieval models like FROMAGe in some cases?", "answer": " GILL is able to outperform retrieval models like FROMAGe on examples where FROMAGe is unable to retrieve relevant images.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " What does the GILLMapper module do?", "answer": " It is a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors to enable image synthesis.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " How does the number of [IMG] tokens (r) affect image generation in GILL?", "answer": " As r increases, image generation generally improves, plateauing around r = 4.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " What results are highlighted by the evaluation on VIST recall@k (\u2191) for GILL compared to other models?", "answer": " GILL performs comparably or better compared to prior approaches, with R@1 at 22.3, R@5 at 45.0, and R@10 at 29.8.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " What effect does increasing input contexts have on the performance of GILL on VIST?", "answer": " The performance of GILL generally improves with increasing input contexts on VIST, showing better results with multimodal context.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " What does the conclusion mention as a promising direction for future research based on the proposed method?", "answer": " Scaling up the LLM backbone, image generation backbone, or visual processing model are promising directions that will likely induce even stronger vision-and-language capabilities.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " What are the key components compared in the effectiveness analysis of GILLMapper against other models?", "answer": " GILLMapper is compared against a linear layer, a multilayer perceptron (MLP), and a 4-layer bidirectional transformer encoder.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " What are the metrics used to measure the effectiveness of GILLMapper compared to other models?", "answer": " The metrics used are Fr\u00e9chet Inception Distance (FID) on the CC3M validation set and CLIP Similarity on VIST.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}, {"question": " How does the ablated model that removes the retrieval loss perform compared to the original model on VIST?", "answer": " The ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, comparable to the original model.", "ref_chunk": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}], "doc_text": "more similar to the groundtruth image. When the full 10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP 8 Table 3: Image generation performance on CC3M [52] and VIST [28] with different text mapping networks. Table 4: GILL image generation re- sults on CC3M [52] with different number of image tokens (r). CC3M VIST Model FID (\u2193) CLIP Sim (\u2191) CC3M VIST Stable Diffusion [49] 13.94 0.598 r FID (\u2193) CLIP Sim (\u2191) Ours + Linear Ours + 3-layer MLP Ours + Transformer Encoder Ours + GILLMapper 15.50 15.33 16.30 15.31 0.500 0.502 0.605 0.641 1 2 4 8 15.93 15.32 15.32 15.31 0.631 0.629 0.642 0.641 2 cap 0.56 3 cap, 2 img Captions + Images 4 cap, 3 img 0.62 Captions only 0.58 0.60 5 cap, 4 img 3 cap 0.64CLIP Similarity 5 cap Performance With Increasing Context on VIST 2 cap, 1 img 4 cap 1 cap Figure 6: Performance of GILL on VIST generation. Table 5: Contextual image retrieval on VIST (5 captions, 4 images as input). \u2020 indicates results from [31]. VIST Recall@k (\u2191) Model CLIP ViT-L [43]\u2020 FROMAGe [31]\u2020 GILL (Ours) R@1 R@5 R@10 8.8 18.2 20.3 22.3 42.7 45.0 29.8 51.8 53.7 Similarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of our model on handling long dialogue-like text inputs. 4.2 Qualitative Results Finally, one of the more compelling applications of GILL is perhaps its ability to generalize to many different tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities in Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as FROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also generally more sensitive to input context compared to Stable Diffusion [49], and can condition on image inputs, in addition to text, to generate more visually and semantically relevant image outputs. 5 Analysis Contextual Image Retrieval In addition to generation, GILL is capable of image retrieval condi- tioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that GILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the image generation objective does not cause image retrieval performance to deteriorate. The Effect of Context GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of GILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions and 1 image are provided as context, the model significantly outperforms the model with 5 text-only captions, highlighting the value of multimodal context over unimodal context. Generation-Only Objective We investigate the effect of removing the retrieval loss (Eq. 4) from the training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity of 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and 0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance, although such a model would only be able to generate images and text and not retrieve images. These results also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has sufficient capacity to perform both generation and retrieval. 9 GILLMapper Module As described in Sec. 3.2, we propose the GILLMapper module, a lightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors. The output maps the LM embeddings into the input space of a text-to-image generation model, en- abling image synthesis. We run several baselines to compare effectiveness, comparing our proposed model against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and (3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token embeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better than these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion generation model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set (which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which is out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper, suggesting that they cannot generalize to longer sequences containing multiple images and texts. Number of [IMG] Tokens We experiment with varying the number of [IMG] tokens, r (Tab. 4). As r increases, generation generally improves, plateauing around r = 4. We observe that lower values of r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive. 6 Conclusion We proposed a method of mapping text-only LLMs to strong visual models. This enables them to learn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved images, and generated images. We show that it is possible to efficiently learn a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and that doing so effectively boosts image generation for tasks that require stronger language context dependence. Finally, we also showcased several compelling qualitative results on a variety of multimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models released in the future. Scaling up the LLM backbone, image generation backbone, or visual processing model, are promising directions that will likely induce even stronger vision-and-language capabilities. Acknowledgements This work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and DARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared Fernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback and helpful discussions on previous versions of this paper. References [1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine"}