{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Crossing_the_Threshold:_Idiomatic_Machine_Translation_through_Retrieval_Augmentation_and_Loss_Weighting_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of Figure 1?", "answer": " Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " How many seeds were the results in Figure 1 averaged across?", "answer": " 5 seeds", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What is the main topic of Figure 2?", "answer": " Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What trend is observed in the quality of translations as idioms become more frequent?", "answer": " The quality of translations increases", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What is the purpose of the loss weighting method in improving translation?", "answer": " To upweight potentially idiomatic phrases in the source language", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What does the kNN-MT method require to improve translation?", "answer": " Enough space on disk to save the datastores", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What type of weighting does upweighting refer to in the context of loss weighting?", "answer": " Sentence-level upweighting", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What is the main focus of Table 3?", "answer": " Performance of commercial systems on idiomatic, literal, and random test sets", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " What is the size of the \u2206LM-base transformer encoder-decoder model mentioned in the experimental settings?", "answer": " 360M parameters", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}, {"question": " How many steps was each model trained for in the experimental settings?", "answer": " 2 million steps", "ref_chunk": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}], "doc_text": "not be tracked. Informative context Uninformative context Figure 1: Accuracy of a transformer in translating a non-compositional phrase after training on datasets of different sizes, with different numbers of non-compositional patterns (only non-compositional translation accuracy is depicted). Results are averaged across 5 seeds, and standard deviation is shown. 0.34 0.8Percentiles 0.9050 ROUGE 0.4 0.4 0.4 0.9000 0.8Percentile of idioms in OS 0.8Percentile of idioms in OS 0.4 0.17Metric Score 0.30 0.16 0.8Percentiles 0.14 0.15 0.8975 BERTScore 0.44 METEOR 0.9075Score 0.42 0.46 0.2 0.48 0.2 0.2 0.2 0.50Metric Score 0.9025 BLEU 0.6 0.36Score 0.32 0.6 0.6 0.6 Language fiidiomatic filiteral firandom-out fridiomatic frliteral System DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base DeepL Google \u2206LM-base BLEU METEOR BERTScore 0.1001 0.0923 0.1608 0.2497 0.2250 0.3592 0.8866 0.8726 0.9126 0.1488 0.1398 0.2093 0.3908 0.3577 0.5050 0.9146 0.9017 0.9350 0.2052 0.2288 0.2365 0.4082 0.4357 0.4971 0.9103 0.9062 0.9145 0.1575 0.1261 0.2001 0.3278 0.2794 0.4393 0.9006 0.8808 0.9211 0.2219 0.2034 0.2778 0.4022 0.3830 0.5504 0.9122 0.9012 0.9377 Figure 2: Automatic metrics \u2013 Quality of DeepL French translations on idiomatic test set bucketed by idiom frequency. The bottom 20% of least common idioms are excluded, as they may occur fewer than 3 times and not be in our test set. frrandom-out jaidiomatic DeepL Google \u2206LM-base DeepL Google \u2206LM-base 0.2854 0.3103 0.2778 0.1172 0.0672 0.09048 0.4650 0.4922 0.5504 0.2735 0.1839 0.2998 0.9125 0.9149 0.9377 0.8932 0.8644 0.9234 Although it\u2019s impossible for us to determine what data these commercial systems were trained on, we examine the frequency of each idiom within OpenSubtitles as a proxy for its overall frequency in the training data, and bucket idioms into quin- tiles based on their occurrence frequency in source text. As idioms become more frequent, the quality of translations increases. An example of DeepL on the French idiom set is shown in Figure 2. Trends for other languages and systems are in Appendix H. This indicates that like in the synthetic experiments, there may be strong frequency effects on translation quality of idioms. jaliteral DeepL Google \u2206LM-base 0.1517 0.0937 0.1416 0.3440 0.2565 0.4222 0.9059 0.8829 0.9222 jarandom-out DeepL Google \u2206LM-base 0.1074 0.1079 0.0948 0.2934 0.2834 0.3436 0.8878 0.8829 0.8946 Table 3: Performance of commercial systems on id- iomatic, literal, and random test sets. There is a clear degradation in performance on idiomatic sentences. 5 Methods to Improve Non-Compositional Translation We explore two methods to improve translation, loss weighting and kNN-MT. These two methods are relatively simple to use, where loss weighting only requires a list of potentially idiomatic phrases in the source language, and kNN-MT only requires enough space on disk to save the datastores. More formally, we consider the basic case of autoregressive machine translation, with a set of parallel sentences in the source (X = {x(i)}N i=1) and target (Y = {y(i)}N i=1) language: D = {(x(i), y(i)), ..., (x(N ), y(N ))}. The model p\u03b8 with parameters \u03b8 is trained by minimizing the loss: L(\u03b8, D) = N (cid:88) \u2113(y(i), p\u03b8(x(i))) i=1 Upweighting here refers to sentence-level up- weighting, where there is a set of sentences A that we\u2019d like to upweight with a weight coefficient \u03b1. In this case, A would be potentially idiomatic sen- tences. We keep all other parameters for training the same as in the base model. L(\u03b8, D) = N (cid:88) \u03b11(x(i)\u2208A)\u2113(y(i), p\u03b8(x(i))) i=1 kNN-MT augments a translation model with a retrieval component (Khandelwal et al., 2021). Given each sentence (x, y), we construct a data- store with keys based on hidden representations constructed from the translation model, and values being the next word in the target sentence. During generation, a probability distribution over next words can be computed based on the retrieved next words and the distance of their keys to the current context. A parameter \u03bb controls inter- polation between the distribution over next words predicted by the base model, and the distribution predicted by the retrieved k neighbours.6 p(y(j) i |x(j), \u02c6 y(j) 1:i\u22121) = \u03bbpkNN(y(j)|x(j), + (1 \u2212 \u03bb)p\u03b8(y(j) i \u02c6 y(j) 1:i\u22121) |x(j), \u02c6y(j) 1:i\u22121) We also combine loss weighting with kNN-MT, where a model is trained with sentence upweighting and interpolated with a datastore based on repre- sentations from the upweight-trained model. Intuitively, these methods make sense to use for idiom translation \u2013 we have previously seen that one problem with non-compositional phrases may 6We run a hyperparameter search using the validation set to find the best kNN-MT settings for each language. Further details are in Appendix C. (2) (3) (4) simply be their rarity. Upweighting training ex- amples that contain idioms may help with under- representation. Furthermore, retrieving similar ex- amples may find occurrences of the same idiom which were translated correctly. 6 Experimental Settings 6.1 Experimental Settings We run experiments on \u2206LM-base, a transformer encoder-decoder model with 360M parameters, a larger version of which ranked first in the WMT21 multilingual translation task (Ma et al., 2021; on Machine Translation , WMT21). We train one \u2206LM model for each language pair. Each model was trained for 2 million steps, and the checkpoint with the best loss on the validation set was kept. Further details are in Appendix C. To decode, we used beam search with a beam size of 5. 6.2 Data Models were trained on OpenSubtitles for each lan- guage pair. Data from test sets were removed, and 10% of the remaining data was used as a validation set. There were 33.8M sentences in the fr-en train set, 22.0M in fi-en, and 1.6M in ja-en. 6.3 Evaluation We use multiple automatic metrics to evaluate trans- lation quality. However, due to the importance of accurate semantic evaluation, the authors (native English speakers and fluent in French and Japanese) conduct a human evaluation inspired by MQM (Lommel et al., 2014). Only errors that would fall under the\u201cterminology\u201d and \u201caccuracy\u201d error types are considered, as we are focused on severe semantic errors. We give a score of 0 for severe errors and a score of 0.5 for major errors. A score of 1 is"}