{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_UNSSOR:_Unsupervised_Neural_Speech_Separation_by_Leveraging_Over-determined_Training_Mixtures_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Who are the authors of the paper discussed in the text?", "answer": " Zhong-Qiu Wang and Shinji Watanabe", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What problem does the UNSSOR algorithm aim to solve?", "answer": " Unsupervised neural speech separation in reverberant conditions with multiple speakers", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What is the key insight behind UNSSOR?", "answer": " In over-determined conditions, a unique solution to speaker separation exists and can be leveraged as supervision", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What is the main contribution of the UNSSOR algorithm?", "answer": " Enforcing a linear-filter constraint to promote separation of speakers in over-determined mixtures", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What method is used to estimate speaker images and linear filters in UNSSOR?", "answer": " A deep neural network (DNN) is used to estimate speaker images and a sub-band linear prediction algorithm named FCP is used for linear filters", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " How does UNSSOR address the frequency permutation problem?", "answer": " By proposing a loss term based on minimizing intra-source magnitude scattering", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What is the difference between supervised and unsupervised neural speech separation models?", "answer": " Supervised models rely on paired clean and corrupted signals for training, while unsupervised models do not require labeled mixtures", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What is the cocktail party problem in the context of speech separation?", "answer": " Separating a recorded mixture of concurrent speech by multiple speakers into individual speaker signals", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " How does the over-determined conditions affect the ill-posed task of unsupervised speech separation?", "answer": " It turns the ill-posed problem into a well-posed one, providing a unique solution for separation", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}, {"question": " What is the purpose of blind deconvolution in the context of unsupervised neural speech separation?", "answer": " To estimate both the speaker images and linear filters without needing labeled mixtures", "ref_chunk": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}], "doc_text": "3 2 0 2 t c O 9 2 ] D S . s c [ 2 v 4 5 0 0 2 . 5 0 3 2 : v i X r a UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Zhong-Qiu Wang and Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA wang.zhongqiu41@gmail.com Abstract In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over- determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR. 1 Introduction In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) [1, 2], where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation [3] has been dramatically advanced by deep learning, since deep clustering [4] and permutation invariant training (PIT) [5] solved the label permutation problem. They (and their subsequent studies [6\u201326]) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms [3]. The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues [27, 28]. How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study. Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task [2], since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate supervision (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). estimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3\u201326] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever supervision that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise. Our insight is that, in multi-microphone over-determined conditions where the microphones out- number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a supervison (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers. Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi- microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include: We enforce a linear-filter constraint between each speaker\u2019s reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers. \u2022 We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP [29] based on the mixture and DNN estimates. We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP. Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation). 2 Related work Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30\u201334], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN"}