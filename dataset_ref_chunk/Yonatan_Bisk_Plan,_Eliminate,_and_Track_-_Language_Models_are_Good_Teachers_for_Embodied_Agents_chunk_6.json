{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_Plan,_Eliminate,_and_Track_-_Language_Models_are_Good_Teachers_for_Embodied_Agents_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the PET framework stand for?", "answer": " Plan, Eliminate, and Track", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " How many demonstrations were used from the training set in the experiment?", "answer": " 140", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " Which module has a significant impact when combined with Plan & Track?", "answer": " Eliminate module", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " What embedding is used in the Action Attention framework for sub-tasks?", "answer": " RoBERTa", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " Why do some objects in the AI2Thor simulator spawn in unexpected receptacles?", "answer": " Some objects do not spawn according to common sense.", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " For which tasks is sub-task planning/tracking particularly helpful?", "answer": " Tasks that require counting procedures", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " What is a limitation of the current Track module in the system design?", "answer": " It does not revisit finished sub-tasks.", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " What is a possible future improvement mentioned in the text related to the Track module?", "answer": " Adding sub-task-level dynamic re-planning", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " Which agent outperforms the BUTLER baseline according to the text?", "answer": " Action Attention agent", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}, {"question": " What does the PET framework not require in terms of fine-tuning?", "answer": " Fine-tuning", "ref_chunk": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}], "doc_text": "Track Action Attention + PET 25 25 35 52.5 9 11 15 27.5 Table 3. Comparison of di\ufb00erent ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insigni\ufb01cant e\ufb00ect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more signi\ufb01cant performance improvement. racies in the human goal speci\ufb01cations. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. incorrectly masks a receptacle that contains the object of interest so the agent fails to \ufb01nd such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment2, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the \u201cmistakes\u201d of our Eliminate module are reasonable. Track Module Experimentally, we \ufb01nd that sub- task planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Ta- ble ??, PET breaks the task of \u201cPlace two soapbar in cabinet\u201d into two repeating set of sub-tasks: \u201ctake soapbar\u2192place soapbar in/on cabinet\u201d. Sub-task plan- ning and tracking, therefore, simplify the hard problem of counting. Eliminate Module We observe that the main source of elimination error occurs when the module 2ai2thor.allenai.org/ithor/documentation/objects/object- types/ Plan, Eliminate, and Track Human Goal Speci\ufb01cation Examples Task Chill a cup and place it in the cabinet. policy (i.e., reading an instruction manual about the environment). GT Gen cool the mug\u2192place the mug in/on co\ufb00eema- chine chill the mug\u2192return the mug to co\ufb00eema- chine Task GT Gen Take the pencil from the desk, put it on the other side of the desk take a pencil\u2192place the pencil in/on shelf pick up the white pencil on the desk\u2192put the white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal speci\ufb01cations (Task), ground-truth (GT) v.s. generated (Gen). In the \ufb01rst example, generated plan di\ufb00ers from the ground truth but the meaning agrees. In the second example, the generated plan largely di\ufb00ers from the ground truth due to the mistake in human goal speci\ufb01cation \u2014 \u201canother side on the desk\u201d instead of \u201cshelf\u201d. References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Je\ufb00rey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic a\ufb00ordances, 2022. URL https: //arxiv.org/abs/2204.01691. 5. Conclusion, Limitations, and Future Work Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no \ufb01ne-tuning and is designed to be compatible with any goal-conditional embodied agents. Andreas, J., Klein, D., and Levine, S. Modular multi- task reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. PMLR, 2017. In our experiments, we combine PET with a novel Ac- tion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to \ufb01t the training set tasks, it demonstrates better generalization to unseen human goal speci\ufb01cation tasks. Finally, our ablation studies show the Plan and Track modules together im- prove the performance of Eliminate module to achieve the best performance. Black, S., Gao, L., Wang, P., Leahy, C., and Bider- man, S. GPT-Neo: Large Scale Autoregressive Lan- guage Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high- level natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve e\ufb00ectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit \ufb01nished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic re- planning to address this limitation or explore other ways in which LLMs can assist the learning of the Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Don- ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Hender- son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M.,"}