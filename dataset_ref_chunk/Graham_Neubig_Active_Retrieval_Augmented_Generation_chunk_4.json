{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Active_Retrieval_Augmented_Generation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of retrievers are used for datasets that rely on knowledge from Wikipedia?", "answer": " BM25", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " Which search engine is used as a retriever for datasets that rely on knowledge from the open web?", "answer": " Bing", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " How are multiple retrieved documents formatted?", "answer": " They are linearized according to their ranking and added to the beginning of the user input using Prompt D.1.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What are the drawbacks of using previously generated tokens as queries in question decomposition approaches?", "answer": " 1) It might not reflect what LMs intend to generate in the future. 2) It can be inefficient as retrieval may occur at inappropriate points. 3) It requires task-specific prompt engineering, limiting generalizability.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What is the goal of multihop QA?", "answer": " To answer complex questions through information retrieval and reasoning.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What type of questions does Commonsense reasoning require?", "answer": " Yes/no questions that require world and commonsense knowledge.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What is the objective of long-form QA?", "answer": " To generate comprehensive answers to questions seeking complex information.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What does open-domain summarization aim to achieve?", "answer": " To generate a comprehensive summary about a topic by gathering information from the open web.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What metric is used in ASQA to evaluate the final answer?", "answer": " Exact match (EM)", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}, {"question": " What is provided in the ASQA-hint setting to guide LMs?", "answer": " A brief hint to stay on track when generating answers.", "ref_chunk": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}], "doc_text": "on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens from the previous window are used as the query: qt = yt\u22121 yt = [w(t\u22121)l+1, ..., wtl]. (t \u2265 2), Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al., 2https://api.openai.com/v1/completions April 23. 3https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api 2023), which retrieve every few tokens, and KNN- LM (Khandelwal et al., 2020), which retrieves ev- ery token.4 We follow Ram et al. (2023) to use a window size of l = 16. Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category: qt = yt\u22121 yt = st. (t \u2265 2), Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while pro- ducing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question. The aforementioned approaches can retrieve ad- ditional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., \u201cWhy did the founder of Versus die?\u201d We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., \u201cWould a pear sink in wa- ter?\u201d We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, \u201cWhere do the Philadel- phia Eagles play their home games?\u201d could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is \u201cThis question is ambiguous in terms of which specific location or venue is being referred to.\u201d Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., \u201cGenerate a sum- mary about Echo School (Oregon) including the No ret. 0.020.040.060.080.02WikiMultihopQAStrategyQAASQAASQA-hintWikiAsp Forward-Looking Active REtrieval augmented generation (FLARE) Single-time ret. Previous-window ret. Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp."}