{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_SantaCoder:_don't_reach_for_the_stars!_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the 4 preprocessing methods investigated on the training data?", "answer": " The 4 preprocessing methods investigated are filtering files from repositories with 5+ GitHub stars, filtering files with a high comments-to-code ratio, more aggressive filtering of near-duplicates, and filtering files with a low character-to-token ratio.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What impact did the new filters have on the training data?", "answer": " The new filters had a modest impact except for the stars filter, which significantly deteriorated performance on text2code benchmarks.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What parameter model was trained using the findings from the experiments?", "answer": " A final 1.1B parameter model, dubbed SantaCoder, was trained on Python, JavaScript, and Java.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " How does SantaCoder compare to previous open-source multilingual models on code generation and infilling tasks?", "answer": " SantaCoder obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen-Multi-2.7B, on code generation and infilling tasks.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What is Multi Query Attention (MQA), and in which models is it implemented?", "answer": " Multi Query Attention (MQA) is an architectural change to the transformer neural network where key and value embeddings are shared across attention heads. It is implemented in AlphaCode and PaLM.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " How can the correctness of generated code be tested?", "answer": " The correctness of generated code can be tested using unit tests or textual similarity metrics.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What two popular benchmarks exist for evaluating code completion in Python?", "answer": " Two popular benchmarks for Python are HumanEval and MBPP.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What is CodeXGLUE and what tasks does it involve?", "answer": " CodeXGLUE is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating documentation.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What was the response to developers who did not wish their source code to be used for training code LLMs?", "answer": " Developers who did not wish their source code to be used for training code LLMs were given the opportunity to opt-out of The Stack.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}, {"question": " What effort was made to redact Personally Identifiable Information (PII) from The Stack?", "answer": " Efforts were made to redact PII by constructing a PII benchmark and annotating entities such as names, emails, usernames, passwords, IP addresses, API keys, and SSH keys.", "ref_chunk": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}], "doc_text": "baseline models. We investigate the impact of 4 preprocessing methods on the training data: \ufb01ltering \ufb01les from repositories with 5+ GitHub stars, \ufb01ltering \ufb01les with a high comments-to-code ratio, more aggressive \ufb01ltering of near-duplicates, and \ufb01ltering \ufb01les with a low character-to-token ratio. We observe modest impact of the new \ufb01lters except for the stars \ufb01lter, which deterio- rates performance on text2code benchmarks signi\ufb01cantly. This is an interesting result given that previous work has explicitly \ufb01ltered for GitHub Stars as a proxy for data quality (Gao et al., 2020; Xu et al., 2022b). Using the \ufb01ndings from these experiments, we train a \ufb01nal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and in\ufb01lling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller. 2 RELATED WORK Code LLMs Recently, there has been an increasing amount of research on using large-scale trans- former models to analyze or generate source code. Many studies have focused on using decoder-only models with a causal language modeling objective (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Christopoulou et al., 2022; Izadi et al., 2022; Xu et al., 2022b; Athiwaratkun et al., 2022), while other studies have investigated encoder (Feng et al., 2020a; Kanade et al., 2020) and encoder-decoder architectures (Li et al., 2022; Ahmad et al., 2021; Wang et al., 2021; Roziere et al., 2021). Bavarian et al. (2022); Fried et al. (2022) propose to use decoder-only models for code- in\ufb01lling tasks using a causal masking mechanism, and Bavarian et al. (2022) argues that training with such a \ufb01ll-in-the middle (FIM) objective does not harm the model\u2019s ability to do left-to-right generation. Shazeer (2019) proposes Multi Query Attention (MQA), an architectural change to the transformer neural network in which key and value embeddings are shared across attention heads, resulting in lower memory requirements and faster inference for large batch settings. Multi Query Attention was implemented in AlphaCode (Li et al., 2022) and PaLM (Chowdhery et al., 2022). Evaluating text-to-code The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence. Textual similarity metrics have also been used to evaluate code (Feng et al., 2020b; Ren et al., 2020); however, they have been shown to correlate only weakly with code correctness (Austin et al., 2021; Chen et al., 2021). Many single-language benchmarks for evaluating code completion exist (Kulal et al., 2019; Iyer et al., 2018; Zhong et al., 2017; Yu et al., 2018; Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Athiwaratkun et al., 2022; Lai et al., 2022). Two of the most popular benchmarks for Python are HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which consist of a natural language description of a function and a set of unit tests. MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages. Python-speci\ufb01c terminology in the prompt is automatically replaced with the equivalent terminology used for each programming language. MBXP (Athiwaratkun et al., 2022) is a concurrent benchmark that uses a similar approach, but differs in the details of type inference, prompt construction, and evaluation. In particular, MBXP uses the same set of assertions in the prompt that it uses to test the correctness of generated solutions. In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness. 3 Preprint Evaluating other tasks Code generation models have also been used to solve a variety of tasks (Tufano et al., 2020; Feng et al., 2020b; Ahmed & Devanbu, 2022; Hellendoorn et al., 2018; Pradel et al., 2020). CodeXGLUE (Lu et al., 2021) is a set of 14 datasets for evaluating code generation models. The tasks include code-to-code tasks like clone detection, code repair, and code translation; text-to-code tasks like code search and code generation; and code-to-text tasks like generating doc- umentation. The programming languages included vary by task; the most common are Python and Java. 3 OPT-OUT PROCESS Developers who do not wish their source code to be used for training code LLMs are given the op- portunity to opt-out of The Stack (Kocetkov et al., 2022). We received 9 opt-out requests before the cut-off date for removing data (31 October 2022). These individuals accounted for 299 repositories. Of these, 161 were already excluded from The Stack v1.0 (because they did not have a permissive license), and 138 were in The Stack v1.0. We honored the requests to opt-out and removed these repositories from The Stack v1.1. After the cut-off date (31 October 2022), we have received more requests for requests and we will remove these repositories prior to releasing The Stack v1.2. 4 REDACTING PERSONALLY IDENTIFIABLE INFORMATION We describe our \ufb01rst efforts to redact PII from The Stack. 4.1 PII BENCHMARK We construct a PII benchmark by annotating the following entities on a small subset of The Stack: names, emails, usernames, passwords, IP addresses, API keys, and SSH keys. We pre-\ufb01ltered 400 samples from a total of 4000 code \ufb01les that were likely to contain Personally Identi\ufb01able Information (PII). We \ufb01rst select 4000 code \ufb01les from 11 programming languages, with a total of 800 samples for Python and C++, 400 samples for Java, JavaScript, TypeScript, and PHP, and 160 samples for C, C#, Markdown, Go, and Ruby. To detect keys in these samples, we used the detect-secrets tool5 with all default plugins activated. In addition, we used regular expressions to detect emails, IPv4 and IPv6 addresses, see Appendix C.1. Twelve members of the BigCode community annotated the \ufb01les on the LightTag platform6, with one annotator assigned per \ufb01le. After the annotation phase, one member reviewed all the annotation tags. To further increase annotation quality, we ran our initial PII detection"}