{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of TTS-based data augmentation in the context of E2E ASR architectures?,answer: To benefit significantly from the TTS data and improve performance.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " What is the difference between classical ASR systems and E2E ASR systems in terms of knowledge integration?,answer: Classical ASR systems use secondary knowledge sources, while E2E ASR systems integrate all knowledge sources into a single global joint model.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " Explain the training process of classical ASR systems.,answer: Classical ASR models are usually trained successively, with knowledge derived from earlier models injected into later training stages.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " How are deep learning models in classical ASR systems sometimes initialized?,answer: They may be initialized using HMM alignments obtained from acoustic models based on mixtures of Gaussians, serving as an initial shallow model.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " What is one approach proposed to improve training of attention-based encoder-decoder models suffering from low training resources?,answer: Using regularization techniques and data augmentation techniques like TTS and SpecAugment.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " How do multilingual approaches contribute to improving ASR development?,answer: They help improve ASR development for low resource tasks in both classical and E2E systems.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " What is the purpose of the beam search algorithm in end-to-end speech recognition decoding?,answer: It is used to estimate the most likely sequence among a subset of possible hypotheses efficiently.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " When is the Greedy search algorithm mainly used in ASR?,answer: It is mainly used in CTC-based methods and ignores the dependency of the output labels.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " What is the role of alignment tokens in the Greedy search algorithm?,answer: Alignment tokens are used to convert the alignment token sequence to the corresponding token sequence.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}, {"question": " How does the performance of the Greedy search algorithm compare to attention and RNN-T based methods?,answer: The Greedy search algorithm has relatively poor performance due to the lack of output dependency compared to attention and RNN-T based methods.", "ref_chunk": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}], "doc_text": "data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data. In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model. Relationship to Classical ASR E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available. While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69]. In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136]. The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220]. It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 12 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 13 VI. DECODING E2E MODELS B. Beam Search This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence \u02c6C among all possible sequences, as follows: The beam search algorithm is introduced to approximately consider a subset of possible hypotheses \u02dcC among all possible hypotheses U \u2217 during decoding, i.e., \u02dcC \u2282 U \u2217 . A predicted output sequence \u02c6C is selected among a hypothesis subset \u02dcC instead of all possible hypotheses U \u2217, i.e., \u02c6C = arg max C\u2208U \u2217 P (C|X) The following section describes how to obtain the recognition result \u02c6C. A. Greedy Search \u02c6C = arg max C\u2208 \u02dcC P (C|X) The beam search algorithm is to find a set of possible hy- potheses \u02dcC, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U \u2217. (6) The Greedy search algorithm is mainly used in CTC, which ignores the dependency of the output labels as follows: \u02c6A = T (cid:89) t=1 (cid:18) arg max at P (at|X) (cid:19) where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence \u02c6A to the corresponding token se- quence \u02c6C. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general. CTC\u2019s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across"}