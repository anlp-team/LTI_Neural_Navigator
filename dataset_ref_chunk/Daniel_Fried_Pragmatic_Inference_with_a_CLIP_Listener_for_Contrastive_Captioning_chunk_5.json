{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Pragmatic_Inference_with_a_CLIP_Listener_for_Contrastive_Captioning_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the automatic evaluation results mentioned in the text?", "answer": " The focus is on evaluating informativity and fluency using different metrics.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " Which approach substantially outperforms Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL) in informativeness?", "answer": " PICL substantially outperforms the mentioned approaches in informativeness.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " How do methods controlling for fluency (PPL) compare in terms of perplexity?", "answer": " Methods controlling for fluency (PPL) pivot around PICL and achieve a similar level of perplexity.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " What does the human evaluation results depict regarding informativeness?", "answer": " Human evaluation results show that PICL substantially outperforms Incre-RSA and E-S in informativeness.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " What is the primary finding related to the informativity-fluency trade-off in the human assessments?", "answer": " The primary finding is that PICL outperforms Incre-RSA along both dimensions of informativeness and fluency.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " In the automatic vs. human evaluation analysis, how is ALBEF accuracy related to human retrieval accuracy?", "answer": " ALBEF accuracy has a strong positive correlation with human retrieval accuracy for model-generated outputs.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " What is the trend seen in human fluency evaluations for captions generated by E-S and Incre-RSA?", "answer": " The trend shows that captions generated by E-S and Incre-RSA without controlling for perplexity are rated as much more disfluent by humans.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " What is the significance of human evaluations according to the text?", "answer": " Human evaluations are important to accurately compare the performance of discriminative captioning systems.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " How do E-S and Incre-RSA differ in the trade-off patterns between informativeness and fluency according to the text?", "answer": " The trade-off patterns for E-S and Incre-RSA are different from that under ALBEF, as optimizing for ALBEF accuracy induces more disfluent generation.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}, {"question": " What implication is suggested regarding the optimal informativeness under human judgment?", "answer": " The optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency.", "ref_chunk": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}], "doc_text": "2703.0 4093.6 380.2 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 62.9 73.2 446.5 366.6 Table 2: Automatic evaluation results on the ImageCode test set: We evaluate informativity using the retrieval ac- curacy of the ALBEF evaluative listener using captions generated by each approach. PICL substantially out- performs Base Speaker, Incre-RSA, Incre-RSA (PPL), and E-S (PPL), achieving a competitive level of infor- mativeness to E-S. In fluency, evaluated using GPT-2 perplexity, methods that control for the fluency (PPL) pivoting around PICL achieve similar level of perplexity, while E-S and Incre-RSA that optimized for informativ- ity are substantially less fluent. listener model in incremental pragmatic caption generation. For both E-S and Incre-RSA, control- ling for fluency negatively affects ALBEF accuracy, which conforms with the trend in Figure 2. Fluency Table 2 also shows the perplexity that GPT-2 assigns to the output of each model on the ImageCoDe test set. As discussed in Section 4.5, Incre-RSA and E-S are less robust when being opti- mized for informativity, which is reflected by their extremely high perplexity. In contrast, when con- trolling for the fluency to match PICL\u2019s validation perplexity, both Incre-RSA and E-S generate sub- stantially more fluent captions with test perplexity similar to PICL, at the cost of predicted informa- tiveness, as shown by a drop in ALBEF accuracy. 5.2 Human Assessment Performance We perform human evaluations (Section 4.3) to val- idate these findings about the informativeness and fluency of the discriminative captioning methods. Informativeness Human retrieval accuracies on model- and human-generated captions are depicted in Table 3. In the setting where models are au- tomatically optimized for predicted informativity (Section 4.5), PICL substantially outperforms the Incre-RSA and E-S methods, with gains in human Method Human Accuracy Fluency Rating Human Base Speaker 81.7 48.7 4.76 4.80 Optimized for Informativity Incre-RSA E-S PICL 50.7 54.0 65.7 2.87 3.59 4.07 Perplexity-Matched to PICL Incre-RSA (PPL) E-S (PPL) 53.3 63.7 4.23 4.54 Table 3: Human evaluation results on 100 sets of images from ImageCoDe test split: Informativity is assessed by the retrieval accuracy of human annotators using cap- tions generated by each approach. PICL outperforms all other models on human informativeness judgments. For fluency, human annotators evaluate using ratings on a 1-5 scale. Similar to results in the automatic evaluations of fluency (Table 2), annotators assign much lower flu- ency scores to E-S and Incre-RSA, which do not control for fluency. accuracy of 11% and 15% respectively. The results indicate that captions generated by PICL are more informative than by other approaches, judged by human annotators. When we control the disflu- ency of the other methods to be similar to PICL (as measured by GPT-2 perplexity in automatic evalua- tions), PICL still substantially outperforms Incre- RSA (PPL) and slightly outperforms ES (PPL). Moreover, for both E-S and RSA, controlling for PPL results in more informative captions, which is not reflected in the automatic evaluations using ALBEF (Table 2), implying that disfluency has a more significant negative effect on informativity for humans. While past work has often relied only on automated evaluations, our results indicate that human evaluations are important to accurately com- pare the performance of discriminative captioning systems. Fluency Table 3 also shows the average fluency scored by human workers for model- and human- generated captions. Similarly to Table 2 captions generated by E-S and Incre-RSA without control- ling for perplexity are much more disfluent as scored by humans. Informativity-Fluency Trade-off We further combine the human accuracy and fluency in Ta- ble 3 for each model and plot them in Figure 3. 55 5.0Fluency Score by Human Annotators 65 50 4.5 60 humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 80 Incre-RSA human Base 45 E-S 3.5 85Human Annotators Retrieval Accuracy (%) 3.0 PICL 75 4.0 70 Figure 3: Human eval results on 100 test split static image sets. To depict the informativity-fluency trade-off under human assessments, we also include a setting of in- formativity hyperparameters for each method with an intermediate level of automatically predicted fluency. Specifically, for each model, we search for its informativity parameter so that the average GPT-2 perplexity of generated captions are as close as possible to the average perplexity of the base speaker + PICL. We refer to the models obtained under this scheme as ES (mid PPL), Incre-RSA (mid PPL) and PICL (mid PPL). With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency. 5.3 Automatic vs. Human Evaluation The analysis above reflects both agreement and mis- match between automatic evaluation and human judgments on different aspects. To further reveal the correlation between them, and lay a founda- tion for future work on discriminative captioning to make automatic evaluations more predictive of human performance, we conduct analysis along both axes of informativity and fluency. ALBEF vs. Human Retrieval Accuracy Fig- ure 4 plots ALBEF against human retrieval accu- racy on the same 100 sets of images. ALBEF accu- racy has a strong positive correlation with human 65 55 50 65 85ALBEF Retrieval Accuracy (%) E-S human Base humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 60 Incre-RSA 80 70 75 80Human Annotators Retrieval Accuracy (%) 75 70 PICL Figure 4: ALBEF accuracy and human accuracy are pos- itively correlated for model-generated outputs, with the exception of disfluent captions produced by the variants of E-S and Incre-RSA that do not control for perplexity. 5.0Fluency Score by Human Annotators humanBaseIncre-RSAIncre-RSA (PPL)Incre-RSA (mid PPL)E-SE-S (PPL)E-S (mid PPL)PICLPICL (mid PPL) 102 Incre-RSA 103 E-S human 4.5 Mean GPT-2 Perplexity (log scale) 4.0 PICL 3.5 3.0 Base Figure 5: Mean perplexity, under GPT-2, is predictive of human fluency evaluations across systems. judgments except for having human, E-S, and Incre- RSA as"}