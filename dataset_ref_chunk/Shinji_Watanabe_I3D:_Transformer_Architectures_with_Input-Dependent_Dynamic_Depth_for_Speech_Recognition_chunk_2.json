{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_I3D:_Transformer_Architectures_with_Input-Dependent_Dynamic_Depth_for_Speech_Recognition_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the total training loss formula mentioned in the text?", "answer": " Ltotal = LASR + \u03bb \u00b7 Lutility", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " What does a gate value of 0 signify in the text?", "answer": " Skipping the module", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " How is the approximation affected as \u03c4 approaches 0?", "answer": " The approximation becomes closer to the discrete version", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " What trick is applied to handle the issue of binary gates not being differentiable?", "answer": " The Gumbel-Softmax trick", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " What type of gate predictor is associated with a specific I3D encoder layer?", "answer": " The local gate predictor (LGP)", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " What is the purpose of the global gate predictor (GGP) mentioned in the text?", "answer": " It is defined for an entire I3D encoder", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " What is the threshold used during inference to produce a binary gate for every module?", "answer": " \u03b2 \u2208 [0, 1]", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " How are MHA and FFN gate probabilities used to make decisions in the text?", "answer": " Decisions are made sequentially from lower to upper layers", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " What is the purpose of using a fixed threshold \u03b2 in the text?", "answer": " To produce a binary gate for every module during inference", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}, {"question": " How are the gate probabilities used to minimize overall computation instead of the number of layers?", "answer": " By using a weighted average of the two types of gates, where the weights depend on their computational costs", "ref_chunk": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}], "doc_text": "the corresponding module will be skipped, which effectively reduces computation. The total training loss is: MHA, g(l) Ltotal = LASR + \u03bb \u00b7 Lutility, (3) (4) (5) process, but it is differentiable w.r.t. \u03b1 and thus suitable for gradient- based optimization. As \u03c4 \u2192 0, the approximation becomes closer to the discrete version. We use \u03c4 = 1 in our experiments. MHA \u2208 R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN \u2208 R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as de\ufb01ned in Sec. 2.3. For the l-th MHA, a discrete probability distribution p(l) Lutility = 1 2N N (cid:88) l=1 (cid:16) MHA + g(l) g(l) FFN (cid:17) , (6) 2.3. Local and global gate predictors where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. \u03bb > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is de\ufb01ned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances. A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) \u221d \u03b1k for any k = 1, . . . , K. To draw a sample from this distribution, we can \ufb01rst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability: We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead. The local gate predictor (LocalGP or LGP) is associated with a speci\ufb01c I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l\u22121) \u2208 RT \u00d7d. This sequence is \ufb01rst converted to a d-dimensional vector x(l\u22121) \u2208 Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively: MHA, p(l) p(l) FFN = LGP(l)(x(l\u22121)), z = arg max k\u2208{1,...,K} log \u03b1k + gk. The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows: (7) where p(l) FFN \u2208 R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a \ufb01xed threshold \u03b2 \u2208 [0, 1] is utilized to produce a binary gate for every module: MHA, p(l) z = softmax ((log \u03b1 + g)/\u03c4 ) , (8) where \u03b1 = (\u03b11, . . . , \u03b1K ), g = (g1, . . . , gK ), and \u03c4 is a tempera- ture constant. Eq. (8) is an approximation of the original sampling MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l) MHA)1 > \u03b2 else 0, FFN)1 > \u03b2 else 0, 1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers. where (p(l) FFN)1 is the probability of executing the FFN. We use \u03b2 = 0.5 by default, but it is also possible to adjust the inference cost by changing \u03b2. MHA)1 is the probability of executing the MHA and (p(l) (9) (10) (11) Transformer I3D-GlobalGP LayerDrop I3D-GlobalGP (\u03b2 varies) I3D-LocalGP 13.5 ) \u2193 ( 13.0 R E W % 12.5 12.0 11.5 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean 30.0 ) \u2193 ( 29.0 R E W % 28.0 27.0 26.0 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. \u03b2 is the threshold for generating binary gates as de\ufb01ned in Eqs. (10) (11). ) \u2193 ( R E W % 12.4 12.2 12.0 11.8 11.6 11.4 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (a) LibriSpeech test clean ) \u2193 ( R E W % 27.5 27.0 26.5 26.0 Transformer LayerDrop I3D-GlobalGP 18 20 22 24 26 28 30 32 34 36 Average number of layers (b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets. The global gate predictor (GlobalGP or GGP), on the other hand, is de\ufb01ned for an entire I3D encoder,"}