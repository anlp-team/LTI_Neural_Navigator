{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_The_Framework_Tax:_Disparities_Between_Inference_Efficiency_in_Research_and_Deployment_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the well-optimized operations that make up ResNet-50 and BERT?", "answer": " Conv2Ds and GEMMs", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " How much speed up do TorchScript and ONNX provide over PyTorch for batch size 1 using average FP16?", "answer": " 34.16% and 71.38% respectively", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " What leads to latency bottlenecks regardless of the precision during inference?", "answer": " Framework overhead", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " What is the purpose of CUDA Graph kernel serialization and PyTorch BetterTransformer framework optimizations?", "answer": " To provide speedups through additional kernel fusion and sparse tensor operations.", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " How do nested tensor operations leverage sparsity in padded variable sequence lengths to reduce latency?", "answer": " They can provide substantial latency reductions when inference is compute-bound.", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " What are the common model architecture design decisions examined in section 4.2?", "answer": " Scaling Model Depth & Width, Downsampling and Hierarchical Pooling, Efficient Mobile Architectures", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " What is the relationship between increases in model depth and latency?", "answer": " Latency increases as each added layer operation requires additional CPU-GPU dispatch.", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " Why do wider model variations see no increase in latency at low batch sizes?", "answer": " Total runtime is constant despite wider operations requiring more floating point operations.", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " What is the performance difference between Funnel Transformer and BERT for inference?", "answer": " Funnel Transformer is slower than BERT due to additional intermediate pooling layers.", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}, {"question": " How do some architectural innovations increase inference latency?", "answer": " By increasing the size of tensor operator graphs.", "ref_chunk": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}], "doc_text": "the rate at which compute kernels overtake CPU dispatch operations. For the well-optimized operations (Conv2Ds and GEMMs) that make up ResNet-50 and BERT, the time per FLOP is reasonably consistent. 4.1 Framework Design Decisions In Figure 4, we observe that frameworks from all execution paradigms exhibit framework bound be- haviors. However, deferred execution TorchScript and static ONNX Runtime, which support com- putational graph compilation (e.g. operator fu- sion), exhibit less framework overhead and provide speedups over eager PyTorch. These increases are especially pronounced at low batch sizes where inference is framework-bound. For batch size 1, TorchScript and ONNX provide an average FP16 speed up of 34.16% and 71.38% over PyTorch, respectively. As batch sizes increase and models become compute bound, there is minimal differ- ence in latency across frameworks as the majority of execution time is spent on kernel execution. Computation with mixed and half precision (FP16) often increases training throughput over single precision (FP32), we observe that frame- work overhead results in latency bottlenecks re- gardless of the precision during inference. As half precision computation is faster due to reduced data movement, GPU kernel execution time takes longer Additionally, we consider both static, serial- ized CUDA graphs and PyTorch BetterTransformer framework optimizations in Figure 5. BetterTrans- former provides speedups through additional kernel fusion and sparse tensor operations that take advan- tage of sparse sequence lengths and padding tokens 101 101 CUDA Graphs Nested Tensors PyTorch 102 102 Latency (s) Batch Size 100 Figure 5: Different framework optimizations lead to la- tency improvements in different regimes for BERT-Base. CUDA Graph kernel serialization reduces launch over- head in the framework bound regime, whereas sparse computation reduces latency at larger batch sizes. to remove redundant computation. To construct sparse inputs, we simulate sam- ples by generating variable length sequences and padding to the maximum sequence length of 128. Sentences are randomly generated according to the sequence length distribution of the Penn Treebank (Taylor et al., 2003), with an average length of 20.92 and a standard deviation of 10.18 tokens. Analysis Utilization of CUDA Graphs substan- tially reduce latency at low batch sizes when infer- ence is bounded by framework overhead from ker- nel launches. However, at larger batch sizes, nested tensor operations can leverage sparsity in padded variable sequence lengths to provide substantial latency reductions when inference is compute- bounded. 4.2 Model Design Decisions We examine a variety of common model architec- ture design decisions and investigate their align- ment with commonly reported efficiency proxies and empirically observed latency. 4.2.1 Scaling Model Depth & Width Assumption Scaling the dimensionality and number of hidden layers is commonly used in NLP and computer vision as a means to explore tradeoffs between model performance and com- putational requirements (He et al., 2016; Touvron et al., 2021; Zhou et al., 2021). Recent work has shown that model end-task performance scales dif- ferently along each axis (Tay et al., 2021, 2022; Nguyen et al., 2021). Figure 6: Comparison of latency for BERT variants that scale model width and depth. Increases in model depth add more framework overhead, whereas increases in model width lead to faster transitions to compute boundedness. We compare our baseline models to variants that scale both model width and depth. We examine 12-layer BERT-Base against its 6-layer DistilBERT (Sanh et al., 2019) variant and experiment across parameterized BERT models, varying the number of encoder layers as well as width of their fully connected and self-attention layers. Analysis When scaling model depth, we observe that latency increases in both the framework- and compute-bound regimes as each added layer op- eration requires an additional CPU-GPU dispatch. Deeper model variants have a larger fixed latency in the framework-bound regime as seen in Figure 6. Counter-intuitively, wider model variations see no increase in latency at low batch sizes. As model execution is framework bound, total runtime is con- stant despite wider operations requiring more float- ing point operations. Instead, increased per-layer kernel execution time causes these wider models to become compute-bound at lower batch sizes. In the compute-bound regime, latency scales more rapidly with batch size for wide models. 4.2.2 Downsampling and Hierarchical Pooling Figure 7: Comparison of BERT and Funnel Transformer latency. Despite using fewer total MAC operations, Funnel is slower than BERT for inference due to the introduction of additional intermediate pooling layers. Assumption Self-attention layers in Transformer architectures are notoriously computationally ex- pensive as their complexity scales quadratically with input sequence length. To reduce this com- putational bottleneck, researchers have developed efficient transformer architectures that seek to re- duce sequence lengths via methods such as down- sampling, low rank approximations, and locality sensitive hashing (Dai et al., 2020; Kitaev et al., 2020; Wang et al., 2020b; Xiong et al., 2021). We examine the performance of the Funnel Transformer which applies average pooling to per- form sequence length reduction every 4 layers. The model achieves comparable accuracy to BERT on downstream tasks while requiring 42% fewer total MAC operations through sequence length reduc- tion. This model achieves similar downstream task performance to BERT-Base and trains 33% faster based on wall-clock time. Analysis While Funnel Transformer reduces to- tal FLOPs and obtains substantial speedups in large- scale training, this speedup does not translate to increased speed of inference as seen in Figure 7. In practice, the average pooling layers used to perform sequence length reductions add additional opera- tions to the computation graph and increase the model\u2019s framework overhead. At low batch sizes, Funnel Transformer is framework bound at a much higher latency than BERT, and remains slower even at larger batch sizes. While some architectural inno- vations decrease the total number of model FLOPs, some approaches increase the size of tensor opera- tor graphs (e.g. vis-a-vis additional layers) which can ultimately increase inference latency. 4.2.3 Efficient Mobile Architectures Figure 8: Latency of Transformer models using efficient variations of convolution and self-attention operations. All of the variants observe lower latency at large batch sizes, but have worse FLOP utilization. Efficient Trans- former variants are slower than BERT"}