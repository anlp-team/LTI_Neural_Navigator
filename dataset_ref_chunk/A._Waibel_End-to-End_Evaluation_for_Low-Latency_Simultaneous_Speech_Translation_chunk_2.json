{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_End-to-End_Evaluation_for_Low-Latency_Simultaneous_Speech_Translation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the three main components of the speech processing described in the text?", "answer": " The three main components are input processing, stability detection and output processing.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " What is the purpose of the voice activity detection in the speech processing component?", "answer": " The voice activity detection generates speech segments based on audio packets which are then processed for speech recognition.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " What is the difference between the revision mode and the fixed mode in low-latency speech translation?", "answer": " In revision mode, both stable and unstable outputs are allowed to be sent, while in fixed mode only stable output is allowed.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " How is stability detection implemented in the speech processing component?", "answer": " Stability detection is implemented using the local agreement two (LA2) method which checks for common prefixes in the output.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " What is the purpose of the backend model in the text processing component?", "answer": " The backend model in the text processing component is used for machine translation.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " How are unstable text messages handled in the revision mode of the text processing component?", "answer": " Unstable text messages are processed by the backend model and the output is sent as unstable text.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " What is the evaluation framework used for testing the system described in the text?", "answer": " The evaluation framework involves sending input audio to the system and evaluating the final returned transcript and translation in an end-to-end fashion.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " How can the framework described in the text be extended?", "answer": " The framework can be extended by deploying additional backend models for different languages, adding new streaming algorithms, or including custom components in the session graph.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " What is the significance of prefix decoding in the backend models for streaming algorithms?", "answer": " Prefix decoding allows sending a prefix which is then forced in the output, aiding in stability detection and output processing.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}, {"question": " What is the purpose of the moving average calculation in the voice activity detection process?", "answer": " The moving average calculation is used to determine if a speech segment should start or end based on audio input exceeding certain thresholds.", "ref_chunk": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}], "doc_text": "device and is shared between different sessions. Because of the division in a stateful middleware and a stateless backend, we are able to share the backend and use batching of the requests. 3) Output processing: The output of a backend request is used to send information to the next com- ponent(s). Furthermore, the state of the correspond- ing session is updated. Our framework supports two modes for low- latency speech translation. First, a revision mode (Niehues et al., 2018) where the component (Auto- matic speech recognition (ASR) or machine trans- lation (MT)) can send stable and unstable outputs. Given more context at a later time step, the com- ponent can revise the unstable outputs. Second, a fixed mode (Liu et al., 2020a; Pol\u00e1k et al., 2022) where the component is only allowed to send stable output. For fixed mode (and the revision mode of the ASR component), the component needs to per- form a stability detection (see Sections 3 and 4 and Figure 2), i.e., determine which parts of the out- put should be considered stable. Note that for our streaming algorithms the backend models need to support prefix decoding, i.e., one can send a prefix which is then forced in the output. Our framework is easily extendable by deploying additional backend models for different languages, adding new streaming algorithms in the middle- ware or adding custom components (e.g., speaker diarization as a preprocessing step before the ASR) and including them in the session graph. 3 Low-latency Speech Processing The speech processing component receives a stream of audio packets and sends chunks of text (transcript or translation) to the mediator. For this two steps are run: Input processing: First, a voice activity detec- tion generates a speech segment that can be ex- tended when new packets of audio arrive. For this we use WebRTC Voice Activity Detector (Wise- man, 2016). Each audio frame (30ms) is classified if it contains speech or not. Then a moving average is calculated. If it exceeds a certain threshold, a new segment is started. New audio is added to this segment until the moving average falls below a cer- tain threshold and the segment ends. Second, the backend model (ASR or speech translation (ST)) is run. If there exist speech segments that already ended, they are processed only once and the output is sent as stable text, other segments are constantly processed until they end. Stability detection and output processing: We use the method local agreement two (LA2) from Pol\u00e1k et al. (2022). The intuition is that if the prefix of the output stays the same when adding more audio, the prefix should be considered sta- ble. Let C denote the chunk size hyperparameter (LA2_chunk_size). The fixed mode works as fol- lows (see Figure 2): It waits until the segment con- tains (at least) C seconds of audio (denoted by M1) and then runs the model but does not output any stable text. Let\u2019s denote this first model output by H1. After the segment contains (at least) C more seconds of audio (denoted by M2) the model is run again with all the audio and outputs H2. Then the component outputs the common prefix of H1 and H2 as stable output S2. After the segment again contains (at least) C more seconds of audio (de- noted by M3) the model is run again with all the audio. However, now S2 is forced as prefix in the ASR/ST model decoding. The model outputs H3 and the common prefix from H2 and H3 is the next stable output S3. This procedure is continued until the speech segment ends. Note that the ASR/ST model has a certain maxi- mum input size due to latency, memory and com- pute constraints. Therefore, if this limit is reached, the input audio to the model as well as the corre- sponding forced prefix is cut away. The revision mode differs from the fixed mode in that the last hypothesis except the common prefix is sent as unstable output. Furthermore, in the time period until the speech segment contains again C more seconds of audio, the currently given audio is run through the model and the hypothesis except the last stable output is sent as unstable output. 4 Low-latency Text Processing The text processing component receives a stream of (potentially revisable) text messages and sends chunks of text (translation) to the mediator. Input processing: First, all input text that ar- rived is split into sentences by punctuation. Then, the backend model (MT) is run. Stability detection and output processing: All sentences containing only stable text are processed once and the output is sent as stable text. For the other sentences containing unstable text the behav- ior depends on the mode. If text is stable or not is given by the speech processing component. The revision mode works as follows: All sen- tences containing unstable text are processed by the backend model and the output text is sent as unstable text. A similar approach is not possible in the speech processing revision mode (see Section 3) since speech segments are not limited in size but the model input size is. For the fixed mode we use the method local agreement from Liu et al. (2020a). The processing is similar to the speech processing. The difference is that the backend model is run when at least one new word is given instead of at least C seconds of audio. In our preliminary experiments, up to at least five words but the results were basically identical since the input is extended by a few words most of the time. Furthermore, only the stable part of the sentences containing unstable text is used as input. This restriction is not necessary in the speech processing component since there is no unstable audio input. 5 Evaluation Framework We evaluate our system in an end-to-end fashion. That is, given an input audio, we send it to the system and evaluate the final returned transcript and translation. We provide an evaluation"}