{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_14.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of ORCA in the given text?,answer: To achieve zero-shot super-resolution and study data alignment and knowledge transfer from pretrained models.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " How do the training times of ORCA compare to other methods in Table 15?,answer: ORCA takes longer time for iteration but converges faster, making it more efficient overall.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " What are the resolutions used in the experiments for ORCA?,answer: 10241 and 1282 for different tasks.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " What is the metric nRMSE used to evaluate in the super-resolution experiment?,answer: Normalized root mean square error.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " How does ORCA perform compared to vanilla fine-tuning and training-from-scratch baselines?,answer: ORCA outperforms both baselines in most cases, emphasizing the importance of distribution alignment.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " What is the loss function used for training in the experiments on OpenML Tabular Datasets?,answer: Cross-entropy loss.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " What split ratio is used for the train-test split in the evaluation protocol of ORCA in Table 18?,answer: 0.5:0.5.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " Which classi\ufb01cation method achieves the best overall performance on the OpenML-CC18 datasets?,answer: ORCA.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " In what way does ORCA differ from the TabPFN method in terms of training approach?,answer: ORCA needs to be trained on a per-task basis, while TabPFN first fits a general prior network offline.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}, {"question": " What concern is raised regarding the use of pretrained language models for tabular tasks?,answer: The concern that these models might have seen the tabular data during pretraining, potentially affecting test metrics.", "ref_chunk": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}], "doc_text": "Parameters PINN FNO U-Net ORCA 1D Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes 1024 1024 1024 1024 1024 \u03b2 = 0.4 \u03bd = 1.0 \u03bd = 0.5, \u03c1 = 1.0 - \u03b7 = \u03b6 = 0.1, rand periodic 6.7E-1 3.6E-1 6.0E-3 1.5E-1 7.2E-1 1.1E-2 3.1E-3 1.4E-3 1.7E-3 6.8E-2 1.1 9.9E-1 8.0E-2 2.2E-1 - 9.8E-3 1.2E-2 3.0E-3 1.6E-3 6.2E-2 2D Darcy Flow Shallow-Water Diffusion-Reaction 128\u00d7128 128\u00d7128 128\u00d7128 \u03b2 = 0.1 - - 1.8E-1 8.3E-2 8.4E-1 2.2E-1 4.4E-3 1.2E-1 1.7E-2 1.6 8.1E-2 6.0E-3 8.2E-1 Table 15: Per epoch and total training time for each method evaluated in Table 14. Baseline numbers are taken from (Takamoto et al., 2022). On 1D tasks, though it takes longer time for ORCA-RoBERTa to iterate over the entire dataset, our method converges faster, so overall ORCA is still more ef\ufb01cient than FNO and U-Net. FNO U-Net PINN ORCA Task Diffusion-Sorption Shallow-Water Resolution 10241 1282 Per epoch (s) Epoch Total (hrs) 97.52 105.16 500 500 13.5 14.6 Per epoch (s) Epoch Total (hrs) 96.75 83.32 500 500 13.4 11.6 Per epoch (s) Epoch Total (hrs) 0.011 0.041 15000 15000 0.046 0.17 Per epoch (s) Epoch Total (hrs) 149.57 35.5 200 200 8.43 2.2 A.5.2. RESULTS FOR ZERO-SHOT SUPER-RESOLUTION (FIGURE 5, RIGHT) In addition to the above experiments, we also study whether under certain conditions, ORCA can achieve zero-shot super- resolution as described in Li et al. (2021). We see that when using convolution with kernel size 1 and the RoBERTa Cross-Modal Fine-Tuning: Align then Re\ufb01ne backbone, ORCA can indeed generalize to higher-resolution inputs. The detailed results are as follows. Table 16: We study zero-shot super-resolution (trained on lower resolution and tested on higher resolution) on the 1D Advection problem. ORCA-RoBERTa achieves this since the nRMSEs are similar across rows for different train-test resolution pairs. Note that the metrics differ slightly from the one reported in Table 14 because the kernel size of the convolution layer in the embedder is searched via ASHA for experiments in Table 14, whereas pointwise convolution with kernel size 1 is used to achieve super-resolution for experiments in this table. Train Resolution (Spatial) Test Resolution (Spatial) nRMSE 1D Advection 1D Advection 1D Advection 256 256 512 256 512 512 1.13E-2\u00b12.71E-4 1.27E-2\u00b19.54E-5 1.02E-2\u00b12.37E-4 A.5.3. RESULTS FOR FINE-TUNING AND TRAIN-FROM-SCRATCH BASELINES Similar to the NAS-Bench-360 experiments, we also want to study how much data alignment and knowledge transfer from pretrained models bene\ufb01t downstream adaptation for PDE tasks. Therefore, we compare ORCA with the vanilla \ufb01ne-tuning baseline (without data alignment) and the train-from-scratch baseline. As shown in the table below, these two baselines underperform ORCA, which shows the importance of distribution alignment. Besides, \ufb01ne-tuning outperforms train-from- scratch on 5/8 tasks. This shows that whether transferring pretrained knowledge can bene\ufb01t downstream adaptation is task-dependent. In some cases, naive \ufb01ne-tuning without data alignment can even harm transfer. Table 17: Normalized RMSEs (\u2193) with error bars of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch on PDEBench datasets. Advection Burgers Diffusion-Reaction Diffusion-Sorption Navier-Stokes Darcy Flow Shallow-Water Diffusion-Reaction Train-from-scratch Fine-tuning ORCA 1.7E-2\u00b17.0E-4 1.4E-2\u00b11.7E-3 9.8E-3\u00b11.4E-4 1.3E-2\u00b14.6E-4 1.4E-2\u00b13.6E-4 1.2E-2\u00b13.6E-4 1.7E-2\u00b12.2E-4 9.3E-3\u00b15.7E-3 3.0E-3\u00b11.5E-4 3.2E-3\u00b11.0E-6 3.1E-3\u00b16.5E-5 1.6E-3\u00b11.7E-4 9.9E-1\u00b13.6E-6 9.9E-1\u00b12.0E-5 6.2E-2\u00b11.9E-3 9.0E-2\u00b13.6E-3 8.1E-2\u00b12.5E-3 8.1E-2\u00b18.1E-4 6.0E-3\u00b13.5E-6 6.1E-3\u00b17.3E-6 6.0E-3\u00b14.5E-6 8.4E-1\u00b11.8E-3 8.3E-1\u00b19.3E-5 8.2E-1\u00b14.6E-5 A.6. Experiments on OpenML Tabular Datasets We obtain the datasets using the built-in get dataset function of the openml library. For preprocessing, we follow the procedure in Dinh et al. (2022). Speci\ufb01cally, we \ufb01rst remove all the rows whose labels are NaN and drop the columns with missing entries. Then, we normalize the columns as follows: Numerical features: we use the StandardScaler class in sklearn to scale the data to have zero mean and unit variance and then concatenate all numerical features as one feature Categorical features: one-hot encoding is used For training, we use the cross-entropy loss as the loss function, with the class weights set to 1/(num class samples). A.6.1. COMPLETE RESULTS FOR TABLE 4 (TOP) To compare with TabPFN (Hollmann et al., 2022) and use the baselines reported in their paper, we follow the same evaluation protocol and use the OVO (one-vs-one) AUROC (Area Under the ROC curve) as the score metric. The train-test split ratio is 0.5:0.5 to account for the limited context length of TabPFN. The detailed results for each method on each task is shown in Table 18, with the task meta-data shown in Table 19. We can see that there is not a single classi\ufb01cation method that performs best on all datasets. However, ORCA obtains good aggregate results in general, and its good performance on many challenging datasets where other baselines do no perform well makes it quite useful in real-life scenarios. We also report the training time for each method in Table 19, which shows that ORCA does not take signi\ufb01cantly longer time than non-deep-learning-based methods. We emphasize that our method needs to be trained on a per-task basis. This Cross-Modal Fine-Tuning: Align then Re\ufb01ne is in contrast with TabPFN, which \ufb01rst \ufb01ts a general prior network of\ufb02ine and then for every new task, inference can be performed online within seconds. Besides, it is worth noting that one concern with using pretrained language models to solve tabular tasks is that these models might have seen the tabular data during pretraining. This may affect the test metrics, but we currently do not have a method to verify the degree of the effect. Table 18: One-vs-one AUROC (\u2191) on 30 OpenML-CC18 datasets. Baseline numbers are taken from (Hollmann et al., 2022). ORCA achieves the best overall performance. LightGBM CatBoost XGBoost AutoGluon TabPFN ORCA-RoBERTa balance-scale mfeat-fourier breast-w mfeat-karhunen mfeat-morphologica.. mfeat-zernike cmc credit-approval credit-g diabetes tic-tac-toe vehicle eucalyptus analcatdata author.. analcatdata dmft pc4 pc3 kc2 pc1 banknote-authentic.. blood-transfusion-.. ilpd qsar-biodeg wdbc cylinder-bands dresses-sales MiceProtein car steel-plates-fault.. climate-model-simu.. 0.9938 0.9786 0.991 0.9979 0.9601 0.9716 0.7288 0.9415 0.7684 0.8247 0.9988 0.9232 0.8931 0.9999 0.5461 0.9301 0.8178 0.8141 0.8321 1 0.7144 0.6917 0.9126 0.9904 0.8556 0.5593 0.9997 0.9925 0.9626 0.9286 0.9245 0.9816 0.9931 0.9986 0.9629 0.9759 0.7256 0.9389 0.7852 0.8383 0.9992 0.9302 0.8979 0.9999 0.5589 0.9413 0.8247 0.8323 0.86 1 0.7403 0.7279 0.9217 0.9931 0.8757 0.5696 0.9999"}