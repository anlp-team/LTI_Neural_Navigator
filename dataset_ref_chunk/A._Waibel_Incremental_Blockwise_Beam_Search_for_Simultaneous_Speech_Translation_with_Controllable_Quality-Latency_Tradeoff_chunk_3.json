{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_Incremental_Blockwise_Beam_Search_for_Simultaneous_Speech_Translation_with_Controllable_Quality-Latency_Tradeoff_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of stopping beams with a repetition or <eos> in the algorithm described in the text?", "answer": " The purpose is to ensure that each individual beam is expanded until it hits the stopping criterion.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " After all the beams are stopped in the algorithm, what is done next?", "answer": " The best beam from the stopped beams is selected.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " Why is length normalization applied when comparing finished hypotheses based on score?", "answer": " Length normalization is applied because the stopped beams may contain hypotheses of different lengths.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " What is the main difference between the blockwise stream- ing beam search algorithm and the standard beam search?", "answer": " The blockwise streaming beam search algorithm applies beam search to the full-context offline models, whereas the standard beam search does not.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " What data sets are used in the experiments described in the text?", "answer": " The experiments use the MuST-C data set for English \u2192 German, English \u2192 Spanish, and English \u2192 French language pairs.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " What toolkit is used for the blockwise speech translation models?", "answer": " The ESP-Net toolkit is used for the blockwise speech translation models.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " What are some characteristics of the encoder and decoder in the blockwise speech translation models?", "answer": " The encoder has 12 layers, and the decoder has 6 layers. The model dimension is 256, with a feed-forward dimension of 2048 and 4 attention heads.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " How are the offline ST models evaluated?", "answer": " The offline ST models are evaluated using Simuleval toolkit. Translation quality is reported using detokenized case-sensitive BLEU, and latency is reported using length-aware average lagging (LAAL).", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " What is the purpose of incremental pruning in the first attempt on incremental BWBS?", "answer": " The purpose is to enable the incremental user experience while processing each block.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}, {"question": " What is the difference in BLEU performance between re-translation and incremental SST according to Table 1?", "answer": " The difference is between 0.6 and 0.7 BLEU in favor of re-translation SST.", "ref_chunk": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}], "doc_text": "only stop beams with a repetition or <eos> (Line 10). This ensures that each individual beam is expanded until it hits the stopping criterion. Once all the beams are stopped, we stop expanding the hypothe- sis for the current block. Following the incremental pruning de- scribed in Section 3.1, we select the best beam from the stopped beams (Line 15). It is important to note that the stopped beams may contain hypotheses of different lengths. Therefore, when comparing the finished hypotheses based on score (Line 14), we apply length normalization [28, 29]. Algorithm 2: Proposed improved blockwise stream- ing beam search algorithm for incremental ST Input Output: A set of hypotheses and scores : A list of blocks, blockwise ST model 1 for each block do 2 Encode block using the blockwise ST model; Stopped \u2190 \u2205; while #active beams > 0 and not max. length do 3 4 5 6 Extend beams and compute scores; for each active beam b do 7 8 9 10 if b contains a repetition or <eos> then Remove the last two tokens from b; Stopped \u2190 Stopped \u222a b; Remove b from the beam search; 11 end 12 end 13 14 15 16 17 end end Sort Stopped by length-normalized score; Set the best hypothesis from Stopped as active beam; Apply incremental policy; // Hold-n or LA 3.3. Blockwise Streaming Beam Search for Full-Context Models Finally, instead of standard beam search, we apply the pro- posed blockwise streaming beam search to the full-context of- fline models. To do so, we implement the onlinization frame- work by [6] described in Section 2.3. Since we use offline mod- els, we recompute the encoder and decoder states after each new chunk. 4. Experiments 4.1. Data In our experiments, we use the English \u2192 German, a Subject- Verb-Object (SVO) language to SOV language pair, English \u2192 Spanish, an SVO-SVO language pair, and English \u2192 French, an SVO-SVO language pair, of the MuST-C [30] data set. We use the training and validation sets during the training of the blockwise models. Finally, we use the tst-COMMON split for the evaluation of the online decoding algorithms. 4.2. Models For the blockwise speech translation models, we use the ESP- Net toolkit [31]. We preprocess the audio with 80-dimensional filter banks. For both language pairs, we built a unigram [32] vocabulary with a size of 4000. We build three models with block sizes of 20, 32, and 40 for each language pair. The en- coder has 12 layers, and the decoder has 6 layers. The model dimension is 256, feed-forward dimension is 2048 with 4 atten- tion heads. To improve the training speed, we initialize the en- coder with weights pretrained on the ASR task of the MuST-C dataset. Further, we employ ST CTC [12, 33] after the encoder with weight 0.3. However, we do not use the CTC loss during inference. Additionally, we employ checkpoint averaging for the last 10 epochs. For the offline ST models, we use encoder-decoder archi- tecture based on Transformer. Specifically, we use the pre- trained offline models introduced in [34].3 The models are im- plemented in Fairseq [35]. The encoder is based on wav2vec 2.0 [36]; therefore, the models\u2019 input is raw single-channel speech with 16k sampling frequency. All models are evaluated using Simuleval [19] toolkit. For the translation quality, we report detokenized case-sensitive BLEU [37], and for the latency, we report length-aware aver- age lagging (LAAL) [7, 38]. In all our experiments, we use beam search with size 6. For the hold-n strategy with the offline models, we use a fixed step size of 280 ms [5]. Additionally, we remove the repetition detection for the offline models. Our ini- tial experiments showed that the offline models do not generate repetitions. On the other hand, the blockwise models are prone to generate repetitions; therefore, we keep the repetition detec- tion on for all blockwise experiments. Lang Re-translation Incremental \u2206 En-De En-Es En-Fr 23.0 27.9 31.5 22.4 27.2 30.8 0.6 -0.7 -0.7 Table 1: Comparison of the re-translation and incremental translation approach in terms of BLEU on tst-COMMON. 4.3. Incremental Blockwise Beam Search In our first attempt on incremental BWBS, we apply incremen- tal pruning after each processed block, but without the incre- mental policies (see Section 3.1). In Table 1, we compare the performance between re-translation and incremental SST. The difference is between 0.6 and 0.7 BLEU in favor of re- translation SST \u2013 we deem this to be an acceptable degradation in order to enable the incremental user experience. Note that since SimulEval latency evaluation expects an incremental out- put, we do not evaluate the latency of re-translation models. 3https://github.com/facebookresearch/fairseq/ blob/main/examples/speech_text_joint_to_text/ docs/pre-training.md U E L B 33 32 31 30 29 28 27 26 25 24 bwbs U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-bwbs-hold-n 40-bwbs-la-n U E L B 33 32 31 30 29 28 27 26 25 24 bwbs 40-ibwbs-hold-n 40-ibwbs-la-n 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 1,500 2,000 2,500 3,000 3,500 4,000 LAAL (ms) LAAL (ms) LAAL (ms) (a) Latency control using block size. (b) Latency control using incremental policies. (c) Proposed improved IBWBS. Figure 2: English \u2192 French blockwise model. Lang Decoding LAAL BLEU Lang Decoding # fw. passes LAAL BLEU En-De En-Es En-Fr Blockwise Models BWBS IBWBS 2433 2355 BWBS IBWBS 2303 2335 BWBS IBWBS 2367 2390 Full-Context Models 23.2 23.8 26.1 27.0 28.4 32.0 En-De BS IBWBS 1,210,691 946,088 1693 1704 25.5 25.4 En-Es BS IBWBS 1,214,720 993,124 1696 1653 29.7 29.0 En-Fr BS IBWBS 1,358,738 1,022,294 1602 1604 33.8 33.7 Table 4: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with hold-n policy using onlinized full- context models. En-De BWBS IBWBS 2838 1879 28.0 27.6 En-Es BWBS IBWBS 4073 2678 33.1 33.0 En-Fr BWBS IBWBS 3420 2668 38.8 38.6 Table 2: Incremental SST with the original BWBS and the pro- posed IBWBS. English to German, Spanish, and French, respectively. For"}