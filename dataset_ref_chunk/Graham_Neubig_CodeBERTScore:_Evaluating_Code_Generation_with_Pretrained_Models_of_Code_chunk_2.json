{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_CodeBERTScore:_Evaluating_Code_Generation_with_Pretrained_Models_of_Code_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does CodeBERTScore allow us to do when comparing code pairs?", "answer": " CodeBERTScore allows comparing code pairs that are lexically different while taking into account the programmatic or natural language context.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " How does CodeBERTScore compute the similarity between tokens in generated code and tokens in reference code?", "answer": " CodeBERTScore computes the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " What is the main contribution of CodeBERTScore according to the text?", "answer": " The main contribution of CodeBERTScore is being a self-supervised metric for NL->Code evaluation, based on BERTScore, which leverages pretrained models without requiring manual labeling.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " Why is it mentioned that comparing code pairs with CodeBERTScore is more correlated with human preference and execution correctness?", "answer": " CodeBERTScore is more correlated with human preference and execution correctness because it can match variable names according to their semantic similarity and functional role in the code.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " What is the purpose of BERTScore in evaluating machine translation outputs?", "answer": " BERTScore is used to encode candidate and reference sentences separately, computing cosine similarity between their vectors to evaluate machine translations.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " How does BERTScore compute sentence-level precision and recall?", "answer": " BERTScore computes sentence-level precision by taking the maximum similarity score for every candidate vector and averages it, and recall by taking the average of maximum similarity scores for every reference vector.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " What is the formula for computing the F1 score in BERTScore?", "answer": " The F1 score in BERTScore is computed as the harmonic mean of precision and recall.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " What is the key difference between CodeBERTScore and BERTScore in terms of the context used in similarity computation?", "answer": " CodeBERTScore computes the similarity between generated and reference code without including the encoded context in the final computation, unlike BERTScore.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " Why does CodeBERTScore compute the F3 score instead of the F1 score mentioned in BERTScore?", "answer": " CodeBERTScore computes the F3 score to weigh recall higher than precision according to METEOR (Banerjee and Lavie, 2005).", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}, {"question": " What type of models does CodeBERTScore use for encoding, and what distinguishes them from other models?", "answer": " CodeBERTScore uses programming language-specific models that are pretrained and released, rather than models intended for natural language processing only.", "ref_chunk": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}], "doc_text": "et al., 2020). the generated code and the reference code inde- pendently with pretrained models, with the inclu- sion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. Code- BERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2. Example A concrete example is shown in Fig- ure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), Code- BERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Fig- ure 1(a)). We note that in this example, the vari- able names are identical across all three code snip- pets. When the variable names of the reference are different than the candidate\u2019s, it is even harder Generated Code **0.5Reference CodeGenerated Codemath xsqrt CodeBERTScoreCodeBERTScoreCodeBERTScorePrecisionRecallF-score # Find the square root of xEncode math.sqrt(x) Reference Code Similarity Matrixx x ** 0.5 Pairwise Cosine Similarity(only between non-punctuationcode tokens) Natural Language Instruction CodeBERT CodeBERT Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of \u27e8natural_language, reference_code\u27e9 and \u27e8natural_language, generated_code\u27e9. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the max across the rows of the resulting matrix to compute Precision and across columns to compute Recall. for token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code. Contributions In summary, our main contribu- tions are: (a) CodeBERTScore: a self-supervised metric for NL\u2192Code evaluation, based on BERTScore, which leverages the benefits of pre- trained models, while not requiring labeling or manually annotated data. (b) An extensive em- pirical evaluation across four programming lan- guages, showing that CodeBERTScore is more correlated with human preference and more cor- related with execution correctness than all pre- vious approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this sub- mission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times. didates is more important than the absolute value of f (\u02c6y, y\u2217). That is, ideally, if a prediction \u02c6y1 is more functionally equivalent to y\u2217 and more preferable by human programmers over a predic- tion \u02c6y2, we wish that a good metric would rank \u02c6y1 higher than \u02c6y2. That is, we seek an f function such that f (\u02c6y1, y\u2217) > f (\u02c6y2, y\u2217). 2.2 Background: BERTScore BERTScore (Zhang et al., 2020) was proposed as a method for evaluating mainly machine transla- tion outputs. The idea in BERTScore is to en- code the candidate sentence (the prediction) and the reference sentence (the ground truth) sepa- rately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences. 2 Evaluating Generated Code 2.1 Problem Formulation Given a context x \u2208 X (e.g., a natural language instruction or comment), a code generation model M : X \u2192 Y produces a code snippet \u02c6y \u2208 Y by conditioning on the intent specified by x. The quality of the generation is evaluated by compar- ing \u02c6y \u2208 Y with the reference implementation y\u2217 \u2208 Y, using a metric function f : Y \u00d7 Y \u2192 R, essentially computing f (\u02c6y, y\u2217). A larger value of f (\u02c6y, y\u2217) indicates that the gen- erated code is more accurate with respect to the reference code, and the way f ranks different can- Given these similarity scores, BERTScore com- putes sentence-level precision by taking the max- imum similarity score for every candidate vec- tor and averaging, and computes recall by tak- ing the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if ev- ery vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore- precision is obtained if every vector from the can- didate sentence is highly cosine-similar to at least one vector from the reference sentence. Ulti- mately, the final score is the F1 score, computed as the harmonic mean of precision and recall. CodeBERTScoreP CodeBERTScoreR = = 1 |\u02c6y[ \u02c6m]| 1 |y\u2217[m]| (cid:88) max y\u2217 i \u2208y\u2217[m\u2217] \u02c6yj \u2208\u02c6y[ \u02c6m] (cid:88) max \u02c6yj \u2208\u02c6y[ \u02c6m] y\u2217 i \u2208y\u2217[m\u2217] sim (y\u2217 i , \u02c6yj) sim (y\u2217 i , \u02c6yj) CodeBERTScoreF1 CodeBERTScoreF3 = = 2 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR CodeBERTScoreP + CodeBERTScoreR 10 \u00b7 CodeBERTScoreP \u00b7 CodeBERTScoreR 9 \u00b7 CodeBERTScoreP + CodeBERTScoreR Figure 3: Main equations for CodeBERTScore 2.3 CodeBERTScore enized sequence, resulting in sequences of vectors: Our approach generally follows BERTScore, with the following main differences: B (\u27e8x1,...,xk,y\u2217 1,...,y\u2217 m\u27e9 B (\u27e8x1,...,xk,\u02c6y1,...,\u02c6yn\u27e9) = \u27e8x1,...,xk, \u02c6y1,..., \u02c6yn\u27e9 1,...,y\u2217 m\u27e9) = \u27e8x1,...,xk,y\u2217 1. We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially com- puting f (\u02c6y, y\u2217, x) rather than f (\u02c6y, y\u2217). instead of computing the F1 score, we also compute F3 to weigh recall higher than precision, follow- ing METEOR (Banerjee and Lavie, 2005). 3. As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only. We use a"}