{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What issue do techniques like LC compression method attempt to resolve?", "answer": " Additional hyperparameters that can be tedious to fine-tune", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " How does the CUTTLEFISH method determine factorization hyperparameters during training?", "answer": " Automatically, on-the-fly", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " Name an alternative transformation method investigated besides LC compression and CUTTLEFISH.", "answer": " Butterfly matrices", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " What is an example of a novel architecture developed for enhanced training or inference efficiency?", "answer": " SqueezeNet", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " Explain the representation of a 2-layer fully connected neural network using factorized weight matrices.", "answer": " f(x) = \u03c3(\u03c3(xW1)W2), where Ws are factorized as UV", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " How can the trainable weights in a multi-head attention layer be factorized?", "answer": " By decomposing all learnable weights W in an attention layer and obtaining U\u00b7V\u00b7", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " What is the purpose of the hybrid NN architecture?", "answer": " To only factorize lower layers while keeping initial layers full-rank", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " Why is it common to train a full-rank model for E epochs before factorizing it?", "answer": " To mitigate the decrease in accuracy when training low-rank factorized models from scratch", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " What is the importance of spectral initialization in factorized low-rank networks?", "answer": " To approximate the behavior of existing initialization methods and improve model accuracy", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}, {"question": " What technique applies weight decay on UVT during factorized low-rank training?", "answer": " Frobenius decay", "ref_chunk": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}], "doc_text": "models (Hu et al., 2021). These techniques all necessitate additional hyperparameters, which can be tedious to \ufb01ne-tune. The LC compression method attempts to resolve this issue by explicitly learning R during model training through alter- nating optimization (Idelbayev & Carreira-Perpin\u00b4an, 2020). However, this approach is computationally demanding. Our proposed CUTTLEFISH method automatically determines all factorization hyperparameters during training on-the-\ufb02y, eliminating the heavy computation overhead and the need for multiple experimental trials for factorization hyperpa- rameter tuning. Alternative transformations have also been investigated, including Butter\ufb02y matrices (Chen et al., 2022), the fu- sion of low-rank factorization and sparsi\ufb01cation (Chen et al., 2021a), and block-diagonal matrices (Dao et al., 2022). Furthermore, novel architectures have been devel- oped for enhanced training or inference ef\ufb01ciency, such as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), Shuf\ufb02eNet (Zhang et al., 2018), Ef\ufb01cientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xcep- tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re- former (Kitaev et al., 2020). 2 PRELIMINARY In this section, we present an overview of the core con- cepts of low-rank factorization for various NN layers, along with a selection of specialized training methods speci\ufb01cally designed for low-rank factorized training. 2.1 Low-rank factorization of NN layers FC/MLP Mixer layer. A 2-layer fully connected (FC) neural network can be represented as f (x) = \u03c3(\u03c3(xW1)W2), where Ws are weight matrices, \u03c3(\u00b7) is an arbitrary activation function, and x is the input data point. The weight matrix W can be factorized as UV(cid:62). A simi- lar approach can be applied to ResMLP/MLP mixer layers, where each learnable weight can be factorized in the same manner (Touvron et al., 2021a; Tolstikhin et al., 2021). Convolution layer. For a convolutional layer with dimen- sions (m, n, k, k), where m and n are the number of input and output channels and k represents the size of a convo- lution \ufb01lter, a common approach involves factorizing the unrolled 2D matrix. We will discuss a popular method for factorizing a convolutional layer. Initially, the 4D tensor W is unrolled to obtain a 2D matrix of shape (mk2, n), where each column represents the weight of a vectorized convo- lution \ufb01lter. The rank of the unrolled matrix is determined by min{mk2, n}. Factorizing the unrolled matrix results in U \u2208 Rmk2\u00d7r and V(cid:62) \u2208 Rr\u00d7n. Reshaping the factor- ized U, V(cid:62) matrices back to 4D yields U \u2208 Rm\u00d7r\u00d7k\u00d7k and V(cid:62) \u2208 Rr\u00d7n. Consequently, factorizing a convolutional layer produces a thinner convolutional layer U with r convo- lution \ufb01lters and a linear projection layer V(cid:62). The V(cid:62)s can also be represented by a 1 \u00d7 1 convolutional layer, such as CUTTLEFISH: Low-rank Model Training without All The Tuning V(cid:62) \u2208 Rr\u00d7n\u00d71\u00d71, which is more suited for computer vision tasks since it operates directly in the spatial domain (Lin et al., 2013; Wang et al., 2021a). Multi-head attention (MHA) layer. A p-head attention layer learns p attention mechanisms on the key, value, and query (K, V, Q) of each input token: et al., 2020). Spectral initialization represents a special case of transitioning from full-rank to low-rank training with E = 0. Additionally, speci\ufb01c regularization techniques have been proposed to enhance the accuracy of low-rank net- works. For example, Frobenius decay applies weight decay on (cid:107)UV(cid:62)(cid:107)2 F during factorized low-rank training. F instead of (cid:107)U(cid:107)2 F + (cid:107)V(cid:62)(cid:107)2 MHA(Q, K, V) = Concat(head1, . . . , headp)WO. Each head performs the computation of: 3 CUTTLEFISH: AUTOMATED LOW-RANK headi = Attention(QW(i) (cid:32) QW(i) Q , KW(i) Q W(i)(cid:62) K K(cid:62) (cid:112)d/p K , VW(i) V ) (cid:33) VW(i) V . = softmax where d is the hidden dimension. The trainable weights W(i) V , i \u2208 {1, 2, . . . , p} can be factorized by simply decomposing all learnable weights W\u00b7 in an atten- tion layer and obtaining U\u00b7V(cid:62)\u00b7 (Vaswani et al., 2017). Q , W(i) K , W(i) 2.2 Training methods for low-rank networks Hybrid NN architecture. It has been noted that factor- izing the initial layers may negatively impact a model\u2019s accuracy (Kone\u02c7cn`y et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). One possible explanation is that NN layers can be viewed as feature extractors, and poor features extracted by the early layers can accumulate and propagate throughout the NN. To address this issue, the hybrid NN architecture was proposed, which only factor- izes the lower layers while keeping the initial layers full- rank (Wang et al., 2021a). The weights of a full-rank L-layer NN can be represented as W = {Wi|1 \u2264 i \u2264 L}. The corresponding hybrid model\u2019s weights can be represented as H = {W1, W2, . . . , WK, UK+1, V(cid:62) K+1, . . . , UL\u22121, V(cid:62) L\u22121, WL}, where K is the number of layers that are not factorized and is treated as a hyperparameter to be tuned manually (Wang et al., 2021a). It is important to note that the last classi\ufb01cation layer, i.e., WL, is usually not factor- ized (Khodak et al., 2020; Wang et al., 2021a). Full-rank to low-rank training. Training low-rank fac- torized models from scratch often results in a decrease in accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). To mitigate this drop, it is common to train the full-rank model for E epochs before factorizing it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). However, determining the appropriate number of full-rank training epochs is treated as a hyperparameter and typically tuned manually in experiments (Wang et al., 2021a). Observations indicate that \ufb01nding the right number of full-rank training epochs is crucial for achieving optimal \ufb01nal model accuracy in low-rank factorized NNs. Initialization and weight decay. Factorized low-rank networks can bene\ufb01t from tailored initialization meth- ods (Ioannou et al., 2015; Khodak et al., 2020). One such method, called spectral initialization, aims to approximate the behavior of existing initialization methods (Khodak FACTORIZED TRAINING In this section, we outline the problem formulation of CUT- TLEFISH, elaborate"}