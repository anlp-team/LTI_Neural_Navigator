{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Enhancing_End-to-End_Conversational_Speech_Translation_Through_Target_Language_Context_Utilization_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the primary focus of the study mentioned in the text?", "answer": " The primary focus is on enhancing end-to-end conversational speech translation through target language context utilization.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " What do the authors propose to improve coherence and overcome memory constraints in end-to-end speech translation?", "answer": " The authors propose incorporating target language context and introducing context dropout.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " Why is including longer context important in machine translation?", "answer": " Including longer context has been shown to benefit machine translation by improving coherence and accuracy.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " What specific approach is introduced in the text to address the limitations in conversational speech translation?", "answer": " The text introduces a context-aware end-to-end speech translation that leverages context in the output side.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " What does the proposed contextual end-to-end speech translation framework employ?", "answer": " The proposed framework employs target language context, robustness to context absence through dropout, and context enrichment with speaker information.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " What are the three unique challenges mentioned in relation to conversational speech translation?", "answer": " The challenges mentioned are high context dependence for meaning, informal language usage, and data scarcity.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " How does the proposed context-aware approach in the text differ from existing work?", "answer": " Unlike existing work, the proposed approach focuses on conversational speech translation and incorporates previous translations as context.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " What are the primary contributions of the study mentioned in the text?", "answer": " The primary contributions are a context-aware end-to-end speech translation framework, improved robustness through context dropout, and context enrichment with speaker information.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " Why is consistency crucial in language understanding according to the text?", "answer": " Consistency is crucial in language understanding because the meaning of utterances often depends on the broader conversational context.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}, {"question": " What is the architecture used in the proposed contextual end-to-end speech translation approach?", "answer": " The architecture includes conformer encoders with hierarchical CTC encoding and transformer decoders.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}], "doc_text": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 6 8 6 5 1 . 9 0 3 2 : v i X r a ENHANCING END-TO-END CONVERSATIONAL SPEECH TRANSLATION THROUGH TARGET LANGUAGE CONTEXT UTILIZATION Amir Hussein1, Brian Yan2, Antonios Anastasopoulos3, Shinji Watanabe2, Sanjeev Khudanpur1 1Johns Hopkins University, 2Carnegie Mellon University, 3George Mason University USA ABSTRACT Incorporating longer context has been shown to benefit machine trans- lation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and over- coming memory constraints of extended audio segments. Addition- ally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker infor- mation. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities. Index Terms\u2014 Speech translation, contextual information, end- to-end models, conversational speech 1. INTRODUCTION To address this limitation, we propose a context-aware E2E-ST that leverages context in the output (target) side. In particular, in a conversational context, we incorporate the previously output sen- tences (in the target language) as the initial condition on the decoder side to generate the translation of the current input utterance. This enables the decoder to effectively utilize textual information from longer utterances and focus on vital parts of the previous context for more accurate and consistent translation. Unlike existing work, we focus on conversational speech translation, an essential facet of daily communication which presents unique challenges: 1) high context dependence for meaning, 2) informal and grammatically in- consistent language usage, and 3) data scarcity. Our study covers three language pairs: Tunisian Arabic-English, Spanish-English, and Chinese-English. Our contributions encompass: (i) a context-aware E2E-ST framework employing target language context, (ii) enhanced robustness to context absence through context dropout, and (iii) con- text enrichment with speaker information. As an additional contribu- tion, we conduct an ablation study to assess the significance of each component (context size, context dropout, and speaker information) on final performance. Finally, we perform a part-of-speech-based analysis to identify where the primary improvements result from the incorporation of context. Speech translation (ST) is crucial for breaking language barriers and enhancing global communication. Seamlessly translating spo- ken conversations across languages impacts cross-cultural interac- tions, education, and diplomacy [1]. Traditionally, ST systems have been built by cascading automatic speech recognition (ASR) and ma- chine translation (MT) models [2\u20138]. However, recently end-to-end speech translation (E2E-ST) approaches, in which the source speech is directly translated to the target language text, have gained more attention [9\u201313]. Despite the promising results of recent research advancements in E2E-ST systems, the produced translations often lack consistency in translation. Consistency is crucial for language understanding, as the mean- ing of utterances often depends on the broader conversational context. In MT, source-side context is commonly utilized to address inaccu- rate choice of pronouns [14], mistranslations of ambiguous words [15], and general incoherence in translation [16]. Several MT studies showed that document level context [17\u201321] outperforms sentence level context [22\u201328]. Analogous to MT, incorporating contextual information in E2E-ST systems is expected to be valuable for co- herent translation, resolving anaphora, and disambiguating words, especially homophones. To study the effectiveness of context on E2E-ST, researchers have used simple concatenation of audio in- put as context and the corresponding translation as target output, reporting improvements in pronoun and homophone translation [29]. However, utilizing source-side context comes with the challenge of encoding very long audio segments, which can easily lead to memory bottlenecks, especially with self-attention based networks [30]. 2. CONTEXTUAL E2E-ST In standard E2E-ST, the goal is to find the most probable target word sequence \u02c6Y of length N , out of all possible outputs Y\u2217. This is done by selecting the sequence that maximizes the posterior likelihood P (Y|X), given a T -long sequence of D-dimensional speech features, represented as X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. In our approach we incorporate K previous translations Ycntx = {yl \u2208 Vtgt|l = 1, \u00b7 \u00b7 \u00b7 , K} in the target language Vtgt. We incorporate the context as an initial condition on the decoder side. Therefore, our objective is to maximize the posterior likelihood given both the input speech and the context: \u02c6Y = arg max N (cid:88) log(P (Yl|Y<l, X, Ycntx)) Y\u2217 l=1 We enrich the context with speaker role information, encoded as speaker tags within Vtgt. Figure 1 presents an illustrative example showcasing a target sentence along with a context of its two preceding sentences. This context is augmented with speaker role information: [SpkA] and [SpkB] represent to the first and second speaker in the conversation, respectively. The [SEP] tag indicates separation be- tween the sentences within the context. (1) ST Decoder ASR CTC ASR Encoder ST Encoder CTC ST ASR Decoder Fig. 1. Illustration of proposed contextual E2E-ST approach. [Context] [SpkA] I\u2019m from Peru, and you? [SEP] [SpkB] Puerto Rico. [Target] [SpkA] Oh, from Puerto Rico, oh, ok. It\u2019s important to note that, unlike [31] where speaker labels are used for prediction, we employ them solely in the context as initial condi- tion and do not predict them. 2.1. E2E-ST Architecture Our proposed contextual E2E-ST builds upon the CTC/Attention architecture composed of conformer encoders with hierarchical CTC encoding and transformer decoders [13, 32] as depicted in Fig 1. The CTC/Attention approach decomposes ST into ASR and MT encoder- decoder models. The ASR encoder, ENCasr(\u00b7), maps input speech X into hidden representation HE asr shown in Eq. (2). H E asr = EN Casr(X) Following this, the representation HE asr from Eq. (2) serves as input to both the ST encoder, ENCst(\u00b7), and the ASR decoder, DECasr(\u00b7), as demonstrated in Eq. (3,4) H E st = EN Cst(H E asr) H D asr = DECasr(H E (4) Given HE st from Eq. (3), the ST"}