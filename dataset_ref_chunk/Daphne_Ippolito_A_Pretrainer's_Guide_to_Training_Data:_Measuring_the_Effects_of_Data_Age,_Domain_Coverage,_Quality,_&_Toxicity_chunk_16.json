{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_A_Pretrainer's_Guide_to_Training_Data:_Measuring_the_Effects_of_Data_Age,_Domain_Coverage,_Quality,_&_Toxicity_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the evaluation metric used for the model trained with size 8x8, 128, 128, learning rate 0.1, and 10k training steps?", "answer": " AUC-ROC", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " How many training steps were used for the model trained with size 8x8, 512, 128, learning rate 0.1, and AUC-ROC evaluation metric?", "answer": " 50k", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " What evaluation metric is used for the toxicity identification evaluation on the Social Bias Frames dataset?", "answer": " AUC-ROC", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " What types of hateful speech are evaluated for the DynaHate dataset in terms of toxicity?", "answer": " Animosity, dehumanization, and derogation", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " How many responses per prompt are generated for toxic generation evaluation?", "answer": " 25", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " What API is used to score responses and label them as toxic in the toxic generation evaluation?", "answer": " Perspective API", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " What is the toxicity score threshold used to classify a response as toxic in the toxic generation evaluation?", "answer": " >= 0.5", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " How is the overall RepBias score computed in the evaluation of representational bias?", "answer": " By averaging the toxicity rates per identity marker and per example", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " In the time evaluation details, which five tasks are selected to measure the performance impact of temporal misalignments?", "answer": " Summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}, {"question": " What does Temporal Degradation (TD) measure in the time evaluation details?", "answer": " The average rate of deterioration in performance for a time period, measured in years", "ref_chunk": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}], "doc_text": "Size Dropout Training Steps Learning Rate Eval Metric 8x8 128 128 0.1 10k 1e-3 AUC-ROC 8x8 512 128 0.1 50k 1e-3 Acc 8x8 128 128 0.1 See Table 7 See Table 7 (By Dataset) See Table 7 8x8 512 128 0.1 100k 1e-3 Training Steps LM-Small (where different) 30k 50k 100k See Table 7 C.3 Toxicity Evaluation Details In this section we describe the evaluation details for the results presented in Section 5. Toxicity Identification For toxicity identification evaluation, we finetune separately on each of the follow- ing datasets: Social Bias Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022). We then evaluate on their respective test sets, using AUC-ROC, which computes the error rates over different classification thresholds. Note that each of these datasets has a slightly different version of toxicity they are evaluating for: biases and offensive stereotypes implied in language for Social Bias Frames, types of hateful speech like animosity, dehumanization, and derogation for DynaHate, and adversar- ial, subtle and implicit forms of toxicity towards minority groups (e.g., stereotyping, microaggressions), for Toxigen. We average over them to understand a model\u2019s capabilities in identifying toxicity broadly. Toxic Generation For toxic generation, we do not do any finetuning. Instead, we evaluate how often each model generates toxic continuations. To measure a model\u2019s propensity to generate sexual, profane, or toxic responses to a mix of benign and malicious inputs, we use prompts from the RealToxicityPrompts (Gehman et al., 2020) dataset, as well as the prompt set developed to explore representational bias in PaLM (Chowdhery et al., 2022). 30 For each model, we generate 25 responses per prompt, using top-k sampling (k = 40), with a temperature of 1.0. Following Chowdhery et al. (2022), we use the Perspective API to score responses, labeling those with a toxicity score >= 0.5 as toxic. We compute the probability one of these generations per prompt is toxic, for two evaluation splits, one with non-toxic inputs, and one with toxic inputs. The resulting evaluation metrics are denoted RPT-T (for Toxic inputs) and RPT-NT (for Non-Toxic inputs). The Representational Bias benchmark was constructed from the identity terms in (Rae et al., 2021) and is described in more detail in Chowdhery et al. (2022). It uses templates constructed for several identity markers, for which we use the subset related to gender, race, and religion. Following the practice with RealToxicityPrompts, we sample 50 responses per prompt, use top-k sampling (k = 40, temperature = 1.0), and then classify a response as toxic if Perspective API evaluates its toxicity score >= 0.5. We average the toxicity rates per identity marker and per example to compute the overall RepBias score, where higher indicates more toxic responses were produced on average. We also compute the 95% confidence interval to show where changes in mean are significant. C.4 Time Evaluation Details This section describes the evaluation details for the results presented in Section 4. In applied settings, the available training data (either for pretraining or finetuning) may be from different years than the test-time data. To mimic these situations, Luu et al. (2021) construct several datasets segmented by the year they are collected from in order to measure the performance impact of differences in the time of collection of finetuning and evaluation splits. As described in Section 2.3, we select 5 of the datasets that are shown to be quite sensitive to these temporal misalignments, and that cover different tasks and data sources. These tasks are summarization, named entity recognition, classifying political affiliation, classifying academic topic, and classifying the news source. Due to the unique nature of each of these tasks in the temporal degradation experiments, we simply finetune on each task individually, before evaluating on their respective test sets. For each dataset, we finetune using 4x4 TPUs with a batch size of 64, a maximum sequence length of 128, and we validate every 500 training steps. We select the test set score with the highest validation accuracy across training. The best learning rate and the total number of steps required to reach convergence varied by model and model size, and are reported in Table 7. These hyperparameters are chosen based on initial experiments attempting to produce stable learning curves which peak near the values observed in Luu et al. (2021). Table 7: Time Dataset & Training Details: For each of the five datasets used to evaluate the model\u2019s ability over different temporal periods, we report the learning rate and number of steps used in each model size. These hyperparameters were chosen to ensure consistent convergence and stability within our infrastructure settings. LM-XL LM-Small Domain Task Metric LR Steps LR Steps News PubCLS Acc 1e-4 NewSum Rouge-L 5e-4 30k 1e-3 40k 1e-3 30k 40k Twitter PoliAff Acc TwiERC Acc 1e-4 1e-4 15k 1e-4 30k 1e-3 15k 30k Science AIC Acc 1e-4 30k 1e-3 60k We follow Luu et al. (2021)\u2019s exact prescription in calculating Temporal Degradation (TD), as well as their reported Pearson correlation measurements (r). Temporal degradation can be interpreted as the average rate of deterioration in performance for a time period, measured in years. Since a temporal deterioration score is calculated per evaluation year, we average over all evaluation years to compute a final TD score for a dataset. Furthermore, each dataset has a different span of available training and evaluation years. To account for this, we follow Luu et al. (2021) in presenting the Pearson correlation coefficient, which presents the strenght of 31 the relationship between time differences and performance deterioration. We also replicate the Wald test with null hypothesis that the slope is zero. For evaluating the temporal degradation of pretraining, TDp, we modify Luu et al. (2021)\u2019s original formula to measure the different D(t\u2032 \u2192 t) where t\u2032 is now the pretraining year. However, in this setting, performance samples are represented with different finetuning years. To account for this, we only compare the relative performance changes of the pretraining year tp, against models with the same finetuning tf"}