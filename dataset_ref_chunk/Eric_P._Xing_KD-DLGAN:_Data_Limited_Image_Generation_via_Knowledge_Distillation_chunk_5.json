{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_KD-DLGAN:_Data_Limited_Image_Generation_via_Knowledge_Distillation_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is KD-DLGAN and what does it aim to improve?,answer: KD-DLGAN is a data-limited image generation framework that aims to improve the generation performance of images.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " What are the two generative knowledge distillation techniques used in KD-DLGAN?,answer: The two generative knowledge distillation techniques used in KD-DLGAN are AGKD and CGKD.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " How does KD-DLGAN complement the state-of-the-art in data-limited image generation?,answer: KD-DLGAN complements the state-of-the-art and greatly improves the generation performance.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " What do the qualitative ablation studies in Fig. 4 demonstrate?,answer: The ablation studies demonstrate that the AGKD and CGKD techniques can produce more realistic generation than the baseline.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " How does the KD-DLGAN improve generation diversity?,answer: KD-DLGAN improves generation diversity by enforcing diverse correlations between images and texts.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " What does LPIPS measure in terms of generation diversity?,answer: LPIPS measures the diversity of generated images, where higher LPIPS indicates better diversity.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " What is the significance of using the Alexnet model for LPIPS evaluation?,answer: The significance of using the Alexnet model for LPIPS evaluation is to evaluate generation diversity.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " How does combining AGKD and CGKD impact generation performance?,answer: Combining AGKD and CGKD leads to the best generation performance, showing that they complement each other.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " How does KD-DLGAN perform compared to other knowledge distillation methods in Table 6?,answer: KD-DLGAN consistently outperforms other knowledge distillation methods by large margins in Table 6.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}, {"question": " What does the comparison with Vision-aided GAN in Table 8 reveal about KD-DLGAN?,answer: The comparison shows that KD-DLGAN consistently outperforms Vision-aided GAN, demonstrating its effectiveness in improving generation performance.", "ref_chunk": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}], "doc_text": "baseline DA [53] (Row 1), the state-of-the-art in data- limited image generation. KD-DLGAN combining AGKD and CGKD generates the most realistic images. Table 3 also show the results of KD-DLGAN when imple- menting over the state-of-the-art data-limited image gener- ation approaches. KD-DLGAN complements the state-of- the-art and improves the generation performance greatly. the two KD techniques complement each other. Qualitative ablation studies in Fig. 4 show that the pro- posed AGKD and CGKD can produce clearly more realistic generation than the baseline, demonstrating the effective- ness of these two generative knowledge distillation tech- niques. In addition, KD-DLGAN combining AGKD and CGKD performs the best, which further veri\ufb01es that AGKD and CGKD are complementary to each other. 4.5. Discussion In this subsection, we analyze our KD-DLGAN from several perspectives. All the experiments are based on the CIFAR-10 and CIFAR-100 dataset with 10% data. Generation Diversity: The proposed KD-DLGAN im- proves the generation diversity by enforcing the diverse cor- relation between images and texts, which eventually im- proves the generation performance. In this subsection, we evaluate the generation diversity with LPIPS [52]. Higher LPIPS means better diversity of generated images. As Ta- ble 5 shows, the proposed KD-DLGAN outperforms the baseline DA [53], the state-of-the-art in data-limited image generation, demonstrating the effectiveness of KD-DLGAN in improving generation diversity. We also show the re- sults of KD-DLGAN without CGKD; it further veri\ufb01es that the improved generation diversity is largely attributed to the CGKD, which distills diverse image-text correlation from CLIP [36] to the discriminator, ultimately improving the generation performance. Note we choose Alexnet model with linear con\ufb01guration for LPIPS evaluation. 4.4. Ablation study The proposed KD-DLGAN consists of two generative knowledge distillation (KD) techniques, namely, AGKD and CGKD. The two techniques are separately evaluated to demonstrate their contributions to the overall generation performance. As Table 4 shows, including either AGKD or CGKD clearly outperforms DA [53], the state-of-the-art in data-limited image generation, demonstrating the effec- tiveness of the proposed AGKD and CGKD in mitigating the discriminator over\ufb01tting and improving the generation performance. In addition, combining AGKD and CGKD leads to the best generation performance which shows that Generalization of KD-DLGAN: We study the general- ization of our KD-DLGAN by performing experiments with different GAN architectures, generation tasks and the num- ber of training samples. Speci\ufb01cally, as shown in Table 1- 3, we perform extensive evaluations over BigGAN and StyleGAN-v2. Meanwhile, we benckmark KD-DLGAN on object generation tasks with CIFAR and ImageNet and face generation tasks with 100-shot and AFHQ. Besides, we per- form extensive evaluations on 100-shot and AFHQ with few hundred samples, CIFAR with 100%, 20% and 10% data, ImageNet with 10%, 5% and 2.5% data. Method CIFAR-10 CIFAR-100 10% data 10% data DA [53] (Baseline) KD-DLGAN w/o CKGD KD-DLGAN 0.202 0.204 0.221 0.236 0.237 0.264 Table 5. KD-DLGAN improves the generation diversity clearly. And the improvement is largely attributed to CGKD, which distills diverse image-text correlations from CLIP [36] to the discrimina- tor. We report LPIP (\u2191) averaged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Fitnets [37] Label Distillation [17] PKD [35] SPKD [41] KD-DLGAN (Ours) 23.34 \u00b1 0.09 22.03 \u00b1 0.07 20.46 \u00b1 0.10 21.34 \u00b1 0.08 19.11 \u00b1 0.07 14.20 \u00b1 0.06 35.39 \u00b1 0.08 33.93 \u00b1 0.09 34.14 \u00b1 0.11 32.15 \u00b1 0.13 31.97 \u00b1 0.10 18.03 \u00b1 0.11 Table 6. KD-DLGAN outperforms the state-of-the-art knowledge distillation methods by large margins, demonstrating the effective- ness of the two generative knowledge distillation techniques de- signed speci\ufb01cally for data-limited image generation. We report FID(\u2193) averaged over three runs. Comparison with state-of-the-art knowledge distilla- tion methods: KD-DLGAN is the \ufb01rst to explore the idea of knowledge distillation in data-limited image generation. To validate the superiority of our designs, we compare KD- DLGAN with state-of-the-art knowledge distillation meth- ods designed for other tasks in Table 6. It shows that our KD-DLGAN outperforms the state-of-the-art knowl- edge distillation approaches consistently by large margins. The superior generation performance demonstrates the ef- fectiveness of our designed generative knowledge distilla- tion techniques for data-limited image generation. Comparison with other Vision-Language teacher models: KD-DLGAN adopted CLIP [36] as the teacher model for knowledge distillation. We perform experiments to study how different vision-language models affect the generation performance. As shown in Table 7, different vision-language models produce quite similar FIDs. We conjecture that these pertrained models provide suf\ufb01cient vision-language information for distillation and the perfor- mance gain mainly comes from our designed generative knowledge distillation techniques instead of the selected teacher model. Comparison with other CLIP-based methods: KD- DLGAN distills knowledge from CLIP [36] to the dis- criminator with two novelly designed generative knowledge distillation techniques while Vision-aided GAN [25] em- ploys off-the-shelf models as additional discriminators for data-limited generation. Table 8 compares KD-DLGAN Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) + TCL [46] + BLIP [26] + CLIP [36] (Ours) 23.34 \u00b1 0.09 14.98 \u00b1 0.09 15.74 \u00b1 0.10 14.20 \u00b1 0.06 35.39 \u00b1 0.08 18.43 \u00b1 0.12 18.88 \u00b1 0.11 18.03 \u00b1 0.11 Table 7. Employing different pretrained vision-language models as teacher models, the results are similar. We report FID(\u2193) aver- aged over three runs. Method CIFAR-10 10% data CIFAR-100 10% data DA [53] (Baseline) Vision-aided GAN [25] KD-DLGAN (Ours) 23.34 \u00b1 0.09 16.24 \u00b1 0.08 14.20 \u00b1 0.06 35.39 \u00b1 0.08 19.11 \u00b1 0.10 18.03 \u00b1 0.11 Table 8. KD-DLGAN outperforms CLIP-based Vision-add GAN [25], demonstrating the effectiveness of our KD-DLGAN in mitigating discriminator over\ufb01tting and improving the generation performance. We report FID(\u2193) averaged over three runs. with Vision-aided GAN. We can observe that KD-DLGAN outperforms CLIP-based Vision-aided GAN consistently, demonstrating its effectiveness in mitigating discriminator over\ufb01tting and improving generation performance. 5. Conclusion In this paper, we present KD-DLGAN, a novel data- limited image generation framework that introduces knowl- edge distillation for effective GAN training with limited data. We design two novel generative knowledge distil- lation techniques, including aggregated generative knowl- edge distillation (AGKD) and correlated generative knowl- edge distillation (CGKD). AGKD mitigates the discrimina- tor"}