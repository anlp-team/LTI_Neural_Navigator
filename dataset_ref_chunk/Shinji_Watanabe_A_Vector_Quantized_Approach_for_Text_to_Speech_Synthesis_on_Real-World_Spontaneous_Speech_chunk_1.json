{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What kind of speech do recent Text-to-Speech (TTS) systems trained on reading or acted corpora achieve near human-level naturalness?", "answer": " Reading or acted speech", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " Why is the ability to handle diverse human speech crucial for AI systems according to the text?", "answer": " To achieve human-level communication", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What type of data does the text mention using for building speech synthesizers?", "answer": " Real-world speech from YouTube and podcasts", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What does the text describe as a mismatch between training and inference alignments in mel-spectrogram based autoregressive models?", "answer": " Leading to unintelligible synthesis", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What is the architecture of the system called MQTTS designed for?", "answer": " Multiple code generation and monotonic alignment", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What does the text mention as an emerging challenge when training TTS systems on real-world speech?", "answer": " Higher variation of prosody and background noise", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What is the reason provided in the text for adopting multiple codebooks in the system design?", "answer": " To cover the diverse prosody patterns presented in spontaneous speech", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What are the properties needed for real-world speech synthesis according to the text?", "answer": " Higher resiliency to input noise and multiple codebooks", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What does MQTTS achieve higher than the non-autoregressive approach according to the text?", "answer": " Prosody diversity", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}, {"question": " What does the text suggest using during inference to encourage the model on generating clean speech despite training on a noisy corpus?", "answer": " A clean silence audio prompt", "ref_chunk": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}], "doc_text": "3 2 0 2 b e F 8 ] S A . s s e e [ 1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu Abstract Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the ef\ufb01cacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures. 1 Introduction A crucial component of Arti\ufb01cial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signi\ufb01cant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI. In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be \ufb01ne-tuned to speci\ufb01c speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments. With real-world speech, we \ufb01rst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We \ufb01nd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- suf\ufb01cient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speci\ufb01c architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3. We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1. Copyright \u00a9 2023, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. 1https://github.com/b04901014/MQTTS 2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram\u2013vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl\u00a8uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modi\ufb01cations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external"}