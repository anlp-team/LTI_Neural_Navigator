{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_Do_All_Languages_Cost_the_Same?_Tokenization_in_the_Era_of_Commercial_Language_Models_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of using a subset of FLORES-200 in the text?", "answer": " To tokenize each sentence in multiple languages and compute the average tokens per sentence.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " Why is it considered that language A is more efficiently tokenized than language B?", "answer": " If it uses fewer tokens per sentence on average.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What method is used to compare the LLM API costs across languages?", "answer": " Using a parallel corpus and computing the average number of tokens in a sequence.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " How are the models evaluated in RQ2 and RQ3?", "answer": " On NLP tasks involving long-form texts at input or output.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What is XNLI?", "answer": " An inference benchmark comprising 11 typologically diverse languages.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What is XFACT?", "answer": " A multilingual fact verification dataset covering 25 languages.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What is XQUAD?", "answer": " A crosslingual question-answering dataset.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What does the Prompting Formulation evaluate?", "answer": " It evaluates both models in a k-shot in-context learning setup.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What does Figure 2 show about the average number of tokens by script?", "answer": " That Latin-script languages require substantially fewer tokens compared to other scripts.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}, {"question": " What influential factors are identified for the non-uniformity of a tokenizer across languages?", "answer": " The proportion of the language in the pretraining data and inherent properties of the language and its writing script.", "ref_chunk": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}], "doc_text": "tokens in dif- ferent languages\u2014we use a subset of FLORES- 200 (Goyal et al., 2022), a multilingual parallel cor- pus containing examples in over 200 languages.7 We tokenize each sentence in the FLORES-200 subset with ChatGPT\u2019s tokenizer8 and compute the average number of tokens per sentence for each language. Using parallel data controls for the same information across languages. We consider that language A is more efficiently tokenized than lan- guage B if it uses fewer tokens per sentence on average. While previous studies have computed fragmentation rates with fertility (\u00c1cs, 2019), we 6https://huggingface.co/docs/api-inference/q uicktour. 7We also experimented with WMT 2021 data (Akhbardeh et al., 2021) and found similar results. Note that the WMT data are focused on European languages. 8see ChatGPT\u2019s tokenizer https://github.com/opena i/tiktoken instead define it as the average number of tokens in a sequence for two reasons. First, our goal is to compare LLM API costs across languages that charge users based on the number of tokens. To control for content, we use a parallel corpus for this analysis. Second, many languages we analyze are understudied and do not have word tokenizers available which are required to compute fertility. For RQ2 and RQ3, to clearly highlight the cost and utility disparities, we evaluate the models on NLP tasks that involve long-form texts either at input or output. We evaluate the models on di- verse, challenging natural language generation and classification tasks on the following benchmarks: Classification We evaluate on (1) XNLI (Con- neau et al., 2018): inference benchmark comprising of 11 typologically diverse languages. It involves two sub-tasks, passage selection and minimum answer span (Gold-P). We focus on the latter task in our experiments. (2) XFACT (Gupta and Srikumar, 2021): a multi- lingual fact verification dataset of naturally existing real-world claims covering 25 languages. a cross-lingual Span Prediction We use XQUAD (Artetxe et al., 2019): a crosslingual question-answering dataset where each example consists of a paragraph, a ques- tion, and the answer as a span in the paragraph. Generation We (1) Cross a cross-lingual Sum (Hasan et al., 2021a): abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs, and, (2) XLSUM (Hasan et al., 2021b): a summarization dataset covering 44 typologically diverse languages. The dataset comprises news articles and summaries in the same language as the article. evaluate on 3.3 Prompting Formulation We evaluate both models in a k-shot in-context learning setup where we also provide task instruc- tions. We experiment with 0 \u2264 k \u2264 X, where X is the maximum number of in-context examples that can be provided. Note that X is not a fixed value, but is determined by the language model API\u2019s limit on the number of input tokens and the fragmentation rate of the language. For all tasks, we provide the instructions in En- glish following Ahuja et al. (2023), who show that on several multilingual benchmarks, English in- structions outperform the in-language prompts (see 122111221128131112 LatinJapaneseHangulCyrillicArabicThaiHebrewDevanagariGreekBengaliTamilTeluguGeorgianTibetanLanguageScript 200 0 100 300Averagenumberoftokens Figure 2: Average number of tokens by script after tokenizing the Flores dataset. The fragmentation rate is lower for Latin script languages and higher for other scripts. Number of languages per language group is indicated at the top of each bar. Table 2 in the Appendix for the prompting format for all tasks). For each task, we randomly sample at most 500 test examples for evaluation. 4 Results and Analysis 4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens? In Figure 2 we show that Latin-script languages are represented with substantially fewer tokens com- pared to languages in other scripts. While Cyrillic and Japanese script languages come close to the Latin script, languages that have their own script, e.g. Telugu and Georgian, require up to 5\u00d7 more tokens to convey the same information. We hy- pothesize that this disparity is due to training data imbalance since ChatGPT\u2019s tokenizer was primar- ily trained with Latin-script languages, mainly En- glish. The training details of ChatGPT are not available. However, we make a reasonable assump- tion that its training dataset has a similar propor- tion of languages as the publicly available large corpus CC100 (Wenzek et al., 2020). If we sort lan- guages shown in Figure 2 based on their data size in CC100 (see Figure 14 in the Appendix), low- resourced languages of Latin script appear to be less fragmented compared to other mid-resourced languages of non-Latin scripts. In Figure 15 in the Appendix, we present a similar analysis for BLOOMZ\u2019s tokenizer. We sort the languages based on their size in the pretraining data (ROOTS corpus; Lauren\u00e7on et al., 2023). We observe that languages with fewer resources generally have a higher av- erage token length. Arabic is an outlier here as it appears to be have more tokens than some other mid-resourced languages. What influences the non-uniformity of a to- kenizer across languages? From our analysis above, we identify two influential factors: (1) the proportion of the language in the pretraining data, and (2) inherent properties of the language and its writing script. While we see some correlation be- tween pretraining data size and fragmentation rate in BLOOMZ, with ChatGPT it is quite different as higher-resourced non-Latin script languages still get excessively tokenized. To disentangle the effects of factors (1) and (2) we train BBPE tokenizers on a variety of languages with diverse scripts with vocabulary sizes ranging from 5,000 to 50,000, while controlling for content and data size. Specifically, we train the tokenizers on parallel corpora and include one language per script. We then use these tokenizers to tokenize the text they were trained on, and compute the average number of tokens per sentence. 5000 500 50 200 Latin Cyrillic 350 50000Vocabsize 10000 Khmer 25000 1000 Tibetan 200 250 Myanmar 100 150 300 400Avgnumberoftokens Hangul Arabic Japanese Figure 3: BBPE tokenizer trained on parallel text from different language scripts with varying vocabulary sizes. We display a larger version with 21 more scripts in Figure 18 in the Appendix. As shown"}