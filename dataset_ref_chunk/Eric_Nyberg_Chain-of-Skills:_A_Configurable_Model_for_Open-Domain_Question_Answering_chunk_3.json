{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Chain-of-Skills:_A_Configurable_Model_for_Open-Domain_Question_Answering_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the formulation discussed in the text encourage hr to do?", "answer": " The formulation encourages hr to capture more information about the question and to focus more on the evidence.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " Why does the formulation spare the need for an extra task-specific header?", "answer": " The formulation spares the need for an extra task-specific header because the model only learns to rerank single passages.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " What is one drawback mentioned for implementing all modules in separate models?", "answer": " The drawback mentioned is that it is apparently inefficient.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " What approach is focused on in order to mitigate the task interference?", "answer": " The approach focused on is mixing skill-specific Transformer blocks with shared ones.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " Why is the use of FFN expert sub-layer found to be sub-optimal for different reasoning skills?", "answer": " Using the FFN expert sub-layer is found to be sub-optimal because it likely hinders knowledge sharing among different skills.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " What is proposed as a more parameter-efficient alternative to using FFN specialization?", "answer": " MHA specialization is proposed as a more parameter-efficient alternative.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " What is the reasoning behind investigating MHA specialization as an alternative?", "answer": " The reasoning is that different skills typically require the model to attend to distinct input parts.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " What is the purpose of the expert configuration shown in Figure 3?", "answer": " The purpose is to route various task inputs to their dedicated sub-layers (experts) for different tasks.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " In the context of multi-task self-supervision, what type of skills are considered for pretraining?", "answer": " Bi-encoder skills (single retrieval, expanded query retrieval, and entity linking) are considered for pretraining.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}, {"question": " What is done during inference to boost retrieval accuracy by combining different skills?", "answer": " During inference, different skills are flexibly combined to boost retrieval accuracy by aligning and sorting the linking and retrieval scores.", "ref_chunk": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}], "doc_text": "[SEP])) (5) where Pr is the set of negative passages concate- nated with the same question. Intuitively, our for- mulation encourages hr [CLS] to capture more in- formation about the question and hr [SEP] to focus more on the evidence. The positive pair where the evidence is supportive likely has higher similarity than the negative ones. Our formulation thus spares the need for an extra task-specific header. As the model only learns to rerank single passages, we compute the score for each passage separately for multi-hop cases. (3) (4) , 3.2 Modular Skill Specialization Implementing all aforementioned modules using separate models is apparently inefficient. As recent work finds that parameter sharing improves the bi- encoder retriever (Xiong et al., 2021b), we thus focus on a multi-task learning approach. One popular choice is to share the text encoder\u2019s parameter of all modules (Liu et al., 2019a). How- ever, this approach suffers from task interference, resulting in degraded performance compared with the skill-specific model (\u00a75.1). We attribute the cause to the competition for the model capacity, i.e., conflicting signals from different skills require attention to individual syntactic/semantic patterns. For example, the text encoder for entity-centric queries likely focuses on the local context around the entity while the expanded query one tends to represent the latent information based on the rela- tion between the query and previous hop evidence. Motivated by recent modular approaches for sparse Transformer LM (Fedus et al., 2021b), we propose to mitigate the task interference by mix- ing skill-specific Transformer blocks with shared ones. A typical Transformer encoder is built with a stack of regular Transformer blocks, each consist- ing of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2015) and layer- normalization (Ba et al., 2016) applied to both sub- layers. The shared Transformer block is identical to a regular Transformer block, i.e., all skill inputs are passed through the same MHA and FFN functions. As shown in Figure 2, for skill-specific Trans- former blocks, we select a specialized sub-layer from a pool of I parallel sub-layers based on the input, i.e., different skill inputs are processed inde- pendently. One option is to specialize the FFN ex- pert sub-layer for individual skills, which is widely used by recent mixture-of-expert models (Fedus et al., 2021b; Cheng et al., 2022). As the FFN sub-layer is found to be important for factual asso- ciations (Meng et al., 2022), we hypothesize that using the popular FFN expert is sub-optimal. Since most reasoning skills require similar world knowl- edge, specializing FFN sub-layers likely hinders knowledge sharing. Instead, different skills typ- ically require the model to attend to distinct in- put parts. Thus, we investigate a more parameter- efficient alternative, i.e., MHA specialization. In our experiments, we find it to be more effective in reducing task interference (\u00a75.1). Figure 3: Expert configuration for COS at pretraining and fine-tuning. Each numbered box is a skill-specific expert. The lines denote input routing where solid ones also indicate weight initialization mappings. Green lines highlight the expanded query routing which is different for pretraining and fine-tuning. Expert Configuration Regarding the modulariza- tion, a naive setup is to route various task inputs to their dedicated sub-layers (experts), i.e., two experts for each bi-encoder task (single retrieval, expanded query retrieval and entity linking) and one expert for each cross-encoder task (entity span proposal and reranking), leading to eight experts in total. To save computation, we make the following adjustments. Given that single and expanded query retrievers share the same set of target passages, we merge the context expert for both cases. Due to data sparsity, we find that routing the expanded queries and reranker inputs which are very similar to separate experts is problematic (\u00a75.1). Thus, we merge the expert for expanded queries and reranker inputs. During self-supervised pretraining with three bi-encoder tasks, we further share the expert for single and expanded queries for efficiency. The overall expert configuration is shown in Figure 3. Multi-task Self-supervision Inspired by the recent success of Izacard et al. (2021), we also use self- supervision on Wikipedia for pretraining. Here, we only consider pretraining for bi-encoder skills (i.e., single retrieval, expanded query retrieval, and entity linking) where abundant self-supervision is available. Unlike prior work focusing only on single-type pretraining, we consider a multi-task setting using individual pages and the hyperlink relations among them. Specifically, we follow Izac- ard et al. (2021) and Wu et al. (2020) to construct examples for single retrieval and entity linking, re- spectively. For single retrieval, a pair of randomly cropped views of a passage is used as a positive example. For entity linking, a short text snippet with a hyperlinked entity (entity mention context) is used as the query, and the first paragraph of its linked Wikipedia page is treated as the target (en- tity description). For a given page, we construct an expanded query using a randomly-sampled short text snippet with its first paragraph, and use one first paragraph from linked pages as the target. 3.3 Inference During inference, different skills can be flexibly combined to boost retrieval accuracy. Those stud- ied configurations are illustrated in Figure 1. To consolidate the evidence set obtained by different skills, we first align the linking scores based on the same step retrieval scores (single or expanded query retrieval) for sorting. Documents returned by multiple skills are considered more relevant and thus promoted in ranking. More details with run- ning examples are provided in Appendix A. 4 Experiments 4.1 Datasets We consider six popular datasets for evaluation, all focused on Wikipedia, with four single-hop data, NQ (Kwiatkowski et al., 2019), WebQ (Be- rant et al., 2013), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (Sciavolino et al., 2021); two multi-hop data, HotpotQA (Yang et al., 2018) and OTT-QA (Chen et al., 2021a). Dataset-specific corpora are used for multi-hop datasets, because HotpotQA requires retrieval hopping between text passages while table-passage hopping is demanded by OTT-QA. For single-hop data, we use the Wikipedia corpus from Karpukhin"}