{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Massively_Multilingual_ASR_with_Auxiliary_CTC_Objectives_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many languages are included in the FLEURS dataset?", "answer": " 102", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " How many hours of training data does each language in FLEURS have?", "answer": " 7-10 hours", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " Which pre-trained Self-Supervised Learning features were used in the experiment?", "answer": " XLS-R and WavLM", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " What augmentation techniques were applied to the acoustic inputs?", "answer": " SpecAugment and speech perturbation", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " What was used to tokenize the input text?", "answer": " SentencePiece with a vocabulary size of 6500", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " What architectures were experimented with as the encoder?", "answer": " Transformer and Conformer", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " What is the CTC weight used in the encoder-decoder setup?", "answer": " 0.3", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " How many attention heads does each encoder layer have?", "answer": " 8", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " How many Transformer layers were used in the decoder?", "answer": " 6", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}, {"question": " How many checkpoints with the highest validation accuracy were averaged?", "answer": " 3", "ref_chunk": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}], "doc_text": "a 102-language ASR dataset. Each utterance is a news snippet read by a speaker and contains only one language. Each language in FLEURS has around 7-10 hours of training data, for a total of 987 training hours. Due to the limited amount of supervised data for each language, we experimented with two pre-trained Self-Supervised Learning (SSL) features that performed well on SUPERB [36]: XLS- R [19] and WavLM [37]. The acoustic inputs are augmented by SpecAugment [38] and speech perturbation [39]. Input text was prepended by language identi\ufb01cation tokens, before tokenization by SentencePiece [40] with a vocabulary size of 6500. 4.2. Model Con\ufb01guration All experiments were conducted through ESPnet2 [41]. We use an encoder-decoder setup trained on the hybrid CTC/Attention [23] multi-task objective, with a CTC weight of \u03bb = 0.3. We experiment with both Transformer [42] and Conformer [43] architectures as the encoder. The encoder has either 18 Transformer layers or 12 (13) (14) (15) (16) (17) (18) Table 2. Comparing the effectiveness of SSL features, reporting CER, MER, LID % accuracy on FLEURS. XLS-R signi\ufb01cantly outperforms WavLM in multilingual ASR. ID Model SSL Features Test CER(\u2193) MER(\u2193) LID(\u2191) Transformer A1 CTC/Attention A2 +SC-CTC B1 CTC/Attention B2 +SC-CTC WavLM WavLM XLS-R XLS-R 14.6 14.4 13.9 13.7 41.8 40.8 39.7 38.8 95.09 94.47 95.73 95.39 Conformer layers. Each encoder layer has 8 attention heads and 2048 hidden units. The 6-layer Transformer decoder also has 8 attention heads and 2048 hidden units each. We average 3 checkpoints with the highest validation accuracy. We perform joint CTC/Attention decoding with a language model, using a beam size of 10 and CTC weight of 0.3. Model parameters totaled to around 102 million. 4.2.1. Baseline Models CTC/Attention: A hybrid CTC/Attention model trained multilin- gually without any intermediate CTC objectives. SC-CTC: A model trained with intermediate self-conditioned CTC [27], as discussed in Sec. 2.2. The intermediate label is identical to the ASR ground truth. We use the same Transformer SC-CTC parameters as [27]: 5 intermediate layers (3, 6, 9, 12, 15) with an intermediate CTC weight of w = 0.5. For the 12-layer Conformer encoder, we use intermediate layers 3,6, and 9. 4.2.2. Proposed Models LIDutt & LIDtok: Models trained with the proposed intermediate tasks that explicitly leverage the LID described in Sec. 3.1. The intermediate layer con\ufb01guration is the same as SC-CTC. In the LIDutt model, all intermediate layers use a single LID token as the output label. For LIDtok, the ground truth is comprised of an LID token for each token in the original utterance. HierLIDutt & HierLIDtok: Our proposed model that incorporates the LID prediction task into a hierarchical setup (Sec. 3.2). The \ufb01rst intermediate layer (layer 3) uses the LID as the CTC objective, while deeper intermediate layers (6,9,12,15) use the ASR text. We report results for both LIDutt and LIDtok as the \ufb01rst objective. 5. RESULTS We report both Character Error Rate (CER) and Mixed Error Rate (MER), along with the language identi\ufb01cation accuracy (LID). MER is calculated using the CER for languages not delimited by white space, and Word Error Rate (WER) for all other languages. Table 2 shows our early experiments with different pre-trained SSL models. While self-conditioning improved the results of both models (A1 vs. A2, B1 vs. B2), XLS-R consistently outperformed WavLM and achieved SOTA performance. This result was apparent in early development, so we did not continue experimentation with WavLM. Table 3 presents our main results in four partitions: 1) prior work, 2) Transformer baselines, 3) Transformers with the proposed methods, and 4) extended studies with Conformers. Our baseline (B1) improves upon previous works (Z1 and Z2) by using XLS-R SSL features [19] with a CTC/Attention architecture. Conditioning on both LID and transcriptions further improves ASR performance (B1 vs B2). Moreover, explicitly conditioning on the LID is more bene\ufb01cial than self-conditioning (B2 vs. C1, C2). Speci\ufb01cally, LIDtok is more effective than LIDutt (C1 v.s. C2); the former even outperforms SC- CTC by 3.0 MER absolute (B2 vs. C2). The addition of hierarchical Table 3. Character error rate (CER), mixed error rate (MER), and language identi\ufb01cation % accuracy (LID) on FLEURS. ID Model Test CER(\u2193) MER(\u2193) LID (\u2191) Prior Work Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] Transformer B1 CTC/Attention B2 +SC-CTC C1 +LIDutt C2 +LIDtok C3 +HierLIDutt C4 +HierLIDtok 14.1 14.6 13.9 13.7 13.6 13.4 13.3 13.3 - 39.7 38.8 37.2 35.8 36.1 36.0 - 95.73 95.39 95.62 95.86 95.43 95.31 Conformer D1 D2 +SC-CTC +HierLIDutt 10.4 10.1 32.9 31.5 95.41 94.92 Table 4. Languages with largest differences in Character Error Rate (CER) (\u2193) between HierLIDutt Conformer and w2v-bert: Georgian (Ka), Cantonese (Yue), Hebrew (He), Swedish (Sv), and Umbundu (Umb). ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC Ka 30.7 31.0 8.0 8.1 Yue 37.0 39.8 15.4 15.3 He 37.2 42.5 18.1 17.0 Oc 11.7 12.7 14.4 17.6 Sv 7.6 7.8 11.7 15.7 Umb 13.1 14.0 23.7 22.4 conditioning, however, shrinks this gap (C3 vs C4). The combination of both LIDutt and SC-CTC improves over solely LIDutt-conditioning by a large degree (C1 vs. C3), suggesting that some amount of token- level conditioning is necessary to take advantage of the technique. We further push ASR performance by applying these methods to the Conformer. All Conformer models outperform their Transformer variants, and HierLIDutt maintains its advantage over SC-CTC (D1 vs. D2). However, due to the increased training instability of the Conformer [44], the other methods do not converge with the same optimization settings. Therefore, due to this difference in training stability and the similar performance of the proposed methods in our Transformer trials, we prefer evaluating HierLIDutt (D2) when training Conformer models. The combination of HierLIDutt and the Conformer yields our best result (D2), which outperforms the CER of previous work in equivalent settings by a wide margin: 4.0 absolute1. 5.1. Analysis To better understand effectiveness of our technique, we conducted an analysis of our results by language. Table 4 compares the best/worst performing languages by HierLIDutt Conformer (D2) relative to w2v- bert (Z1), which"}