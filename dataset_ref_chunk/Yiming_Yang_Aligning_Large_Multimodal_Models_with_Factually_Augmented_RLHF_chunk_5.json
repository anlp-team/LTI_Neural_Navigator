{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Aligning_Large_Multimodal_Models_with_Factually_Augmented_RLHF_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What percentage of cases does GPT-4 agree with human judgments when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B?,answer: 94%", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What are the main evaluation metrics used for MMHAL-BENCH?,answer: LLaVA-Bench and MMHAL-BENCH", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " How many multiple-choice questions are included in the MMBench evaluation framework?,answer: 2,974", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What technique is employed by POPE for evaluating Large Multimodal Model object perception tendencies?,answer: Polling-based query technique", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What improvement is observed in the capabilities of the LLaVA-SFT+ 7B model on MMBench?,answer: 52.1%", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What is the sample efficiency of LLaVA-SFT+ compared to models like Kosmos and Shikra?,answer: Utilizing only 280k fine-tuning data\u2014a 5% fraction", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What scores does LLaVA-RLHF attain on MMHAL-BENCH at the 7b and 13b scales?,answer: 2.05 (7b) and 2.53 (13b)", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What datasets significantly contribute to performance enhancements in the evaluation?,answer: A-OKVQA, Flickr30k, and VQA-v2", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " What are the performance gains from amalgamating A-OKVQA, Flickr30k, and VQA-v2 datasets?,answer: Compounded performance gains across various capability benchmarks", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}, {"question": " In the ablation analysis, what is compared alongside Fact-RLHF?,answer: Standard RLHF", "ref_chunk": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}], "doc_text": "with adequate information from MMHAL-BENCH, GPT-4 can make reasonable decisions aligned with human judgments. For exam- ple, when deciding whether hallucination exists in responses from LLaVA13BX336 and IDEFICS80B, GPT-4 agrees with human judgments in 94% of the cases. Please see the Appendix for the example image-question pairs and GPT-4 prompts we used for MMHAL-BENCH evaluation. 3.3 RESULTS We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation met- rics for their high alignment with human preferences. In addition, we conducted tests on widely- recognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiple- choice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across vary- ing instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. High-quality SFT data is crucial for capability benchmarks. By delving into the specific per- formances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable im- provement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT+) in Ta- bles 4 and 7. LLaVA-SFT+ 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it\u2019s worth noting that LLaVA-SFT+ does 8 Preprint Table 4: CircularEval multi-choice accuracy results on MMBench dev set. We adopt the following abbreviations: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reason- ing; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from Liu et al. (2023b). LLM Data Overall LR AR RR FP-S FP-C CP OpenFlamingo9B MiniGPT-47B LLaMA-Adapter7B Otter-I9B Shikra7B Kosmos-2 InstructBLIP7B IDEFICS9B IDEFICS80B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13B\u00d7336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 5k 52k 2.8M 5.5M 14M 1.2M 1M 1M 1.2M 158k 220k 280k 158k 220k 280k 6.6 24.3 41.2 51.4 58.8 59.2 36.0 48.2 54.6 44.0 38.7 52.1 51.4 47.5 57.5 60.1 4.2 7.5 11.7 32.5 25.8 46.7 14.2 20.8 29.0 19.1 16.7 28.3 24.2 23.3 25.8 29.2 15.4 31.3 35.3 56.7 56.7 55.7 46.3 54.2 67.8 54.2 48.3 63.2 63.2 59.7 65.7 67.2 0.9 4.3 29.6 53.9 58.3 43.5 22.6 33.0 46.5 34.8 30.4 37.4 39.1 31.3 54.8 56.5 8.1 30.3 47.5 46.8 57.2 64.3 37.0 47.8 56.0 47.8 45.5 53.2 50.2 41.4 57.9 60.9 1.4 9.0 38.6 38.6 57.9 49.0 21.4 36.6 48.0 24.8 32.4 35.9 40.0 38.6 51.0 53.8 5.0 35.6 56.4 65.4 75.8 72.5 49.0 67.1 61.9 56.4 40.6 66.8 66.1 65.8 68.5 71.5 trail behind models like Kosmos and Shikra. Despite this, LLaVA-SFT+ stands out in terms of sample efficiency, utilizing only 280k fine-tuning data\u2014a 5% fraction of what\u2019s employed by the aforementioned models. Furthermore, this enhancement isn\u2019t confined to just one model size. When scaled up, LLaVA-SFT+ 13BX336 achieves commendable results, attaining 57.5% on MMBench and 82.9% on POPE. Comparatively, the effect of RLHF on the capability benchmarks is more mixed. LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT+ by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA- RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. RLHF improves human alignment benchmarks further. From another angle, even though high- quality instruction data demonstrates large gains in capability assessment, it does not improve much on human-alignment benchmarks including LLaVA-Bench and MMHAL-BENCH, which is also evident in recent LLM studies (Wang et al., 2023). LLaVA-RLHF show a significant improvement in aligning with human values. It attains scores of 2.05 (7b) and 2.53 (13b) on MMHAL-BENCH and improves LLaVA-SFT+ by over 10% on LLaVA-Bench. We also presented qualitative examples in Table 1, which shows LLaVA-RLHF produces more reliable and helpful outputs. 3.4 ABLATION ANALYSIS We conduct ablation studies on LLaVA7B and evaluate over the four aforementioned benchmarks. 3.5 ABLATION ON HIGH-QUALITY INSTRUCTION-TUNING DATA In Table 5, we evaluate the impact of individual instruction-tuning datasets. For the sake of sim- plicity, we did not adjust the mixture rate, earmarking that consideration for future research. Our findings indicate that A-OKVQA (Schwenk et al., 2022) contributes significantly to performance enhancements, boosting results by +9.8% on MMBench and a more modest +3.8% on POPE. In contrast, VQA-v2 (Goyal et al., 2017a) is particularly influential on POPE, where it leads to a 6% improvement, while only having a slight impact on MMBench. This differential can possibly be attributed to the overlapping \u201cYes/No\u201d format in VQA and the multiple-choice structure of A- OKVQA. Flickr30k notably enhances the performance in LLaVA-Bench and MMHAL-BENCH \u2014 a 9 Preprint Table 5: Abalation studies on methodologies (SFT, RLHF, and Fact-RLHF), data mixtures (LLaVa with additional datasets), and model sizes of the policy model (PM) and the reward model (RM). SFT Data Method SFT SFT SFT SFT SFT PM RM 7b 7b 7b 7b 7b - - - - VQA AOK Flickr \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 MMBench 38.7 42.9 48.5 37.8 52.1 POPE LLaVA-B MMHAL-B 76.0 82.0 79.8 77.6 82.7 81.0 30.4 34.7 46.6 86.3 1.3 2.0 1.1 1.5 1.8 RLHF RLHF RLHF Fact-RLHF 7b 7b 7b 7b 7b 7b 13b 13b \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 40.0 50.8 48.9 51.4 78.2 82.7 82.7 81.5 85.4 87.8 93.4 94.1 1.4 1.8 1.8 2.1 likely consequence of the inherently grounded nature of the task. Furthermore, amalgamating these three datasets results in compounded performance gains across various capability benchmarks. 3.6 ABLATION ON FACT-AUGMENTED RLHF We compare the performance of Fact-Augmented RLHF (Fact-RLHF) with standard RLHF in Ta- ble 5. Our findings indicate that while the conventional RLHF exhibits improvement on"}