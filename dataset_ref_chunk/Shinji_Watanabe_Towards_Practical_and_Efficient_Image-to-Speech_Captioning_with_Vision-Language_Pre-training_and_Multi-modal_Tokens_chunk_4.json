{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Towards_Practical_and_Efficient_Image-to-Speech_Captioning_with_Vision-Language_Pre-training_and_Multi-modal_Tokens_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main advantage of initializing the speech decoder with a pre-trained text decoder?,        answer: The speech decoder can inherit the language model knowledge of the large-scale pre-trained text decoder.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What does the Ablation study in Table 2 confirm regarding vision-language pre-training in image-to-speech captioning?,        answer: It confirms the effectiveness of vision-language pre-training in improving performance.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What does Table 3 compare in terms of performance for image-to-speech captioning on Flickr8k and COCO?,        answer: It compares the performance of image captioning, cascaded models, and other methods.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What are the reasons mentioned for developing an end-to-end Im2Sp model instead of using separate image captioning and text-to-speech systems?,        answer: 1) More than 40% of languages have no writing systems, making text-based model not feasible. 2) It helps reduce inference time and maintenance costs.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " How does the Im2Sp model performance compare to previous state-of-the-art methods according to the evaluation results?,        answer: The Im2Sp model outperforms the previous methods with large gaps in all metrics.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What is highlighted about the performance of the Im2Sp model compared to the popular image captioning system, SAT?,        answer: The Im2Sp model can now catch up with the performance of the text-based system (i.e., SAT).    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What method does the proposed Im2Sp model use to achieve better performance in contrast to previous methods?,        answer: It achieves better performance by using an off-the-shelf ASR model.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What are the subjects asked to rate in the Mean Opinion Score (MOS) tests mentioned in the text?,        answer: The naturalness of the generated speech and how correctly the generated speech describes the input image.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What does the MOS test reveal about the Im2Sp method in terms of generating speech and maintaining quality when using image units?,        answer: It shows a trade-off between descriptiveness and speech quality, but helps reduce data storage and computation costs.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}, {"question": " What key advantage is highlighted in the Im2Sp method by employing image units instead of raw images as inputs for the system?,        answer: It can greatly reduce data size (bits) while still achieving reasonable performances.    ", "ref_chunk": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}], "doc_text": "initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], en- abling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Table 2. Ablation study to confirm the effectiveness of vision-language pre-training in image-to-speech captioning. Vision-Language Pre-training Flickr8k COCO Image Encoder Speech Decoder BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE \u2717 \u2713 \u2713 \u2717 \u2717 \u2713 12.9 17.7 20.6 17.1 20.6 22.0 40.7 45.9 48.4 31.4 45.8 53.6 10.3 14.0 15.8 17.4 20.9 25.9 19.1 21.3 23.8 44.0 46.3 50.4 50.5 64.7 81.1 12.2 15.1 17.5 Table 3. Image-to-speech captioning performance comparisons on Flickr 8k and COCO. We also report the performance of image captioning and cascaded models for analysis purposes. We utilize an off-the-shelf TTS model [44] for measuring the performance of cascaded models. Modality Methods Flickr8k COCO BLEU-4 METEOR ROUGE CIDEr SPICE BLEU-4 METEOR ROUGE CIDEr SPICE SAT [2] 21.3 20.3 24.3 23.9 Image captioning (Image\u2192Text) Ours 30.8 26.9 55.8 93.8 20.0 38.7 29.5 59.1 131.2 23.3 Ours (Image Unit) 23.4 22.0 48.9 63.3 15.4 29.9 25.2 52.8 97.4 18.6 Cascaded (Image\u2192Text & Text\u2192Speech) Ours Ours (Image Unit) 29.1 22.3 26.0 21.3 54.6 48.0 84.9 57.7 18.9 14.6 36.1 28.2 28.4 24.3 57.5 51.6 117.2 87.4 21.9 17.5 Wang et al. [5] 3.5 11.3 23.2 8.0 Image-to-Speech Captioning (Image\u2192Speech) SAT-FT-VQ3 [28] Effendi et al. [29] Ours 12.5 14.8 20.6 14.5 17.4 22.0 39.1 32.9 48.4 24.5 45.8 53.6 9.5 15.8 23.3 25.9 21.2 23.8 47.8 50.4 73.2 81.1 14.9 17.5 Ours (Image Unit) 16.7 19.6 44.2 41.2 13.1 20.1 21.4 46.4 64.0 15.0 Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cas- caded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and text- to-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods [28, 29], we can con- firm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the pre- vious method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the text- based system (i.e., SAT). Please note that the works [28, 29] uti- lized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects Table 4. Mean Opinion Score (MOS) comparisons with 95% confi- dence interval, and Neural MOS scores on COCO. Methods Human Evaluation (MOS) Neural MOS [26, 27] Naturalness Descriptiveness MOSNet \u2191 SpeechLMScore \u2193 SAT-FT-VQ3 [28] Ours 2.870\u00b10.095 4.275\u00b10.086 2.978\u00b10.131 3.968\u00b10.108 4.12 4.26 4.25 4.17 Ours (Image Unit) 4.228\u00b10.089 3.725\u00b10.122 4.33 4.16 are asked to rate the naturalness of the generated speech and how cor- rectly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based met- rics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Ta- ble 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and perfor- mance, similar to [35]. However, we can achieve reasonable perfor- mances by achieving better performances than the previous state-of- the-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computa- tion costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw im- ages as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1] M. Hasegawa-Johnson et al., \u201cImage2speech: Automatically generat- ing audio descriptions of images,\u201d Casablanca 2017, p. 65, 2017. [2] K. Xu et al., \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in"}