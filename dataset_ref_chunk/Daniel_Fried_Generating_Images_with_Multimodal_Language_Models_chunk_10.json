{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Generating_Images_with_Multimodal_Language_Models_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What F1 scores are achieved by the methods: always retrieving, always generating, and deciding randomly?", "answer": " 0.267, 0.389, and 0.451 respectively.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " What method achieves an F1 score range of 0.261 to 0.559 depending on the threshold used?", "answer": " Heuristic method.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " What type of classifier is trained with binary cross-entropy loss over the training set of PartiPrompts annotations?", "answer": " Linear classifier.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " Why is the linear classifier preferred over the heuristic baseline in the final model?", "answer": " It requires less hyperparameter tuning and performs comparably on quantitative metrics.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " What is the F1 score range achieved by the linear classifier depending on the probability threshold used?", "answer": " 0.393 to 0.552.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " According to the text, how does GILL perform compared to Stable Diffusion on PartiPrompts?", "answer": " GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " What score does GILL achieve for BLEU-4 in image captioning on MS-COCO (2017)?", "answer": " 0.1059.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " In the evaluations on VQAv2, what is the zero-shot val accuracy achieved by GILL?", "answer": " 0.3178.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " What does GILL leverage as its backbone?", "answer": " An LLM backbone.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}, {"question": " How does GILL perform when longer text context is provided compared to Stable Diffusion?", "answer": " GILL continues to improve and outperforms Stable Diffusion when 7 or more rounds of dialogue are provided.", "ref_chunk": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}], "doc_text": "an image, always generating an image, or simply deciding randomly (with a prior proportional to class frequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively. 2. Heuristic: We also consider a simple heuristic which considers the maximum cosine similarity of the retrieval embedding against the entire image candidate set (i.e., the training set of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the maximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This achieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives F1 of 0.261). 3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the LLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained with the binary cross-entropy loss over the training set of PartiPrompts annotations. This linear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability threshold used (a threshold of 0.5 gives an F1 score of 0.547). We use the linear classifier in our final model, as it requires less hyperparameter tuning compared to the heuristic baseline, and performs comparably on quantitative metrics. During generation of qualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier generally performed well for many prompts, and decided correctly whether to retrieve or generate. D Qualitative Results We present further qualitative samples in Fig. 7. We find that GILL is able to process complex text prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On VisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against groundtruth images). We attribute these improved results to the stronger text representations of the LLM, and the effectiveness of our GILLMapper network. 16 Table 6: Results on PartiPrompts for classifying retrieval or generation. Method F1 Always retrieve Always generate Random Heuristic Linear classifier Human performance 0.267 0.389 0.451 0.261 \u2013 0.559 0.393 \u2013 0.552 0.851 Table 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning, we report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying the normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020 indicates our reimplementation. Captioning VQA Model Frozen\u2020 [56] MAGMA [19] FROMAGe [31] Ours BLEU-4 METEOR - 0.1023 0.1059 - 0.2873 0.2529 0-shot 0.2553 0.2835 0.2851 0.3178 E Other Evaluations E.1 Comparison to Prior Multimodal LMs We ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are presented in Tab. 7. We found that GILL is comparable to models trained with similar compute and data. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior approaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of 0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on the MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529, which is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also capable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these models. We note that these scores are lower than SOTA models, as they are usually much larger and trained with significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33] uses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter scales to further push its capabilities is an exciting avenue for future work. E.2 Increasing Context on VisDial GILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such as improved sensitivity to long input contexts. In the main paper, we showed that GILL can better condition on longer image and text inputs to generate more relevant images for VIST [28]. We run a similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as input context to GILL and Stable Diffusion (SD) [49]. The results are presented in Fig. 8. We find that when longer text context is provided to both models, the performance of generating relevant images steadily improves. Interestingly, SD performance plateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7 or more rounds of dialogue are provided. These results showcase the improved sensitivity of our model to conditioning on long, dialogue-like text. Despite both approaches using the same image generation backbone, GILL is able to better make use of longer dialogue-text inputs (despite being only finetuned on image caption data). 17 I\u2019m not sure VisDial Inputs VisDial Inputs 9 Q: what color is the snowboard?A: the snowboard is grey in color I'm a big fan \u201cA digni\ufb01ed beaver wearing glasses, a vest, and colorful neck tie. He stands next to a tall stack of books in a library.\u201d Stable Di\ufb00usion Ours Ours Ours Ours Ours Q: what color is the toilet?A: it is white I prefer out\ufb01ts that are more modest Q: is this a single person bathroom?A: yes, it is Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion Stable Di\ufb00usion User prompts Q: is the man alone?A: yes, the man is alone 8 Q: can you see the sky?A: no it's totally dark Q: do they seem like they like each other?A: can't tell Generated \u201ca group of penguins in a snowstorm\u201d 8 Q: can you tell what breed they are?A: i can't really tell what breed they are, perhaps german shepherd Q: is the man wearing a cap?A: the man is wearing a black cap \u201cSnow mountain and tree re\ufb02ection in the lake\u201d Q: are they looking at each other?A: no, they are facing away from each other I think you should wear a headband Q: are they both wearing a hat?A: only 1 is wearing a hat Q: are they standing in grass?A:"}