{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_KIT\u2019s_Multilingual_Speech_Translation_System_for_IWSLT_2023_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the speech translation system described in the text?", "answer": " The focus is on the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " How many languages does the system translate into?", "answer": " The system translates into 10 languages of varying amounts of resources.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " What approach is used for adaptation when training data from the target domain is not available?", "answer": " A retrieval-based approach (kNN-MT) is used for effective adaptation.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " What method is used to integrate incremental training data from data augmentation?", "answer": " Adapters are used to easily integrate incremental training data.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " What is observed about the performance of cascaded systems compared to end-to-end systems?", "answer": " Cascaded systems outperform end-to-end systems on scientific talk translation.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " What is the main challenge on the source side while translating scientific talks?", "answer": " The main challenge is dealing with non-native speakers and varying recording conditions.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " What is a key factor in translating domain-specific terminologies effectively?", "answer": " Accurate translation of terminologies that rarely occur in the training data is crucial.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " Why is effective few-shot or zero-shot adaptation crucial for this translation task?", "answer": " Effective adaptation is crucial as no training data from the same domain is provided.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " What pretrained models are leveraged in the speech translation system?", "answer": " The system leverages pretrained models including WavLM, mBART50, and DeltaLM.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}, {"question": " How is the ASR system adapted to the target domain?", "answer": " The ASR system is adapted using n-gram re-weighting and synthesized data for the target domain.", "ref_chunk": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}], "doc_text": "3 2 0 2 l u J 2 1 ] L C . s c [ 3 v 0 2 3 5 0 . 6 0 3 2 : v i X r a KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues Karlsruhe Institute of Technology firstname.lastname@kit.edu Abstract Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match In this the conditions in real-life use-cases. paper, we describe our speech translation sys- tem for the multilingual track of IWSLT 2023, which evaluates translation quality on scientific conference talks. The test condition features accented input speech and terminology-dense contents. The task requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily in- tegrate incremental training data from data aug- mentation, and show that it matches the perfor- mance of re-training. We observe that cascaded systems are more easily adaptable towards spe- cific target domains, due to their separate mod- ules. Our cascaded speech system substantially outperforms its end-to-end counterpart on sci- entific talk translation, although their perfor- mance remains similar on TED talks.1 1 Introduction This paper summarizes Karlsruhe Institute of Tech- nology\u2019s speech translation system for the multilin- gual track of IWSLT 2023 (Agarwal et al., 2023). In this track, the task is to translate scientific talks in English into 10 languages: Arabic (ar), Chinese (zh), Dutch (nl), French (fr), German (de), Japanese (ja), Persian/Farsi (fa), Portuguese (pt), Russian (ru), Turkish (tr). The talks are from presentations in the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) and com- piled by Salesky et al. (2023). Translating scientific talks presents several chal- lenges. On the source side, most speakers are non-native, and the recording conditions often vary. This requires acoustic robustness to accents and noise. On the target side, domain-specific termi- nologies are frequently used, calling for accurate translation of these words that rarely occur in the training data. The styles of the talks, e.g. formal- ity, also differ from other domains. As no training data from the same domain is provided, effective few-shot or zero-shot adaptation is crucial. As the task focuses on one-to-many translation, it is also an interesting testbed for whether mul- tilinguality improves speech translation quality. For text-to-text translation, the gain from multi- linguality is mostly concentrated in many-to-one translation (Aharoni et al., 2019; Fan et al., 2021), i.e., multilinguality on the source side. In con- trast, for X-to-many translation, it remains unclear whether incorporating more target languages im- proves translation quality. In this system description paper, we present cascaded and end-to-end systems for the English- to-many speech translation task. We lever- age pretrained models, including WavLM (Chen et al., 2022), mBART50 (Tang et al., 2020), and DeltaLM (Ma et al., 2021). The systems do not use additional data beyond the allowed corpora, and therefore fall under the constrained data condition. For the cascaded system, to handle the unique style of scientific talks, we use kNN-MT (Khan- delwal et al., 2021) to bias the output generation towards the target domain. Moreover, as no target monolingual data is provided, we use data diversi- fication (Nguyen et al., 2020) to enrich the existing parallel data. We also use adapters (Rebuffi et al., 2017; Bapna and Firat, 2019) as a lightweight ap- proach for incremental learning and language adap- tation. For the ASR model, we improve over last year\u2019s performance by using a more recent audio encoder (Chen et al., 2022) and adding a dedicated decoder. To adapt the ASR system to the target domain, we use n-gram re-weighting and synthe- sized data for the target domain. For the end-to-end 1Code available at https://github.com/ dannigt/kit-iwslt2023-multilingual system, we use our machine translation model for knowledge distillation. We also ensemble models trained with and without synthesized speech data. Our main findings are as follow: For cascaded ST systems, we can effectively adapt the model towards a target domain/style using kNN-MT (Khandelwal et al., 2021). A datastore as small as a few hundred sentence pairs was sufficient for achieving consistent gains (avg. +0.8 BLEU over 10 languages). Besides the common use-case of adding language-specific capacity, adapters (Bapna and Firat, 2019) is also an effective method when subsequently adding training data. Em- pirically, we show it matches the performance of re-training on all new data. For ASR, lexical constraints for domain adap- ation are more easily integrated in CTC mod- els. For encoder-decoder model, the control could be achieved by TTS-synthesized source speech, but it requires more careful tuning. 2 Data and Preprocessing After describing the evaluation data (\u00a72.1), we out- line the training data and preprocessing steps for our automatic speech recognition (ASR; \u00a72.2), ma- chine translation (MT; \u00a72.3), casing/punctuation restoration (\u00a72.4), and speech translation (ST; \u00a72.5) models. 2.1 Development and Test Data In the multilingual track, the testing condition is scientific conference talks. Therefore, we primarily rely on the ACL development (dev) set (Salesky et al., 2023) for validation. It consists of English transcripts of the talks and translations into the 10 target languages. The systems are then evaluated on a blind test set. The dev and test sets consist of 5 talks each. The paper abstracts for all talks are available in English. The talks are pre-segmented. In all experiments, we use the given segmentation. We also report performance on tst-COMMON of MuST-C (Di Gangi et al., 2019), tst2019 and tst2020 from previous years\u2019 evaluations (Anasta- sopoulos et al., 2021, 2022). An overview of the development and test data is in Table 1. Dev/Test set Hours # Utterances Domain ACL dev tst-COMMON tst2019 tst2020 1.0 4.9 4.8 4.1 468 ACL conference talks TED talks 2823 TED talks 2279 TED talks 1804 Table 1: Overview of development and test"}