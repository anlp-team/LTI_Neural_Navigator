{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_13.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one strategy proposed to handle Noisy Label Learning?", "answer": " One strategy proposed is designing robust loss functions that are resilient to noise.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What are the three broad categories into which techniques for handling Noisy Label Learning can be divided?", "answer": " The three categories are robust loss functions, noise estimation, and noise correction.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What is the key task involved in assuming the noisy label originates from a probability distribution?", "answer": " The key task is to estimate the underlying transition probabilities.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " How does the Co-Teaching approach deal with correcting noisy labels?", "answer": " Co-Teaching uses multiple differently trained networks for correcting noisy labels.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What technique involves generating new labels equivalent to soft or hard pseudo-labels estimated by the model?", "answer": " One technique involves generating new labels equivalent to soft or hard pseudo-labels estimated by the model.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What is the primary objective of Multiple Instance Learning (MIL)?", "answer": " The primary objective of MIL is to predict labels for new, unseen bags.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What is the foundational idea behind frameworks that focus on instance-level prediction in MIL?", "answer": " The foundational idea is the dynamic or static labeling of instances according to the bag label.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What is one approach for dealing with label noise in the context of MIL?", "answer": " One approach involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What does the framework IRNet focus on in the context of Partial Label Learning with noisy labels?", "answer": " IRNet focuses on noisy sample detection and label correction.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}, {"question": " What is the key goal of DALI, as mentioned in the text?", "answer": " The key goal of DALI is to reduce the negative impact of detection errors in label learning.", "ref_chunk": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}], "doc_text": "mixup (Zhang et al., 2017) can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling. Numerous techniques have been proposed to handle Noisy Label Learning (NLL) (Song et al., 2022). They can broadly be divided into three categories: robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020b), noise estimation (Xiao et al., 2015a; Goldberger & Ben-Reuven, 2016; Liu et al., 2020; Zhang et al., 2021d; Northcutt et al., 2021), and noise correction (Han et al., 2018; Li et al., 2020; Liu et al., 2022). Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The l1 loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions (Zhang & Sabuncu, 2018; Wang et al., 2019c; Ma et al., 2020a; Yu et al., 2020). Additionally, methods that re-weight loss (Liu & Tao, 2016) have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works (Goldberger & Ben-Reuven, 2016) incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward (Patrini et al., 2016), prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data (Northcutt et al., 2021) or additional assumptions about the data (Zhang et al., 2021e). Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples (Liu et al., 2020). This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model (Tanaka et al., 2018; Yi & Wu, 2019). Co-Teaching uses multiple differently trained networks for correcting noisy labels (Han et al., 2018). SELFIE (Song et al., 2019) corrects a subset of labels by replacing them based on past model outputs. Another study in Arazo et al. (2019) uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup (Zhang et al., 2017) to enhance performance. Similarly, DivideMix (Li et al., 2020) employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch (Berthelot et al., 2019b). 21 Preprint B.4 MULTI-INSTANCE LEARNING Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of \u2019bags\u2019, each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags. Dietterich et al. (1997) pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem. One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in (Foulds & Frank, 2010). Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework. Following this development, various efforts (Tao et al., 2004a;b) focused on carrying out bag-level predictions based on count-based assumptions. Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by Maron & Lozano-P\u00e9rez (1997) is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label. Empirical studies, as detailed in (Vanwinckelen et al., 2016), have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction. To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations (Zhang et al., 2021a). B.5 MIXED IMPRECISE LABEL LEARNING Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ (Wang et al., 2022b), an extended version of PiCO (Wang et al., 2022a), is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features. IRNet (Lian et al., 2022b) is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI (Xu et al., 2023) is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang,"}