{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Unlimiformer:_Long-Range_Transformers_with_Unlimited_Length_Input_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main idea of the Unlimiformer approach?", "answer": " The Unlimiformer approach allows for processing unlimited input sequences by offloading cross-attention computation to a single k-nearest-neighbor (kNN) index.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " What is the purpose of Unlimiformer in the context of transformers?", "answer": " Unlimiformer aims to extend the capabilities of existing pretrained encoder-decoder transformers to accept inputs of unbounded length at test time.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " How does Unlimiformer enable processing input sequences of up to 500k tokens?", "answer": " Unlimiformer achieves this by allowing every attention head in every decoder layer to retrieve its top-k keys from a k-nearest-neighbor (kNN) index, rather than attending to every key.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " What are some tasks for which the Unlimiformer approach is particularly useful?", "answer": " Unlimiformer is especially useful for tasks involving long narratives, such as book summarization, where inputs can exceed 500k tokens.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " How does Unlimiformer differ from other approaches like Longformer-Encoder-Decoder (LED)?", "answer": " Unlimiformer does not require further training of position embeddings or global attention weights, making it a more computationally efficient solution for handling long inputs.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " What kind of datasets can benefit from the capabilities of Unlimiformer?", "answer": " Datasets with inputs that are many times longer than the maximum input length of traditional models can benefit from Unlimiformer.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " In Unlimiformer, how are cross-attention heads able to attend to tokens from the entire input sequence?", "answer": " By performing a k-nearest-neighbor (kNN) search to select a set of per-attention-head tokens to attend to from the full-length input.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " What is the primary advantage of using Unlimiformer when fine-tuning transformer models?", "answer": " Unlimiformer not only outperforms strong long-range transformers like LED and PRIMERA but can also be applied on top of such models to further improve their performance.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " How does Unlimiformer handle attention computation for long input sequences?", "answer": " Unlimiformer uses a k-nearest-neighbor (kNN) index to allow the model to attend only to the top-k keys, which covers more than 99% of the attention mass.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}, {"question": " What type of memory can the kNN index of Unlimiformer be stored in, and how efficiently can it be queried?", "answer": " The kNN index can be stored in either GPU or CPU memory and can be queried in sub-linear time, making it a memory-efficient solution.", "ref_chunk": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}], "doc_text": "3 2 0 2 t c O 0 3 ] L C . s c [ 3 v 5 2 6 1 0 . 5 0 3 2 : v i X r a Unlimiformer: Long-Range Transformers with Unlimited Length Input Amanda Bertsch Uri Alon\u2217 Graham Neubig Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig,mgormley}@cs.cmu.edu Abstract Since the proposal of transformers (Vaswani et al., 2017), these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020) by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available, and support LLaMA-2 as well2. 1 Introduction Transformers (Vaswani et al., 2017) have risen as the dominant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019), T5 (Raffel et al., 2020)) or 1024 tokens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016). To address inputs between 1024 and 16,384 tokens, specialized long-context models sparsify or approximate attention (e.g. Longformer (Beltagy et al., 2020), Performers (Choromanski et al., 2020)), allowing the maximum input length to quadruple while remaining computationally feasible. Most long-document summarization and question-answering datasets, such as SCROLLS (Shaham et al., 2022), are included in this range. Yet tasks that involve long narratives, such as book summarization (Kry\u00b4sci\u00b4nski et al., 2021), can con- tain inputs exceeding 500k tokens. Figure 1 shows the input lengths of several popular summarization and question-answering datasets, plotted against common context window lengths; the longest inputs are more than 34 times longer than Longformer\u2019s context window. In these extremely-long-input cases, vanilla transformers cannot be simply scaled, as na\u00efve self- attention has quadratic complexity. Long-input transformers usually modify the base architecture, and thus necessitate re-pre-training the model from scratch, which requires significant computational \u2217Now at Google DeepMind 2https://github.com/abertsch72/unlimiformer 37th Conference on Neural Information Processing Systems (NeurIPS 2023). c Decoder Layer d def Cross attention abcdef bcd querykNN Search Input: c Encoder e Encode chunks Retrieved hidden states Index of one long input ab Figure 2: In this example, a given LM\u2019s encoder\u2019s maximum input length is 2 tokens. A 6-token input is encoded in chunks and indexed in an index. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlimiformer, we perform kNN search to select a 2-token context for each attention head from the index. This makes cross-attention attend to tokens from the entire input sequence, without adding parameters and without changing the given LM\u2019s architecture. resources. Other architectures such as Longformer-Encoder-Decoder (LED; Beltagy et al., 2020) can leverage pretrained models, but they still need to further train new position embeddings or global attention weights, which is computationally and environmentally costly. We introduce Unlimiformer, a retrieval-based approach to augment pretrained language mod- els to accept inputs of unbounded length at test time. Given a long input sequence, Unlimi- former constructs a k-nearest-neighbor (kNN) index over the hidden states of all input tokens. Then, every standard cross-attention head in ev- ery decoder layer queries the kNN index, such that the kNN distances are the attention dot- product scores, and attends only to the top-k input tokens. In preliminary experiments, we found that the top-k attention keys cover more than 99% of the attention mass, and thus attend- ing only to the top-k keys is an accurate approx- imation of the full, exact, attention. Unlimi- former can be injected into any existing encoder- decoder transformer to permit unbounded inputs. The index can be stored in either GPU or CPU memory, needs to hold only a single vector per input token, and can be queried in sublinear time. Unlimiformer is illustrated in Figure 2. 104 GovReport (Avg) WikiSum (Avg) NarrativeQA (Max) 105 16384 tokens XSum (Avg) BookSum (Max) CNN/DM (Avg) 4096 tokens WikiSum (Max) BookSum (Avg) 103 1024 tokens ArXiv (Avg) Input tokens NarrativeQA (Avg) Figure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models\u2019 maximum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022). Unlimiformer is a generic approach: it can be applied to trained models and improve existing checkpoints without adding weights and without further training. When finetuning Unlimiformer, performance is even further improved: across a variety of long-range datasets, not only that Unlimiformer performs better than strong long-range transformers such as LED (Beltagy et al., 2020), PRIMERA (Xiao et al., 2022), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but Unlimiformer can be applied on top of such models to further improve them. 2 Unlimiformer Given a trained encoder-decoder transformer, Unlimiformer allows each cross-attention head to choose separate keys to attend to from the full-length input, at each decoding step. We inject a kNN 2 search into each decoder layer: prior to cross-attention, the model performs a nearest-neighbor search in a kNN index to choose a set of per-decoder-layer per-attention-head tokens to attend to. 2.1 Encoding"}