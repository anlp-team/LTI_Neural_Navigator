{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Best-Case_Retrieval_Evaluation:_Improving_the_Sensitivity_of_Reciprocal_Rank_with_Lexicographic_Precision_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the evaluation method lexicographic precision (lexiprecision) introduced in the text?", "answer": " The main focus of lexiprecision is to improve the sensitivity of reciprocal rank in measuring the quality of rankings for users interested in exactly one relevant item.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " According to the text, in what year did Cooper propose the Type 1 expected search length (ESL1) as a metric for ranking systems?", "answer": " Cooper proposed ESL1 in 1968.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " What does RL1 stand for in the context of ranking systems evaluation?", "answer": " RL1 stands for recall level 1.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " What issue does the text address regarding the use of reciprocal rank in evaluating ranking systems?", "answer": " The text addresses the issue of reciprocal rank being brittle when discriminating between ranking systems, especially in modern evaluation settings with high-precision systems.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " How does lexiprecision address the lack of sensitivity of reciprocal rank?", "answer": " Lexiprecision preserves differences detected by reciprocal rank while empirically improving sensitivity and robustness in a broad set of retrieval and recommendation tasks.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " What is the significance of lexiprecision over reciprocal rank in terms of tie-breaking between ranking systems?", "answer": " Lexiprecision provides a theoretically-justified ordering when RL1 is tied, preserving all strict orders and avoiding numerous ties observed with reciprocal rank.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " What is the purpose of Hasse diagrams in the comparison between lexiprecision and reciprocal rank orderings?", "answer": " Hasse diagrams are used to compare the total order of lexiprecision with the partial order of reciprocal rank, showing how lexiprecision scales with the number of relevant items and reduces tied performance.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " How does the paper contribute to the theoretical understanding of evaluation in the field of information retrieval?", "answer": " The paper contributes by providing a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision, demonstrating how lexiprecision improves discriminative power while being strongly correlated with RL1 metrics.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " What is the observation regarding ceiling effects in RL1 evaluation according to the text?", "answer": " The observation is that ceiling effects are inherent in RL1 evaluation due to standard ranking problems and the symmetric group over n elements in predicting relevance.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}, {"question": " What is the mathematical definition of reciprocal rank (RR1) provided in the text?", "answer": " Reciprocal rank is defined as RR1(\ud835\udf0b) = 1/p1, where p1 is the position of the first highest-ranked relevant item in a ranking \ud835\udf0b.", "ref_chunk": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}], "doc_text": "3 2 0 2 n u J 3 1 ] R I . s c [ 1 v 8 0 9 7 0 . 6 0 3 2 : v i X r a Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Fernando Diaz Google Montr\u00e9al, QC, Canada diazf@acm.org ABSTRACT Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation set- tings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall require- ments. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic pre- cision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks. 1 INTRODUCTION Evaluating ranking systems for users seeking exactly one relevant item has a long history in information retrieval. As early as 1968, Cooper [7] proposed Type 1 expected search length or ESL1, defined as the rank position of the highest ranked relevant item. In the context of TREC-5, Kantor and Voorhees [12] proposed using the reciprocal of ESL1 in order to emphasize rank changes at the top of the ranked list and modeling the impatience of a searcher as they need to scan for a single item. Over the years, reciprocal rank (and less so ESL1) has established itself as a core metric for retrieval [5] and recommendation [4], adopted in situations where there is actually only one relevant item as well as in situations where there are multiple relevant items. Given two rankings, reciprocal rank and ESL1 always agree in terms of which ranking is better. Because of this, we refer to them collectively as the recall level 1 or RL1 metrics. Despite the widespread use of reciprocal rank, recent evidence suggests that it may brittle when it comes to discriminating between ranking systems [10, 19, 20]. In particular, the low number of unique values of reciprocal rank means that, especially when evaluating multiple highly-performing systems, we are likely to observe tied performance. Voorhees et al. [21] demonstrate that these conditions exist in many modern deep learning benchmarks. We address these issues by theoretically interpreting RL1 as a population-level metric we refer to as best-case retrieval evaluation. This allows us to propose a generalization of the RL1 ordering based on social choice theory [17] and preference-based evaluation [8]. This evaluation method, lexicographic precision or lexiprecision, RL1lexicographicprecision (1,2,3,4,5)(1,2,3,4,6)(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)\u2026\u2026 (1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)\u2026 Figure 1: Hasse diagram of possible positions of relevant items. Each tuple represents the possible positions of five relevant items in a corpus of \ud835\udc5b items. RL1 metrics such as reciprocal rank (left) have \ud835\udc5b \u2212 4 unique values and, therefore, result in a partial order over all possible positions of relevant items. Lexicographic precision (right) is a total order over all possible positions of relevant items that preserves all strict orders in RL1 evaluation. preserves any strict ordering between rankings based on RL1 while also providing a theoretically-justified ordering when RL1 is tied. We compare lexiprecision and RL1 orderings using Hasse dia- grams in Figure 1. On the left, we show the partial order of all possible positions of five relevant items in a corpus of size \ud835\udc5b. Since reciprocal rank and ESL1 only consider the position of the first relevant item, we only have \ud835\udc5b different relevance levels. While this may not be an issue in general (since \ud835\udc5b is usually large), the num- ber of rankings within each level can be very large and multiple highly effective systems can result in numerous ties. In contrast, lexiprecision has one relevance level for each unique arrangement of relevant items. That is, the number of relevance levels scales with the number relevant items and, by design, two rankings are tied only if they place relevant items in exactly the same positions. In this paper, we contribute to the theoretical understanding of evaluation through a detailed study of RL1 metrics, best-case retrieval evaluation, and lexiprecision. In Section 2, we motivate our work by showing that RL1 has fundamental theoretical limits, especially in situations where there are multiple relevant items. In Section 3, we demonstrate that RL1 can be interpreted as best-case retrieval evaluation, allowing us to to address its limitations by using methods from social choice theory and generalizing it as lexiprecision. In Section 5, we then conduct extensive empirical analysis to show that lexiprecision is strongly correlated with RL1 metrics while substantially improving its discriminative power.1 2 MOTIVATION Our work is based on the observation that ceiling effects are inher- ent in RL1 evaluation. Assume a standard ranking problem where, given a query with \ud835\udc5a associated relevant items, a system orders all \ud835\udc5b documents in the collection in decreasing order of predicted relevance. The set of all possible rankings of \ud835\udc5b is referred to as the symmetric group over \ud835\udc5b elements and is represented as \ud835\udc46\ud835\udc5b. For a given ranking \ud835\udf0b \u2208 \ud835\udc46\ud835\udc5b, let \ud835\udc5d\ud835\udc56 be the position of the \ud835\udc56th highest-ranked relevant item. We can then define reciprocal rank as RR1 (\ud835\udf0b) = 1 . \ud835\udc5d1 When no relevant document is retrieved (e.g. if no relevant items are in the system\u2019s top \ud835\udc58 retrieval), we set RR1 = 0. For two rank- ings, we define \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) = RR1 (\ud835\udf0b) \u2212RR1 (\ud835\udf0b \u2032). For the remainder of this section, we will use reciprocal rank for clarity although the analysis applies to ESL1 as well. Although we can easily see that there are \ud835\udc5b different values for RR1 (\ud835\udf0b), we are interested in the distribution of"}