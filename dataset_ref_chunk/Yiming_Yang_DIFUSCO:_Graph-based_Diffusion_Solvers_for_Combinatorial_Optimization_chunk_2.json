{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_DIFUSCO:_Graph-based_Diffusion_Solvers_for_Combinatorial_Optimization_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the experimental results show about DIFUSCO in comparison to previous probabilistic NPC solvers?", "answer": " They show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " What type of models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization problems?", "answer": " Autoregressive models.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " What challenges do autoregressive models face for large-scale NPC problems?", "answer": " They face high time and space complexity challenges due to their sequential generation scheme and quadratic complexity in the self-attention mechanism.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " How do non-autoregressive constructive heuristics solvers address scalability issues?", "answer": " They assume conditional independence among variables in NPC problems.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " What does DIFUSCO use to enhance its expressive power compared to previous non-autoregressive methods?", "answer": " It uses an iterative denoising scheme to generate the final heatmap.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " What is the main process of typical diffusion models in the continuous domain?", "answer": " They progressively add Gaussian noise to the clean data in the forward process and learn to remove noises in the reverse process.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " How is the optimization objective defined for finding the optimal solution in a CO problem instance?", "answer": " The optimization objective is to find the optimal solution xs\u2217 for a given instance s by minimizing the objective function cs(x) = cost(x, s) + valid(x, s).", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " For the Traveling Salesman Problem (TSP), what is x in the optimization objective equation?", "answer": " x is the indicator vector for selecting a subset from N edges.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " How do probabilistic neural NPC solvers tackle instance problem s?", "answer": " They define a parameterized conditional distribution p\u03b8(x|s) such that the expected cost cs(x) * p(x|s) is minimized.", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}, {"question": " What is the joint distribution in diffusion models that is the learned reverse process?", "answer": " The joint distribution p\u03b8(x0:T) = p(xT) * \u03a0 from t=1 to T p\u03b8(xt-1|xt).", "ref_chunk": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}], "doc_text": "Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes. 2 Related Work 2.1 Autoregressive Construction Heuristics Solvers Autoregressive models have achieved state-of-the-art results as constructive heuristic solvers for combinatorial optimization (CO) problems, following the recent success of language modeling in the text generation domain [106, 11]. The first approach proposed by Bello et al. [6] uses a neural network with reinforcement learning to append one new variable to the partial solution at each decoding step until a complete solution is generated. However, autoregressive models [64] face high time and space complexity challenges for large-scale NPC problems due to their sequential generation scheme and quadratic complexity in the self-attention mechanism [106]. 2.2 Non-autoregressive Construction Heuristics Solvers Non-autoregressive (or heatmap) constructive heuristics solvers [53, 27, 28, 92] are recently proposed to address this scalability issue by assuming conditional independence among variables in NPC problems, but this assumption limits the ability to capture the multimodal nature [57, 33] of high- 2 quality solution distributions. Therefore, additional active search [6, 92] or Monte-Carlo Tree Search (MCTS) [27, 98] are needed to further improve the expressive power of the non-autoregressive scheme. DIFUSCO can be regarded as a member in the non-autoregressive constructive heuristics category and thus can be benefited from heatmap search techniques such as MCTS. But DIFUSCO uses an iterative denoising scheme to generate the final heatmap, which significantly enhances its expressive power compared to previous non-autoregressive methods. 2.3 Diffusion Models for Discrete Data Typical diffusion models [100, 102, 40, 103, 85, 56] operate in the continuous domain, progressively adding Gaussian noise to the clean data in the forward process, and learning to remove noises in the reverse process in a discrete-time framework. Discrete diffusion models have been proposed for the generation of discrete image bits or texts using binomial noises [100] and multinomial/categorical noises [5, 44]. Recent research has also shown the potential of discrete diffusion models in sound generation [118], protein structure generation [77], molecule generation [108], and better text generation [52, 38]. Another line of work studies diffusion models for discrete data by applying continuous diffusion models with Gaussian noise on the embedding space of discrete data [30, 69, 24], the {\u22121.0, 1.0} real-number vector space [16], and the simplex space [37]. The most relevant work might be Niu et al. [86], which proposed a continuous score-based generative framework for graphs, but they only evaluated simple non-NP-hard CO tasks such as Shortest Path and Maximum Spanning Tree. 3 DIFUSCO: Proposed Approach 3.1 Problem Definition Following a conventional notation [88], we define Xs = {0, 1}N as the space of candidate solutions {x} for a CO problem instance s, and cs : Xs \u2192 R as the objective function for solution x \u2208 Xs: cs(x) = cost(x, s) + valid(x, s). (1) Here cost(\u00b7) is the task-specific cost of a candidate solution (e.g., the tour length in TSP), which is often a simple linear function of x in most NP-complete problems, and valid(\u00b7) is the validation term that returns 0 for a feasible solution and +\u221e for an invalid one. The optimization objective is to find the optimal solution xs\u2217 for a given instance s as: xs\u2217 = argmin cs(x). x\u2208Xs This framework is generically applicable to different NPC problems. For example, for the Traveling Salesman Problem (TSP), x \u2208 {0, 1}N is the indicator vector for selecting a subset from N edges; the cost of this subset is calculated as: costTSP(x, s) = (cid:80) denotes the weight of the i-th edge in problem instance s, and the valid(\u00b7) part of Formula (1) ensures that x is a tour that visits each node exactly once and returns to the starting node at the end. For the Maximal Independent Set (MIS) problem, x \u2208 {0, 1}N is the indicator vector for selecting a subset from N nodes; the cost of the subset is calculated as: costMIS(x, s) = (cid:80) i(1 \u2212 xi),, and the corresponding valid(\u00b7) validates x is an independent set where each node in the set has no connection to any other node in the set. i xi \u00b7 d(s) , where d(s) i i Probabilistic neural NPC solvers [6] tackle instance problem s by defining a parameterized condi- tional distribution p\u03b8(x|s), such that the expected cost (cid:80) cs(x) \u00b7 p(x|s) is minimized. Such probabilistic generative models are usually optimized by reinforcement learning algorithms [111, 63]. In this paper, assuming the optimal (or high-quality) solution x\u2217 s is available for each training in- stance s, we optimize the model through supervised learning. Let S = {si}N 1 be independently and identically distributed (IID) training samples for a type of NPC problem, we aim to maximize the likelihood of optimal (or high-quality) solutions, where the loss function L is defined as: x\u2208Xs L(\u03b8) = Es\u2208S [\u2212 log p\u03b8(xs\u2217|s)] (3) Next, we describe how to use diffusion models to parameterize the generative distribution p\u03b8. For brevity, we omit the conditional notations of s and denote xs\u2217 as x0 as a convention in diffusion models for all formulas in the the rest of the paper. 3 (2) 3.2 Diffusion Models in DIFUSCO From the variational inference perspective [60], diffusion models [100, 40, 102] are latent variable models of the form p\u03b8(x0) := (cid:82) p\u03b8(x0:T )dx1:T , where x1, . . . , xT are latents of the same dimen- sionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) = p(xT ) (cid:81)T t=1 p\u03b8(xt\u22121|xt) is the learned reverse (denoising) process that gradually denoises the latent variables toward the data distribution, while the forward process q(x1:T |x0) = (cid:81)T t=1 q(xt|xt\u22121) gradually corrupts the data into noised latent variables. Training is performed by optimizing the usual variational bound on negative log-likelihood: E [\u2212 log p\u03b8(x0)] \u2264 Eq (cid:20) \u2212 log p\u03b8(x0:T ) q\u03b8(x1:T |x0) (cid:21) = Eq (cid:20)(cid:88) (cid:21) DKL[q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)] \u2212 log p\u03b8(x0|x1) + C t>1 where C is a constant. Discrete Diffusion In discrete diffusion"}