{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_14.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the proposed process in the studies mentioned in the text?,answer: An iterative label propagation process is proposed, operating between partially labeled examples and unlabeled instances.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " Why do the mentioned methods not scale well to larger datasets like CIFAR-10?,answer: The methods don\u2019t scale well due to the challenge of handling a combination of partial, limited, and noisy labels simultaneously.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " What is the significance of the study and the proposed framework according to the text?,answer: The study and framework hold significance as there is no existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " What does Figure 3 illustrate?,answer: Figure 3 illustrates different imprecise label configurations on 4 instances with 3 classes using edge and circle notations.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " What is ELBO and its role in the EM framework?,answer: ELBO, or Evidence lower bound, is the core quantity in EM, representing the KL divergence between two probability distributions.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " In the mixed imprecise label learning setting, what happens to the supervised and unsupervised objectives on labeled and unlabeled data?,answer: On labeled data, the supervised objective involves noisy partial labels and unsupervised objective is consistent with noise transition model. On unlabeled data, it follows unsupervised consistency regularization.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " What is the purpose of the forward-backward algorithm on the NFA of imprecise labels?,answer: The algorithm provides an alternative explanation for the EM framework by generating soft-targets through a linear complexity process.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " Why is the NFA necessary in scenarios with limited information?,answer: The NFA reduces complexity to linear by considering all possible labeling paths in scenarios with limited information.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " How does the text suggest computing the posterior probability P(Y|X, I; \u03b8)?,answer: It suggests using the forward-backward algorithm on the NFA to compute the joint probability of I and the latent ground truth label.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}, {"question": " What approach does the text recommend to calculate P(yi = y, I|X; \u03b8t) efficiently?,answer: The text recommends using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC), to compute it in linear time complexity.", "ref_chunk": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}], "doc_text": "2020). An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don\u2019t scale well to larger datasets like CIFAR-10. Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning. 22 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc80 \ud835\udc99\ud835\udfce \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1\ud835\udc7a \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1$\ud835\udc80 \ud835\udc80\ud835\udc73\ud835\udc99\ud835\udfce\ud835\udc8d\ud835\udc99\ud835\udfcf\ud835\udc8d\ud835\udc99\ud835\udfd0\ud835\udc96\ud835\udc99\ud835\udfd1\ud835\udc96 \ud835\udc99\ud835\udfce Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 3: Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol y and circle to denote the finite state x. Different label configurations I correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths. (a) Full label, the transition paths are definite Y . (b) Partial label, the transition paths follow candidate symbols S. (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. C METHODS C.1 DERIVATION OF VARIATIONAL LOWER BOUND Evidence lower bound (ELBO), or equivalently variational lower bound (Dempster et al., 1977), is the core quantity in EM. From Eq. (5), to model log P (X, I; \u03b8), we have: (cid:90) log P (X, I; \u03b8) = Q(Y ) log P (X, I; \u03b8)dY = (cid:90) Q(Y ) log P (X, I; \u03b8) P (Y |X, I; \u03b8) P (Y |X, I; \u03b8) dY = = (cid:90) (cid:90) Q(Y ) log Q(Y ) log P (X, I, Y ; \u03b8)Q(Y ) P (Y |X, I; \u03b8)Q(Y ) P (X, I, Y ; \u03b8) Q(Y ) dY \u2212 dY (cid:90) Q(Y ) log P (Y |X, I; \u03b8) Q(Y ) dY (10) where the KL divergence DKL(Q(Y )||P (Y |X, I; \u03b8)). Replacing Q(Y ) with P (Y |X, I; \u03b8t) at each iteration will ob- tain Eq. (5). the first term is the ELBO and the second term is C.2 INSTANTIATIONS MIXED IMPRECISE LABEL LEARNING In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \u02c6s. For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels: LCE (cid:0)p (cid:0)y | As(x), \u02c6s; \u03b8, \u03c9t(cid:1) , p (cid:0)y | Aw(x), \u02c6y; \u03b8t, \u03c9t(cid:1)(cid:1) + LCE (p (\u02c6y | Aw(x); \u03b8, \u03c9) , \u02c6s) (11) We can compute both quantity through the noise transition model: p(y|x, \u02c6s; \u03b8, \u03c9t) \u221d p(y|x; \u03b8) (cid:89) T (y|\u02c6y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). (12) \u02c6y\u2208\u02c6s y\u2208[C] C.3 FORWARD-BACKWARD ALGORITHM ON THE NFA OF IMPRECISE LABELS This section provides an alternative explanation for the EM framework proposed in the main paper. We treat the problem of assigning labels Y to inputs X as generating a sequence of symbols Y , such that I is satisfied. Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001). The NFA of imprecise labels can be formally defined with a finite set of states X, a finite set of symbols Y , a transition function f \u25e6 g, a start state of x0, and the accepting state of I, determining the available transition paths in the NFA, as shown in Fig. 3. We demonstrate that, Although we can directly derive the 23 Preprint soft-targets utilized in loss functions of each setting we study from Eq. (5), they can also be computed by performing a forward-backward algorithm on the NFA. While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to O(1) for computing the posterior without any statistical information imposed from I. To compute the posterior probability P (Y |X, I; \u03b8), without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation- invariant sequence. We further assume that each yi is conditionally independent given the whole sequence of training samples X, which allows us to re-write Eq. (5) as: \u03b8t+1 = arg max (cid:88) (cid:88) P (yi = y|X, I; \u03b8t) [log P (yi = y|X; \u03b8) + log P (I|X, Y ; \u03b8)] . \u03b8 i\u2208[N ] y\u2208[C] The problem of computing the posterior then reduces to calculate the joint probability of I and the latent ground truth label of the i-th sample taking the value y given the whole input sequence X: P (yi = y|X, I; \u03b8t) = P (yi = y, I|X; \u03b8t) P (I|X; \u03b8t) . Subsequently, P (yi = y, I|X; \u03b8t) can be computed in linear time complexity O(N C) at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) (Graves et al., 2006). More specifically, for any symbol yi in the NFA of imprecise label information, the joint is calculated with the"}