{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Comparative_Study_on_E-Branchformer_vs_Conformer_in_Speech_Recognition,_Translation,_and_Understanding_Tasks_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the BLEU score for E-Branchformer on the Callhome dataset compared to Conformer?,answer: 21.9 vs. 21.2", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " Which dataset contains single-turn user interactions with a home assistant?,answer: SLURP", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " How are SLU tasks formulated in the experiments?,answer: As seq2seq problems", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " What optimizer is employed for training?,answer: Adam", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " What does the SSL frontend stand for in the experiments?,answer: Single-Source Learning frontend", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " How many times is training done for low-resource SLUE datasets with different random seeds?,answer: 3 times", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " What is the main conclusion about E-Branchformer in the study?,answer: It outperforms Conformer in various speech processing tasks", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " What is the purpose of releasing training configurations and pre-trained models?,answer: To ensure full reproducibility", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " What is the source of support for the work mentioned in the Acknowledgements section?,answer: National Science Foundation grants", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}, {"question": " What are some future research directions mentioned in the Conclusion section?,answer: Evaluating E-Branchformer on more diverse datasets in low-resource languages and noisy environments", "ref_chunk": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}], "doc_text": "is employed. 4.2. Results Table 5 shows the ST results. E-Branchformer achieves a higher BLEU score than Conformer on Callhome (21.9 vs. 21.2) and also shows minor improvements on MuST-C and Fisher with a slightly smaller model size. This suggests that E-Branchformer is capable of handling non-monotonic sequence transductions such as translation where source-to-target word re-ordering may occur. 5. Speech understanding experiments 5.1. Setups Data. Three SLU datasets are used. SLURP [53] is a multi- domain corpus for intent classi\ufb01cation and entity recognition, which contains single-turn user interactions with a home assis- tant. SLUE [54] is a low-resource benchmark containing natu- rally produced speech for named entity recognition (NER) and sentiment analysis. STOP [55] is a large-scale corpus for spo- ken task-oriented semantic parsing. Models. As in ESPnet-SLU [19], SLU tasks are formulated as seq2seq problems. The input is a sequence of speech features, and the output is a sequence of text tokens including special SLU labels. Then, the same E2E ASR models can be applied. Speci\ufb01cally, we employ AED models with joint CTC [31, 32], where the decoder is a 6-layer Transformer. Similar to [56], an SSL frontend is used for SLUE and STOP. Training. We follow the ESPnet2 recipes for data prepara- tion, training and decoding. Speed perturbation and SpecAug- ment [36] are performed for data augmentation. The Adam [37] optimizer with warmup [7] is employed. Table 6: Spoken language understanding results on test sets. SLURP shows intent classi\ufb01cation accuracy (%) and SLU-F1 (%). SLUE-voxpopuli shows micro and macro F1 (%). SLUE- voxceleb shows macro F1 (%). STOP shows exact match accu- racy (%). \u2020 means a frozen SSL frontend is used but not counted. Dataset Conformer E-Branchformer Params Results \u2191 Params Results \u2191 SLURP [53] SLUE-voxpopuli [54] SLUE-voxceleb [54] STOP [55] 109.4 \u2020 32.4 \u2020 32.4 \u2020 114.3 86.5 / 76.9 68.6 / 55.8 38.5 73.2 110.2 \u2020 33.5 \u2020 33.5 \u2020 146.8 87.4 / 77.6 68.7 / 55.9 38.1 74.0 5.2. Results Table 6 shows SLU results. E-Branchformer has superior per- formance over Conformer on SLURP with a similar model size. It also achieves a higher accuracy on STOP, but the model size is larger due to the reuse of con\ufb01guration from LibriSpeech 960h. For low-resource SLUE-voxpopuli (NER) and SLUE-voxceleb (sentiment analysis) corpora, training is done 3 times with dif- ferent random seeds, and metrics are averaged. We observe that E-Branchformer is slightly better on SLUE-voxpopuli but worse on SLUE-voxceleb. This indicates that the frozen SSL frontend might be more important than the additional encoder layers in low-resource SLU tasks. 6. Conclusion This work investigates the effectiveness of E-Branchformer in various speech processing tasks, including ASR, ST, and SLU, and compares it to Conformer using different E2E frameworks. Our extensive experiments in publicly available benchmarks have shown that E-Branchformer outperforms Conformer in a wide variety of tasks, and can be more stable when training with large model size or on small datasets. In addition, we share var- ious training tips and we will release our training con\ufb01gurations and pre-trained models to ensure full reproducibility, which can greatly bene\ufb01t the speech community. Future research direc- tions would include evaluating E-Branchformer on more di- verse and challenging datasets in low-resource languages and noisy environments. Additionally, the E-Branchformer archi- tecture may also improve the performance of SSL models such as WavLM [57] and data2vec 2.0 [58]. 7. Acknowledgements This work used PSC Bridges2 and NCSA Delta through alloca- tion CIS210014 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 8. References [1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. ICASSP, 2016. [2] A. Zeyer et al., \u201cImproved Training of End-to-end Attention Mod- els for Speech Recognition,\u201d in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., \u201cState-of-the-art speech recogni- tion with sequence-to-sequence models,\u201d in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., \u201cImprovements to Deep Con- volutional Neural Networks for LVCSR,\u201d in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., \u201cJasper: An End-to-End Convolutional Neural Acoustic Model,\u201d in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., \u201cContextNet: Improving Con- volutional Neural Networks for Automatic Speech Recognition with Global Context,\u201d in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,\u201d in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., \u201cRecent developments on espnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021. [12] Y. Peng et al., \u201cBranchformer: Parallel MLP-attention architec- tures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., \u201cE-Branchformer: Branchformer with En- hanced Merging for Speech Recognition,\u201d in Proc. SLT, 2022. [14] V. Panayotov et al., \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015. [15] A. Graves et al., \u201cConnectionist temporal classi\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [16] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech, 2018. [18] H. Inaguma et al., \u201cESPnet-ST: All-in-one speech translation toolkit,\u201d in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., \u201cESPnet-SLU: Advancing Spoken Language Un- derstanding Through ESPnet,\u201d in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning"}