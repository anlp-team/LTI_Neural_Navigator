{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_MOSAIC:_Learning_Unified_Multi-Sensory_Object_Property_Representations_for_Robot_Perception_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What were the four object properties encapsulated in the 2D unified representations?", "answer": " Object categories, material, deformability, and hardness.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " Why do the observed inconsistencies arise in the visualizations of object properties in Fig. 3?", "answer": " The observed inconsistencies arise because not every object possesses all the properties being visualized.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " How many examples were included in the dataset for learning unified representations?", "answer": " 400 examples.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " In the 2D space, how do objects within the same category or material composition appear?", "answer": " They form tight clusters.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " What insight do the visualizations of object properties provide?", "answer": " They show the efficiency of capturing object semantics, material characteristics, and relationships among objects with similar properties.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " What method did Sinapov et al. use for category recognition, according to Table II?", "answer": " Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " How did the MOSAIC framework perform in terms of object category recognition compared to state-of-the-art results?", "answer": " It demonstrated higher recognition accuracy in seven out of ten behaviors and achieved comparable accuracy in the remaining three.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " What was the target object selection rate of the full MOSAIC framework in Level-1 of the fetch object task?", "answer": " 99.0%.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " In Level-4 of the fetch object task, what challenge was introduced?", "answer": " The inclusion of two distractor objects resembling the target object.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}, {"question": " In Level-5 of the fetch object task, what descriptive words related to specific property categories were included in the command?", "answer": " Deformability, shape, size, transparency, and weight.", "ref_chunk": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}], "doc_text": "weighted combination of each behavior\u2019s performance on the training data. We also compare our recognition accuracy total number of commands Fig. 3: 2D unified representations derived from autoencoder trained on Push behavior\u2019s data: (A) Object categories, (B) Material, (C) Deformability, and (D) Hardness properties. with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to raw multi- sensory data for object category classification. For the fetch object task (see Algorithm 2), the set B contains only the Look behavior for the non-interactive condition and all 9 interactive behaviors for the interactive condition. V. RESULTS An Illustrative Example. Let\u2019s consider a scenario where the robot performs the Push behavior on 80 objects (4 objects x 20 categories), recording visual, acoustic, and haptic data. With each object undergoing 5 trials, this yields a dataset of 400 examples (80 objects \u00d7 5 trials). Using our MOSAIC framework, we use this data to learn unified representations. For visualization, we subjected these representations to di- mensionality reduction using a linear autoencoder, resulting in a concise 2-dimensional latent space (Fig. 3). This visual- ization encapsulates four object properties: object categories, material, deformability, and hardness. Distinct colors are used to differentiate objects based on different values of these properties. To maintain clarity, we selectively plot only specific categories or objects with particular properties. Due to the absence of certain properties in some objects, the observed inconsistencies in Fig. 3 for different properties arise, as not every object possesses all the properties. These visualizations unveil meaningful insights. Objects within the same category or material composition form tight clusters in the 2D space, showing the efficiency of our unified representations in capturing object semantics and material characteristics. The deformability properties plot demon- strates a separation between rigid and deformable objects, with brittle ones inclining towards deformable. Similarly, in the hardness properties plot, hard objects cluster on one side, while soft and squishy objects gravitate towards the opposite side. Essentially, our unified representations effectively en- code objects with similar properties, as evidenced by distinct clusters of similar objects, even when these objects belong to different categories or material groups across various property categories. This illustrates MOSAIC\u2019s capacity to capture nuanced object attributes and relationships, a pivotal aspect of its performance across diverse tasks. TABLE II: Category recognition accuracy (%) for each behavior. Behavior Sinapov et al.[14] Tatiya et al.[15] MOSAIC-w/o-SA MOSAIC (ours) Look 67.7 \u2014 86.4 \u00b1 1.2 87.4 \u00b1 2.0 Grasp Hold Lift Drop Poke Push Shake Tap Press 65.2 67.0 79.0 71.0 85.4 88.8 76.8 82.4 77.4 71.4 76.8 77.8 78.0 73.8 67.4 83.6 81.6 58.8 72.2 \u00b1 6.7 68.0 \u00b1 5.3 72.8 \u00b1 4.2 73.2 \u00b1 3.8 81.6 \u00b1 2.2 85.6 \u00b1 3.5 81.2 \u00b1 6.2 81.2 \u00b1 5.7 71.6 \u00b1 8.7 74.0 \u00b1 5.8 69.6 \u00b1 5.2 77.8 \u00b1 5.7 77.2 \u00b1 5.9 86.4 \u00b1 1.0 89.4 \u00b1 4.4 84.0 \u00b1 5.6 84.4 \u00b1 1.8 77.8 \u00b1 6.4 All behaviors \u2014 \u2014 95.2 \u00b1 3.6 95.6 \u00b1 3.9 Object Category Recognition Results. Object category recognition results are presented in Table II. Note that the Look behavior only relies on visual modality, and the \u201cAll behaviors\u201d row at the bottom refers to all 9 interactive behav- iors combined. Our approach, using unified representations, exhibits a remarkable level of competitiveness compared to state-of-the-art results for this dataset, demonstrating higher recognition accuracy in seven out of ten behaviors. For the re- maining three behaviors, we achieved comparable accuracy. We achieved this level of performance using a straightfor- ward linear model on top of the unified representations, a contrast to previous methods. Notably, the prior work [15] employed a specialized neural network architecture tailored specifically for this task, while [14] relied on handcrafted features. Furthermore, our results consistently indicate that our full framework, including self-attention, outperforms the counterpart without self-attention. This underscores the utility of the multi-sensory unified representation and the effectiveness of the self-attention mechanism in enhancing the robot\u2019s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot\u2019s ability to execute instruc- tions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for \u201cLook\u201d without self-attention. This suggests that learning unified rep- resentations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot\u2019s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving the non-visual properties like deformability and weight, TABLE III: MOSAIC\u2019s target object selection (%) in various levels of the fetch object task, with and without Self-Attention. Look (non-interactive) Interactive w/o-SA MOSAIC w/o-SA MOSAIC LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 74 61 60 54 82 65 74 70 97 84 86 72 99 81 83 77 LEVEL 5 DEFORMATION SHAPE SIZE TRASPARENCY WEIGHT 45 85 62 62 52 48 80 74 62 63 71 97 72 51 85 74 95 75 63 85 interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and"}