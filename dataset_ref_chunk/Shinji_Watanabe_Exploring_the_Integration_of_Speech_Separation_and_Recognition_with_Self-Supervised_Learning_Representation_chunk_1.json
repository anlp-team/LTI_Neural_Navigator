{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploring_the_Integration_of_Speech_Separation_and_Recognition_with_Self-Supervised_Learning_Representation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main topic of the text?,        answer: The integration of speech separation and recognition with self-supervised learning representation.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " Which universities are involved in the research described in the text?,        answer: Tokyo Metropolitan University, Carnegie Mellon University, Shanghai Jiao Tong University, and Universit`a Politecnica delle Marche.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What is the purpose of exploring multi-channel separation methods in the research?,        answer: To improve automatic speech recognition (ASR) performance as an ASR front-end in reverberant and noisy-reverberant scenarios.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What feature is employed to improve the recognition performance in the research?,        answer: Self-supervised learning representation (SSLR).    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What technique is used in the proposed integration to achieve a 2.5% word error rate in the WHAMR! test set?,        answer: TF-GridNet-based complex spectral mapping and WavLM-based SSLR.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " How have conventional SSE models been improved in recent times?,        answer: By the invention of deep clustering and permutation invariant training, allowing training neural networks for speech separation in a supervised manner.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What is the potential downside mentioned regarding the correlation between speech separation and ASR performance?,        answer: The separation performance may degrade after fine-tuning, even if the word error rate decreases.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What is the purpose of integrating MIMO-IRIS in the research?,        answer: To explore an end-to-end integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What is the main result achieved by the best MIMO-IRIS model after joint training?,        answer: SotA ASR performance on the WHAMR! dataset with a word error rate of 2.5%.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}, {"question": " What is the transcription sequence denoted for each speaker in the system described?,        answer: Rk.    ", "ref_chunk": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}], "doc_text": "3 2 0 2 l u J 3 2 ] D S . s c [ 1 v 1 3 2 2 1 . 7 0 3 2 : v i X r a 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 22-25, 2023, New Paltz, NY EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION Yoshiki Masuyama,1\u22c6 Xuankai Chang,2\u22c6 Wangyou Zhang,3 Samuele Cornell,4 Zhong-Qiu Wang,2 Nobutaka Ono,1 Yanmin Qian,3 Shinji Watanabe,2 1Tokyo Metropolitan University, Japan 2Carnegie Mellon University, USA 3Shanghai Jiao Tong University, China 4Universit`a Politecnica delle Marche, Italy ABSTRACT Neural speech separation has made remarkable progress and its in- tegration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we ex- plore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recog- nition performance from the case with filterbank features. To fur- ther improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separa- tion and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%). Index Terms\u2014 speech separation, speech recognition, self- supervised learning, joint training, beamforming 1. INTRODUCTION Speech separation and enhancement (SSE) is a crucial front-end for various applications such as speaker diarization, automatic speech recognition (ASR), and spoken language understanding [1\u20133]. The speech separation field has been revolutionized by the invention of deep clustering [4] and permutation invariant training (PIT) [5], which allow us to train deep neural networks (DNNs) for speech separation in a supervised manner. Previous speech separation methods based on time-frequency (T-F) masking [4\u20137] used a DNN to estimate the T-F mask for each speaker from the short-time Fourier transform (STFT) of the observed mixture. Meanwhile, time-domain methods [8\u201310] have demonstrated promising results by directly processing time-domain signals in an end-to-end (E2E) manner. Recently, fully complex STFT-domain methods have been proven to be extremely effective [11\u201313]. In particular, TF-GridNet [13] has achieved state-of-the-art (SotA) performance on several SSE benchmarks [4, 6, 14], including both monaural and multi- channel cases. Despite these impressive recent improvements in separation performance, it is still unclear how and when they can also lead to better ASR performance. Most conventional SSE models are trained to minimize signal- level differences between the separated and target speech [8, 9]. Pre-trainingPre-trainingPre-training Frozen WavLMWavLM JointCTC/AttentionJointCTC/Attention Joint fine-tuningFine-tuned Mask-based MVDRorTF-GridNet Figure 1: Overview of our E2E integration. We pre-train speech separation, SSLR, and ASR models separately, and fine-tune the speech separation and ASR models jointly while freezing WavLM. This could lead to mismatches with respect to the subsequent ASR task. To address this issue, several attempts [15\u201322] have been made by integrating SSE and ASR models with joint optimization. For robust ASR, a neural beamformer and a joint connectionist tempo- ral classification (CTC)/attention-based encoder-decoder were in- tegrated and optimized with the ASR objectives [18]. This inte- gration was extended to multi-speaker settings including MIMO- Speech [20]. It aims to directly improve the performance of multi- speaker ASR while preserving the modularity of the entire system, as opposed to a fully E2E black-box approach [23\u201325]. The inter- mediate separated speech achieves a good separation quality [20], although any signal-level criteria are not used for training. Self-supervised learning (SSL) models such as Wav2Vec 2.0 [26], HuBERT [27], and WavLM [28] have shown consider- able potential in a wide range of speech processing tasks [29, 30]. Recently, IRIS [31] demonstrated impressive results with an E2E model that integrates monaural speech enhancement, WavLM, and ASR models. MultiIRIS [32] expanded IRIS to perform multi- channel speech enhancement and demonstrated the effectiveness of the joint training under noisy and reverberant conditions. Building upon MultiIRIS, this paper investigates MIMO-IRIS: an E2E integration of speech separation, SSLR extraction, and ASR for multi-channel multi-speaker overlapping scenarios. We explore the combination of SSLR-based ASR models [33] with TF-GridNet [13] as well as well-established beamforming techniques as illus- trated in Fig. 1. We perform an extensive experimental validation on the spatialized WSJ0-2mix [6] and WHAMR! [14] datasets, assess- ing both separation and ASR performance. Interestingly, our ex- periments show that the correlation between speech separation and ASR performance is not precisely positive. The separation perfor- mance after fine-tuning degraded the separation performance while the word error rate (WER) decreases. This is especially true for TF- \u22c6Equal contribution. 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics GridNet-based complex spectral mapping, while mask-based beam- forming [34, 35] results in less degradation. Despite this, our best MIMO-IRIS model after joint training achieves SotA ASR perfor- mance on the WHAMR! dataset with a WER of 2.5%, comparable to SotA results on clean single-speaker WSJ evaluation sets [33]. 2. END-TO-END MULTI-CHANNEL MULTI-SPEAKER ASR WITH SPEECH SEPARATION AND SSLR Given an L-sample, M -channel mixture signal X = (xm)M RM \u00d7L consisting of K speakers and noises N = (nm)M formulate the mixing process as follows: m=1 \u2208 m=1, we xm = K (cid:88) sk,m + nm, (1) k=1 where sk,m \u2208 RL is the source image of speaker k at microphone m. The transcription sequence for speaker k is denoted as Rk. This section describes each part of the proposed E2E system, depicted in Fig. 1, including speech separation, SSLR extraction, and ASR. 2.1. Speech Separation The goal of speech separation is to estimate each speaker\u2019s signal \u02c6sk,r at a reference microphone r \u2208 {1, . . . , M } from the mixture X, which can be written as: {(cid:98)s1, . . . , (cid:98)sK } = SS(X). Depending on the number of input microphones, the task can be divided into monaural and multi-channel"}