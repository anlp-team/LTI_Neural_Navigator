{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the text use to measure the reconstruction error between ground truth and synthesis?,answer: MCD (Mel-Cepstral Distortion)", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " How many mel-cepstral coefficients (MCEPs) are extracted in the text?,answer: 23", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " Which metric is used to calculate the reconstruction quality of the quantizer?,answer: RCER (Reconstruction Error)", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " What does the text suggest about using a single codebook (N = 1)?,answer: It drastically distorts the input phone sequence and results in low speech quality.", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " What size of codebook is used for comparisons in the text?,answer: 65536 and 1024 codes", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " Why is it indicated that multiple code groups are necessary?,answer: A single codebook fails to reconstruct the original input, indicating the need for multiple code groups.", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " How does the quantizer in the text compare to the mel-spectrogram based HiFi-GAN in terms of speech quality?,answer: The quantizer has slightly higher RCER but significantly better speech quality (MOS-Q) than the HiFi-GAN.", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " What code size is used in the text for experiments when little quality and intelligibility difference is observed?,answer: 160", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " Why is Tacotron 2 not included for quantitative evaluations in the text?,answer: Its syntheses are not understandable, indicating poor performance.", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}, {"question": " What issue does applying the duration predictor resolve in Transformer TTS?,answer: It resolves the alignment issue during inference, improving performance.", "ref_chunk": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}], "doc_text": "and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coef\ufb01cients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split. 5 Results 5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not suf\ufb01cient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we \ufb01nd that a single code- book is suf\ufb01cient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8. 5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2. Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers. Table 2: Comparison of TTS models. MOS is with 95% con\ufb01dence interval. MCD is with one standard deviation. Method #Params. WER (%) \u2193 SSS \u2191 P-FID \u2193 MCD \u2193 MOS-N \u2191 GT n/a 14.9 n/a 0.11 n/a 4.01\u00b10.06 VITS \u2248 40M \u2248 100M 28.4 24.8 0.718 0.734 15.48 12.04 8.94\u00b11.16 8.73\u00b11.19 3.84\u00b10.06 3.84\u00b10.06 Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor) \u2248 100M 82.6 90.2 74.3 0.434 0.331 0.451 442.29 491.49 196.09 12.49\u00b12.77 12.79\u00b14.35 11.70\u00b12.12 2.34\u00b10.09 2.23\u00b10.10 2.75\u00b10.09 MQTTS \u2248 40M \u2248 100M \u2248 200M 24.9 22.3 22.2 0.727 0.751 0.762 11.67 8.58 6.21 10.67\u00b11.78 10.22\u00b11.68 10.17\u00b11.70 3.79\u00b10.06 3.87\u00b10.06 3.89\u00b10.06 MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor) \u2248 100M 34.3 27.0 53.2 0.740 0.740 0.725 10.20 12.12 16.65 10.86\u00b12.00 10.47\u00b11.81 10.88\u00b11.62 3.76\u00b10.06 3.75\u00b10.07 3.67\u00b10.06 hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signi\ufb01- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context. Model Variants. We can observe the ef\ufb01cacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im- pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less in\ufb02uence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation. Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insuf\ufb01cient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to \ufb01nd a con\ufb01guration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity"}