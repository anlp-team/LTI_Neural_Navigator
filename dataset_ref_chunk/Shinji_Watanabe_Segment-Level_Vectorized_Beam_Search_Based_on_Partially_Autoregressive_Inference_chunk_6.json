{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Segment-Level_Vectorized_Beam_Search_Based_on_Partially_Autoregressive_Inference_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the proposed framework mentioned in the text?", "answer": " Partially autoregressive framework (PAR)", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " What is the goal of the PAR framework?", "answer": " To obtain a new trade-off balance between accuracy and latency", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " How does PAR utilize the strengths of NAR and AR methods?", "answer": " By compensating for the weaknesses inherent in each method", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " What was the speedup achieved by the LS-960 pre-trained model using PAR?", "answer": " A 13.75\u00d7 speedup with the same Word Error Rate (WER)", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " What is one limitation of the PAR framework mentioned in the text?", "answer": " Memory usage issues", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " In future work, what type of devices does the text mention extending the PAR framework to?", "answer": " Edge devices with limited computational resources", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " What is the title of Reference [1] mentioned in the text?", "answer": " Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " Who are the authors of Reference [5] mentioned in the text?", "answer": " Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " What is the title of Reference [9] mentioned in the text?", "answer": " Non-autoregressive neural machine translation", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}, {"question": " What is one of the authors listed for Reference [16]?", "answer": " Navdeep Jaitly", "ref_chunk": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}], "doc_text": "due to the inac- curacy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimat- ing the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to ob- tain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advan- tage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75\u00d7 speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usabil- ity of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenar- ios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. ICASSP, 2013, pp. 6645\u20136649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00a8uter, and Shinji Watanabe, \u201cEnd-to-end speech recogni- tion: A survey,\u201d arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng- dong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe, \u201cE-Branchformer: Branchformer with enhanced merging for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [7] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, \u201cRobust speech Christine McLeavey, and Ilya Sutskever, recognition via large-scale weak supervision,\u201d 2022. [8] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, \u201cESPnet- ST: All-in-one speech translation toolkit,\u201d in Proc. ACL, On- line, 06 2020, pp. 302\u2013311, Association for Computational Linguistics. [9] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher, \u201cNon-autoregressive neural machine transla- tion,\u201d in Proc. ICML, 2018. [10] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu, \u201cA survey on non-autoregressive generation for neural machine translation and beyond,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [11] Nanxin Chen, Shinji Watanabe, Jes\u00b4us Villalba, Piotr \u02d9Zelasko, and Najim Dehak, \u201cNon-autoregressive transformer for speech recognition,\u201d IEEE Signal Processing Letters, vol. 28, pp. 121\u2013 125, 2021. [12] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly, \u201cImputer: Sequence modelling via imputation and dynamic programming,\u201d in Proc. ICML, 2020, pp. 1403\u20131413. [13] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cMask CTC: Non- Autoregressive End-to-End ASR with CTC and Mask Predict,\u201d in Proc. Interspeech, 2020, pp. 3655\u20133659. [14] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, \u201cAlign- Refine: Non-autoregressive speech recognition via iterative re- alignment,\u201d in Proc. NAACL-HLT, 2021, pp. 1920\u20131927. [15] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NeurIPS, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc. [16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocab- ulary conversational speech recognition,\u201d in Proc. ICASSP, 2016, pp. 4960\u20134964. [17] Yosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi In- aguma, Tatsuya Komatsu, Jaesong Lee, Jumon Nozaki, Tianzi Wang, and Shinji Watanabe, \u201cA comparative study on non- autoregressive modelings for speech-to-text generation,\u201d in Proc. ASRU, 2021, pp. 47\u201354. [18] Yann LeCun, John Denker, and Sara Solla, \u201cOptimal brain damage,\u201d in Proc. NeurIPS, 01 1989, vol. 2, pp. 598\u2013605. [19] Frankle Jonathan and Carbin Michael, \u201cThe lottery ticket hy- pothesis: Finding sparse, trainable neural networks,\u201d in Proc. ICLR, 2019. [20] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu- rana, David Cox, and Jim Glass, \u201cPARP: Prune, adjust and re- prune for self-supervised speech recognition,\u201d Proc. NeurIPS, vol. 34, pp. 21256\u201321272, 2021. [21] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, \u201cDistil- HuBERT: Speech representation learning by layer-wise distil- lation of hidden-unit bert,\u201d in Proc. ICASSP, 2022, pp. 7087\u2013 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, \u201cVectorized beam search for CTC- in Proc. Interspeech, attention-based speech recognition,\u201d 2019, pp. 3825\u20133829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, An- mol Gulati, Yonghui Wu, and Ruoming Pang, \u201cFastEmit: Low- latency streaming ASR with sequence-level emission regular- ization,\u201d in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, \u201cWeNet: Production oriented streaming and non- streaming end-to-end speech recognition toolkit,\u201d in Proc. In- terspeech, 2021, pp. 4054\u20134058."}