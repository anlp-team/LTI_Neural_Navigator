{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Value_Kaleidoscope:_Engaging_AI_with_Pluralistic_Human_Values,_Rights,_and_Duties_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of distilling knowledge from powerful generative models like GPT-4 in the VALUEPRISM dataset?", "answer": " The purpose is to obtain high-quality knowledge for deriving plural considerations.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " How does GPT-4 excel compared to average human annotations in VALUEPRISM?", "answer": " GPT-4 excels at enumerating a wide range of value alternatives.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " What percentage of the distilled data in VALUEPRISM is deemed high quality after verification with human annotators?", "answer": " 91% of the distilled data is deemed high quality.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " How are clean situations derived in VALUEPRISM?", "answer": " Clean situations are derived by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " What is the proportion of toxic, NSFW, or sexually explicit content in the situations dataset, and how was it balanced?", "answer": " The proportion is down-sampled to 5% of all data to balance out the outsize proportion, leading to an increase in overall diversity of the dataset.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " In the Values, Rights, and Duties Generation process, what does GPT-4 prompt to generate for each situation?", "answer": " GPT-4 prompts to generate relevant values, rights, and duties, along with open-text rationales.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " How is VALUEPRISM converted for multi-task training in the multi-task setup?", "answer": " VALUEPRISM is converted into a sequence-to-sequence format for multi-task training, where relevance task data is sampled contrastively.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " What is the base model used for KALEIDO in the Model Training phase?", "answer": " Flan-T5 3B is used as the base model for KALEIDO.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " How is the diversity of values, rights, and duties ensured in the KALEIDOSYS system?", "answer": " The system overgenerates values and removes low-quality and repetitive outputs based on relevance score and text similarity.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}, {"question": " What methods are used to tune the system parameters in KALEIDOSYS for maximizing RougeL-Sum F1 score?", "answer": " Gibbs sampling is used to tune the relevance score threshold and similarity thresholds.", "ref_chunk": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}], "doc_text": "oppose oppose Duty Duty to express displeasure Duty to be a considerate driver support oppose Table 1: Example outputs from KALEIDOSYS. Relevance (2-way classification) Is a value relevant for a situation? Some values are more relevant than others. Valence (3-way classification) Does the value support or oppose the action, or might it depend on context? Disen- tangling the valence is critical for understanding how plural considerations may interact with a decision. Explanation (open-text) How does the value relate to the action? Generating a post-hoc rationale for why a value con- sideration may relate to a situation. 3.2 Dataset: VALUEPRISM We leverage the symbolic knowledge distillation (West et al. 2022a) pipeline to distill high-quality knowledge from pow- erful generative models like GPT-4, which have been shown to compare favorably to human annotations on quality, cov- erage, and diversity (West et al. 2022a; Gilardi, Alizadeh, and Kubli 2023; Ziems et al. 2023). Importantly, based on our preliminary exploration, GPT-4 excels at enumerating a wide range of value alternatives compared to average hu- man annotations. We verify the dataset\u2019s quality with hu- man annotators and show that 91% of the distilled data is deemed high quality, surpassing typical quality of human generated data (West et al. 2022a; Hwang et al. 2021; Zhou et al. 2023). Details on dataset statistics and splits are pro- vided in App. F.1 and examples from VALUEPRISM can be found in App. A. Situations Starting from 1.3M human-written base situa- tions 3, we obtain a set of 31K clean situations for deriving plural considerations by carefully filtering out ill-formatted, irrelevant, and low-quality situations using few-shot Flan-T5 (Chung et al. 2022). To balance out an outsize proportion of toxic, NSFW, or sexually explicit content, we down-sample these situations to 5% of all data, leading to an increase in the overall diversity of the dataset, as measured by the nor- malized count of unique n-grams (dist-2: .23\u2192.36, dist-3: .54\u2192.67, details in App. F.1). Values, Rights, and Duties Generation For each of the 31K situations, we prompt GPT-4 to generate relevant val- ues, rights, and duties, with open-text rationales. GPT-4 also attributes whether the corresponding value, right, or duty supports (justifies), opposes (condemns), or depends on the context or interpretation. Details of data generation and prompting are in Appendix F.1 and M. The resulting dataset is rated as high-quality (91%) by human evaluation (\u00a74.1). Multi-task Setup We convert VALUEPRISM into a sequence-to-sequence format for multi-task training. The relevance task data is sampled contrastively, where positive examples are from GPT-4 and negative examples are drawn from values mapped to other situations. We split the data (by actions) into 80% train/10% validation/10% test. 3.3 Model Training We use Flan-T5 3B (Chung et al. 2022) as the base model for KALEIDO. After fitting hyperparameters with a grid search, we train on a mixture of all four tasks with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping. Training takes 19 hours on two A100 GPUs. Training details are in App. G. 3.4 A System of Diverse Values: KALEIDOSYS We use KALEIDO to generate a diverse set of values, rights, and duties by overgenerating (top 100 beams) and remov- ing low-quality and repetitive outputs via the relevance score and text similarity respectively. We use Rouge-score (Lin 2004) for n-gram similarity and a Transformers (Wolf et al. 2020) mpnet model4 for sentence embeddings. See Fig. 2 for an illustration of the system and Appendix H/Algorithm 1 for more details. We tune the system parameters (relevance score threshold, similarity thresholds) using Gibbs sampling (Casella and George 1992) to maximize RougeL-Sum F1 score on the validation set. Ablation experiments in \u00a75.1 3Sourced from user queries submitted to the Delphi demo. 4https://huggingface.co/sentence-transformers/all-mpnet-base- v2 provide insights on each system component, and example model outputs can be found in Table 1 and App. B. 4 Data Analysis 4.1 VALUEPRISM is high-quality We conduct human validation of VALUEPRISM to assess the its quality on the Mechanical Turk platform5. Given the gen- erated situation and values, rights, and duties and their ex- planations, we ask the annotators to assess the relevance and quality of the generations. The results show that annotators find the great majority of the data as high quality. 91% of the values/rights/duties was marked as good by all three annota- tors and 87% of the valences were marked as correct by all three annotators. In an attempt to find any values that may have been missed, we also prompt crowdworkers to fill in any miss- ing values, rights, or duties. Crowdworkers didn\u2019t seem to find it easy to come up with missing values as we get sug- gestions 0.35% of the time. Full annotation details for this and other studies are in App. I. 4.2 Evaluation by diverse annotators Prior research has reported unjust bias in LLMs against marginalized groups (Sap et al. 2019; Feng et al. 2023). We evaluate VALUEPRISM by recruiting a diverse population of 613 annotators6 through CloudResearch (Litman, Robinson, and Abberbock 2017) targeting those marginalized groups to the extent possible7. We collect 31k annotations across 683 values, rights, and duties in the context of 100 situations, along with demographic information across eight categories. The annotators mark 1) if they agree with each value, right, or duty listed for a given situation and 2) if they spot any missing perspective. We do not find notable statistical sig- nificance, and do not reject the null hypothesis that there is no difference between groups. Additional group statistics, p-values, and qualitative analyses are in App. E. 4.3 Diversity of VALUEPRISM We analyze the diversity of the situations, and values, rights, and duties from three perspectives: lexical diversity that cal- culates uniqueness n-grams, topical diversity that assesses semantic diversity via topic analysis8, and clustering. Both 5For this and other human studies, we have acquired the opin- ion of our institutions\u2019s Internal Review Board. The opinion finds our project exempt from a full review process in accordance to the United States Federal regulation 45 CFR 46, and we have acquired"}