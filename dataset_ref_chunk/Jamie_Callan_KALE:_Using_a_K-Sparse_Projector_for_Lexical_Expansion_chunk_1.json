{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_KALE:_Using_a_K-Sparse_Projector_for_Lexical_Expansion_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the method proposed in the paper?", "answer": " KALE", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What type of projector does KALE use for lexical expansion?", "answer": " K-sparse projector", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " Which datasets were used to examine the effectiveness of KALE?", "answer": " MSMARCOv1 passage retrieval dataset, TREC Deep Learning dataset, and BEIR datasets", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What are the advantages of combining KALE with the original lexical vocabulary?", "answer": " Improvement in retrieval accuracy with only a modest increase in computational cost", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What is the focus of current research with dense retrieval methods?", "answer": " Negative examples for training, knowledge distillation, better training methodologies, and larger models", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What kind of encoder model is typically used in the dual encoder architecture for dense representations?", "answer": " Transformer encoder model (e.g., BERT)", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What are the drawbacks of storing dense representations in memory?", "answer": " Large memory requirements and larger search times", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What is the goal of KALE in terms of generating semantic concepts?", "answer": " To allow a neural model to generate important semantic concepts from its training dataset", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What kind of encoder model does KALE leverage?", "answer": " DistilBERT model", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}, {"question": " What are some examples of sparse approaches mentioned in the text?", "answer": " BM25, DeepCT, DocT5Query, CCSA, UHD-BERT, SPLADE, and more", "ref_chunk": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}], "doc_text": "KALE: Using a K-Sparse Projector for Lexical Expansion Lu\u00eds Borges Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA lborges@andrew.cmu.edu Bruno Martins IST and INESC-ID University of Lisbon Lisbon, Portugal bruno.martins@tecnico.ulisboa.pt Jamie Callan Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA callan@andrew.cmu.edu ABSTRACT Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neu- ral language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost. CCS CONCEPTS \u2022 Information systems \u2192 Query representation; Document representation; Retrieval effectiveness; Retrieval efficiency. KEYWORDS Neural Information Retrieval, Learned Sparse Representations, Effi- ciency in Neural Retrieval. ACM Reference Format: Lu\u00eds Borges, Bruno Martins, and Jamie Callan. 2023. KALE: Using a K- Sparse Projector for Lexical Expansion. In Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201923), July 23, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3578337.3605131 a first-stage retriever, or they use in-memory indexes for perform- ing either exhaustive or Approximate Nearest-Neighbour (ANN) searches over the document collection. On the other hand, methods based on sparse representations use inverted indexes for efficient retrieval over large collections stored in disk, and these particular indexing and query processing strategies have been extensively studied within the community [37, 43]. It is therefore of interest to consider ways of computing sparse representations with the same type of neural approaches that support the better performing dense representations, envisioning indexing with inverted indexes. The dual encoder architecture is a popular method of converting text to dense representations [8, 9, 14, 20, 21, 30\u201332, 39]. Queries and documents are processed with a Transformer encoder model (e.g., BERT), and a similarity between the resulting dense vectors can then be computed. The current research focus with dense re- trieval lies on the choice of the negative examples for training [21, 32, 39], knowledge distillation [20, 21], better training method- ologies [9, 30, 32], and larger models [27]. Dense representations are typically stored in memory and incur not only in large mem- ory requirements, but also in larger search times, since queries are compared to every document in the collection. On the other hand, modern sparse approaches generally leverage Transformer models to process the text and create new sparse representations, which involve reweighting existing document terms, expanding the document with new terms, or both. These methods either project the dense representations into a known lexical vocabulary, e.g. the BERT wordpiece vocabulary [7, 24], or into new high dimen- sional vocabulary spaces [12], often with a larger vocabulary size than BERT. State-of-the-art learned sparse representations typi- cally perform reweighting/expansion over the BERT vocabulary, but are limited to the concepts captured by the aforementioned set of terms. Additionally, expanding queries/documents can become expensive, in terms of query latency and index size. Approaches that project dense representations into high dimensional spaces usually underperform reweighting/expansion techniques, and are used in isolation, replacing the existing English vocabulary. 1 INTRODUCTION Neural retrieval approaches, based on the computation of dense vec- tor representations for documents and queries, tend to outperform methods based on sparse representations [26, 41]. Dense methods typically perform a re-ranking of a small set of results obtained from Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0073-6/23/07. . . $15.00 https://doi.org/10.1145/3578337.3605131 This paper presents KALE (K-spArse Projector for Lexical Expansion), a simple and fast approach based on a typical dual- encoder that produces sparse representations in a new vocabulary space. Instead of relying on the BERT vocabulary, a new vocabulary is created, with the goal of allowing a neural model to generate important semantic concepts from its training dataset. Ideally, these terms should capture concepts beyond those from the lexical vo- cabulary, and can be used either as a replacement of the original English vocabulary, or as an addition to existing representations. KALE consists of a simple encoder, leveraging a DistilBERT model, in order to generate sparse representations of the input text. In order to train the model, a frozen teacher distills knowledge into ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Dense approaches (A) DPR [14] (B) ANCE [39] (C) TCT-ColBERT [20] (D) Condenser [8] (E) RocketQA [30] (F) TCT-ColBERTv2 [21] (G) CoCondenser [9] (H) RocketQAv2 [32] (I) MASTER [42] (J) RetroMAEv2 [38] (K) ABS [2] + CoCond. [9]\u2217 MRR@10 0.311 0.330 0.364 0.366 0.370 0.375 0.382 0.388 0.415 0.426 0.447 Sparse approaches (a) BM25 [33] (b) DeepCT [6] (c) DocT5Query [28] (d) CCSA [18] (e) UHD-BERT [12] (f) SPLADE [7] (g) DocT5Query\u2013 [10] (h) DeepImpact [24] (i) SpaDE [3] (j) EfficientSPLADE [17] (k) LexMAEv2"}