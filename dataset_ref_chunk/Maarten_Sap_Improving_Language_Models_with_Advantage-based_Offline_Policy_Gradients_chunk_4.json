{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Improving_Language_Models_with_Advantage-based_Offline_Policy_Gradients_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main dataset used in the experiment mentioned in the text?", "answer": " The Helpful and Harmless assistant dataset.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " How many instances of user-assistant conversations are there in the dataset?", "answer": " The dataset contains 170,000 instances of user-assistant conversations.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " What are the four subsets of the dataset mentioned in the text?", "answer": " The subsets are Harmlessbase, Helpfulbase, Helpfulonline, and Helpfulrejection.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " How many training conversations are there in total for the task?", "answer": " There are 143,000 train conversations.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " What architecture is chosen as the reference policy in the experiment?", "answer": " LLaMA-7B base architecture.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " How many different training runs were executed for the experiment?", "answer": " 36 different training runs.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " What is employed as the reward model for algorithms using rewards?", "answer": " A 1.4B parameter classifier trained on human preference labels.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " What problem do the preference-based methods, especially DPO and DPO (ref. free), face according to the text?", "answer": " They suffer from high variance across random seeds.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " What factor can make A-LOL KL unstable as mentioned in the text?", "answer": " Minimizing the KL penalty term instead of policy-gradient loss.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}, {"question": " What is reported in Table 2 in relation to the HHA aggregate test reward and diversity evaluation?", "answer": " Average metrics obtained by each method across three random seeds.", "ref_chunk": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}], "doc_text": "a ranking loss and interpolates it with the negative log-likelihood for stability. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. We cannot test with preference-based methods in tasks with multiple rewards or where human-labeled preferences are not available. Reward-based Baselines We also compare with R-LOL (Equation 4) and other related reward- based offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4). 4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations that end with a pair of model- generated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversa- tions which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and Helpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from 5 Preprint under review. Figure 2: HHA validation trends of preference (left), reward (middle), and advantage-based (right) offline RL algorithms compared with negative log-likelihood (NLL) training over three random seeds. Song et al. (2023a) with minor data cleaning.5 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses. Reference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset6 as the reference policy. We also test PPO7 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier8 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 1.3\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.9 We present the implementation details of all tested methods in Appendix B.1. 4.1 HHA RESULTS Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3 and notice one of the seeds shows performance deterioration and mode-collapse (Shumailov et al., 2023; Go et al., 2023). Automatic Evaluation and Analysis We employ a larger 6.5B parameter reward model10 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance. 5Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories. 6https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 7We use the huggingface TRL (von Werra et al., 2020) implementation of PPO. 8https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 9https://huggingface.co/reciprocate/ppo hh pythia-6B 10https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 6 Preprint under review. Table 2: HHA aggregate test reward and diversity evaluation of NLL, PPO, and other offline RL methods averaged across three random seeds. For comparison, we also report the performance of Test responses and reference policy (\u03c0ref) in the first three rows, along with another external PPO model* in the last row. Overall, the best A-LOL methods achieve comparable rewards to DPO, while approaching the distribution of Test good responses in terms of length and diversity. Oddly, DPO and DPO (ref. free) tend to generate longer and less diverse responses while PPO generates the shortest. Algorithm #instances Harmless base (2210) base (2278) Helpful online (1085) rejection (2634) Avg. Reward (8207) Avg. Length Diversity: Avg Distinct-1,2,3 Test good responses Test bad responses \u03c0ref (LLaMA-7B) 54.8 50.0 54.8 40.3 34.3 36.5 61.6 59.4 49.4 50.5 45.3 41.5 50.3 45.4 44.7 46.7 45.3 51.1 .099/.471/.773 .099/.468/.771 .067/.246/.404 + MLE or NLL 60.7 44.0 56.5 49.5 51.9\u00b10.5 43.3\u00b13.5 .084/.336/.552 Preference-based offline RL + PRO + DPO (ref. free) + DPO 63.0 53.6 60.9 46.4 50.1 55.2 57.6 54.4 61.0 51.8 52.3 58.5 54.1\u00b10.6 52.3\u00b11.4 58.6\u00b10.8 43.4\u00b12.5 90.5\u00b11.1 66.7\u00b15.9 .084/.339/.560 .049/.226/.432 .065/.288/.503 Reward-based offline RL + wBC + R GOLD + R-LOL 62.4 62.7 63.7 47.0 46.7 47.0 59.7 59.0 59.4 53.2 52.6 53.1 54.8\u00b10.4 54.5\u00b10.3 55.1\u00b10.6 42.7\u00b11.1 44.2\u00b13.1 38.6\u00b12.4 .091/.380/.622 .086/.355/.584 .095/.394/.640 Advantage-based offline RL + A-LOL (ref. free) + A-LOL + A-LOL seq. + A-LOL KL 63.8 64.3 64.4 65.7 49.2 50.3 50.9 50.7 60.7 61.1 62.2 61.1 54.7 55.8 55.8 56.2 56.4\u00b10.6 57.3\u00b10.3 57.6\u00b10.5 57.9\u00b10.3 40.7\u00b11.7 42.3\u00b11.9 40.1\u00b12.4 44.0\u00b11.7 .095/.400/.651 .094/.403/.657 .105/.464/.727 .090/.387/.639 Online RL + PPO PPO* (pythia 6B) 63.3 48.6 39.9 32.1"}