{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the TTS community shifting towards, and why?,answer: The focus of the TTS community is shifting towards non-autoregressive models due to their better robustness, inference speed, and lower training difficulty.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is VITS, and how does it achieve near-human naturalness on the VCTK dataset?,answer: VITS is an end-to-end model that uses a stochastic duration predictor for better diversity. It achieves near-human naturalness on the VCTK dataset.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the approach taken by Quantization-based Generative Models for TTS?,answer: Quantization-based Generative Models use a two-stage approach where a quantizer encodes the image or speech into discrete tokens, and an autoregressive transformer models the sequential distribution of these tokens.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is DiscreTalk and how does it utilize a VQVAE and a transformer?,answer: DiscreTalk is a TTS system that discretizes speech using a VQVAE and a transformer for autoregressive generation.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What are unit-based language models (uLMs) trained on, and what is a u2s vocoder used for?,answer: Unit-based language models are trained on discrete tokens derived from representations of SSL models. A u2s vocoder is used to map the tokens to speech.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " How does MQTTS differ from other TTS systems in terms of architecture and codebook usage?,answer: MQTTS differs from other TTS systems in the use of multi-code and architecture designed for multi-code generation and monotonic alignment. It learns codebooks specialized for speech synthesis instead of using SSL representations.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What approach is taken to use real-world speech for TTS, and how is denoising incorporated?,answer: An approach to using real-world speech for TTS is to incorporate denoising. Recent work trained the denoising module and the TTS system jointly with external noise data.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the second stage of training for the MQTTS model, and what components are involved?,answer: In the second stage of training for the MQTTS model, a transformer is trained to perform autoregressive generation on discrete codes, conditioned on a speaker embedding and a phoneme sequence.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What architecture is adopted for the Quantizer in MQTTS, and why are modifications made to it?,answer: HiFi-GAN is adopted as the backbone architecture for the Quantizer in MQTTS. Modifications are made to prevent the explosion of quantization loss and training divergence.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " Why does MQTTS utilize multiple codebooks for quantization, and how are the codes generated?,answer: MQTTS utilizes multiple codebooks for quantization as a single codebook results in poor reconstruction of the input signal for real-world speech. The individual codes are generated through a slicing and concatenation process.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}], "doc_text": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}