{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the ground-truth symbol sequence denoted as Ce in the text?,        answer: (c1, . . . , cL, \u27e8eos\u27e9)    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " How do AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol?,        answer: P (Ce|X) = P (Ce|H(X)) = \u03a0_{i=1}^{L+1} P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X))    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " Explain the role of the context vector, vi, in AED models.,        answer: It summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " What is the special significance of the symbol c0 = \u27e8sos\u27e9 in AED models?,        answer: It serves as the first input to the attention-based decoder before any outputs are produced.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " What is the benefit of AED models over models like CTC or RNN-T?,        answer: AED models do not make any independence assumptions between model outputs and the input acoustics.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " Why does using AED models come at a cost?,        answer: Previously generated context vectors are not revised as the decoding proceeds, leading to left-right asymmetry in decoding.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " How is the context vector, vi, computed in AED models?,        answer: By employing the attention mechanism.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " What is the main idea behind content-based attention in the context of attention mechanisms?,        answer: Content-based attention computes the attention score based on the relevance between the encoder output and the decoder state.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " What are some drawbacks of models that rely on explicit alignment like CTC or RNN-T?,        answer: They are non-streaming and sensitive to the length of the acoustic sequences, requiring special processing for long-form audio.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}, {"question": " How does the transformer model handle attention in its architecture?,        answer: The transformer model uses only content-based dot-product attention but incorporates location information through positional encoding.    ", "ref_chunk": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}], "doc_text": "given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, \u27e8eos\u27e9), the ground-truth symbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9 symbol. AED models compute the conditional probability of the output sequence augmented with the \u27e8eos\u27e9 symbol as: P (Ce|X) = P (Ce|H(X)) = L+1 (cid:89) P (ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9 , H(X)) = i=1 L+1 (cid:89) = i=1 L+1 (cid:89) P (ci|si, vi) i=1 where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci\u22121, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label: si = Decoder(vi\u22121, si\u22121, ci\u22121) The symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics, (5) and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55]. 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = 1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then defines a score between the model state after outputting i \u2212 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: \u03b1t,i = the weight \u03b1t,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci\u22121, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights: vi = (cid:88) \u03b1t,iht t A number of possible attention mechanisms have been explored in the literature: the most common forms are called \u2018content-based attention\u2019, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 proposed [15], where fi,j is a convolutional feature vector extracted from \u03b1i\u22121, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads \u2013 v1 i \u2013 which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44]. i , . . . , vK i ; . . . ; vK i D. From Implicit to Explicit Alignment Modeling independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit"}