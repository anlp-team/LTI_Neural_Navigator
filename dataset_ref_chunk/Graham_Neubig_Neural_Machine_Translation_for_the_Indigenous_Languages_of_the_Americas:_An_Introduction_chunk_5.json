{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Neural_Machine_Translation_for_the_Indigenous_Languages_of_the_Americas:_An_Introduction_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is backtranslation and how does it help in low-resource machine translation?", "answer": " Backtranslation is a method where a trained model translates monolingual data into a target language, creating pseudo parallel data for training new models. It helps in leveraging monolingual data to improve translation.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " What is the purpose of multilingual supervised training data in machine translation?", "answer": " Multilingual supervised training data allows training a model to map a sentence from any source language into any target language contained in the training data. This approach has gained popularity and efficiency in recent years.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " What are the three training algorithms for multilingual machine translation models?", "answer": " The three training algorithms are: 1) many source languages and one target language (many-to-one), 2) one source language and many target languages (one-to-many), and 3) many source and target languages (many-to-many).", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " How does multi-task training contribute to improving machine translation performance?", "answer": " Multi-task training aims to enhance the main task of machine translation by adding auxiliary tasks to the training process. This can involve sharing parameters between tasks for improved efficiency.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " What is data augmentation in machine translation and how does back-translation play a role in it?", "answer": " Data augmentation in machine translation involves generating new data to train models. Back-translation, a method of data augmentation, generates pseudo parallel data using a trained model to improve translation.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " Explain the concept of pivoting in machine translation.", "answer": " Pivoting in machine translation is used when no direct parallel corpus exists between two languages, but both have a parallel corpus with a third language. Two machine translation systems are trained: one translating from Language A to Language C, and the other from Language C to Language B.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " What is transfer learning in machine translation and how does it help improve performance?", "answer": " Transfer learning refers to using knowledge from one task to enhance performance on a related task. In machine translation, pretraining models on cross-language representations can lead to improved translation accuracy.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " What is unsupervised machine translation (UMT) and how does it differ from zero-shot translation?", "answer": " Unsupervised machine translation (UMT) does not rely on any parallel text and uses only monolingual data. This is different from zero-shot translation, which utilizes parallel data for other language pairs.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " Why is a big monolingual dataset essential for approaches like unsupervised machine translation (UMT)?", "answer": " A big monolingual dataset is crucial for unsupervised machine translation approaches as they do not have access to parallel text. This dataset helps in training models solely on monolingual data.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}, {"question": " What are some of the challenges in machine translation for indigenous languages of the Americas?", "answer": " Machine translation for indigenous languages of the Americas faces challenges such as the scarcity of resources and data, the need for specialized models, and the complexity of language structures.", "ref_chunk": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}], "doc_text": "machine translation. Figure 1 shows a general overview of the methods and options to improve LRL MT. For a more detailed understand- ing of this techniques we refer the reader to spe- cialized low-resource MT surveys (Haddow et al., 5The complete EGIDS scale can be found at https:// www.ethnologue.com/about/language-status 6Agglutination refers to a concatenation of morphemes, with minimal changes to the surface form. Low amount ofparallel dataCollect new data Parallel datato 3th language Zero-shotMulti-taskDataAugmentationBackTranslationSentenceModificationMultilingualTransfer LearningElicitationAnnotatingcommonlyused speechor textNoisy paralleldocumentsto parallelsentencesPivotingUnsupervisedMT DATANo parallel data Only monolingualdataAdditionalAnnotated data*Monolingual*Parallel data inother languages*Pre-trainedmodels* Any data(Monolingual,Multilingual, etc)*AnnotatorsNoisy ParallelDataSpeechrecordings Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows basic scenarios, solutions, and common requirements for each method, with the section describing the method. \u03b8\u03b8\u03b8 1) One to many2) Many to one3) Many to many L1L2Ln...L1L2Ln...L1Ln...L1Ln... DL1D'L2 L1 \u21fe L2 L2 \u21fe L1 DecodeTrain D'L2DL1 Figure 3: Backtranslation Figure 2: An overview of different multilingual setups. single-language pair models for LRLs. 6.2 Multi-task Training 2022; Wang et al., 2021; Ranathunga et al., 2021). 6.1 Multilingual Supervised Training data With Dparallel pairs {(L1, L2), . . . , (Lm, Ln)} we can train a model that is able to map a sentence from any source language Lx into any target language Ly that is contained in Dparallel (see 2). These multilingual NMT models have seen a growth in popularity and efficiency in recent years. We will now cover the different training algorithms for these models: 1) many source languages and one target language (many-to-one), 2) one source and many languages (one-to-many), and 3) many target source languages and many target languages (many-to-many). For a general overview of multilingual MT, we refer the reader to surveys dedicated to this topic (Tan et al., 2019; Dabre et al., 2019). Johnson et al. (2017) are the first to introduce a multilingual NMT model, trained on translating from a large number of languages to English as well as in the opposite direction. The authors show that these models improve over a multilingual between set different of parallel language Multi-task training (Caruana, 1997) aims to im- prove the performance of the main task \u2013 MT in our case \u2013 by adding one or more auxiliary tasks to the training. The easiest way is to share all pa- rameters of the network, using the ideas already explored in multilingual NMT (\u00a76.1). This can be done with a special flag in the input that specifies the current task. It is also possible to share only the encoder and have two separate decoders for each task. Multilingual Modeling In order to handle mul- tilinguality it is also possible to adapt modify the NMT models. The main proposals to do so has been: sharing all parameter except the attention mechanism of a RNN NMT model (Blackwood et al., 2018); parameter sharing in the transformer architecture Sachan and Neubig (2018); 6.3 Data Augmentation Back-Translation A straightforward way to leverage monolingual data for low-resource MT is to generate a meaningful signal with the help of an already initialized MT model (see Figure 3). This method is called back-translation (BT; Sen- nrich et al., 2016b): With monolingual data M Lx in source language Lx and a trained model that is able to translate from Lx into a target language Ly we can generate a translation M \u2032Ly . This pseudo parallel data (M Lx, M \u2032Ly ) is then used to train a new model in the opposite direction. This process can be applied iteratively to improve the transla- tion (Hoang et al., 2018). Sentence Modification Other methods to gen- erate more parallel sentences are based on lexi- cal substitution. Fadaee et al. (2017) explores re- placing frequent words with low-frequency ones in both source and target to improve the transla- tion of rare words. This is done using language models (LMs) and automatic alignment. Pivoting If no parallel corpus between lan- guages Lx and Ly is available, but both of them have parallel corpora with a third language Lp, pivoting is an option. The basic idea is to train two MT systems: one that translates Lx \u2192 Lp and another for Lp \u2192 Ly. Pivoting has first been introduced for SMT (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007). 6.4 Semi-supervised and Unsupervised MT Transfer Learning via Pretraining Transfer learning refers to using knowledge learned from one task to improve performance on a related task (Weiss et al., 2016). In recent years this approach has gained popularity with big multilingual mod- els such as Conneau and Lample (2019) that pro- poses training the encoder and the decoder sep- arately in order to get cross-language represen- tations (XLM). This idea has further been ex- tended by Song et al. (2019, MASS) to masking a sequence of tokens from the input (multilingual MASS (Siddhant et al., 2020)). Another approach is to train the entire transformer model as a denois- ing autoencoder (BART; Lewis et al., 2019) ( mul- tilingual BART (mBART) (Liu et al., 2020)). It is also possible to pretrain a transformer in a multi- task, text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020) (multilingual version (Xue et al., 2021)). Unsupervised MT UMT covers approaches that do not require any parallel text, relying only on monolingual data. This differs from zero-shot translation, which uses parallel data for other lan- guage pairs. Early approaches tackled the prob- lem with an auto-encoder with adversarial train- ing (Lample et al., 2017) or with auto-encoders with a shared encoding space as well as separate decoders for each target language (Artetxe et al., 2018). The main problem for these approches is the need of a big monolingual dataset, that is not available for most ILA. 7 Advances in MT for the indigenous languages of the Americas In recent years the interest in MT for indigenous languages of the Americas has increased. The task is not easy. The first usage of NMT systems has not"}