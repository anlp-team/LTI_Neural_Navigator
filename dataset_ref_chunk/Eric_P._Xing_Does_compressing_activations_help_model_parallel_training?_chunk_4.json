{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Does_compressing_activations_help_model_parallel_training?_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the compression method C in the 6-Layer Transformer model?", "answer": " The purpose of the compression method C is to reduce the message size for the all-reduce operation to decrease TP communication time.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " What does the de-compression method DC do in the communication process?", "answer": " The de-compression method DC is used after the communication to decompress the message that was compressed.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " In the experiments, which model is used by default for compression of activations?", "answer": " The BERTLarge model with 24 layers is used by default for compression of activations.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " What is the goal of studying different pairs of C and DC in the paper?", "answer": " The goal is to analyze the impact and effectiveness of different compression and de-compression pairs (C and DC) on model parallel training.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " What is the significance of using AE and quantization algorithms for compression?", "answer": " Among all evaluated compression algorithms, only AE and quantization methods preserve fine-tuning accuracy.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " How much accuracy loss is observed when using AE and quantization algorithms for compression?", "answer": " The accuracy loss is within 3% except for CoLA and RTE datasets when using AE and quantization algorithms for compression.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " What is the impact of low-rank activations on sparsi\ufb01cation-based compression algorithms?", "answer": " Sparsi\ufb01cation-based compression algorithms (Top-K/Random-K) lose important information and do not preserve model accuracy if activations are not low-rank.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " How do machine bandwidth differences impact the performance benefits from compression?", "answer": " Machine bandwidth differences can affect the performance benefits from compression, where lower bandwidth links may lead to more throughput benefits from compression methods.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " Why does AE outperform other compression methods in the experiments?", "answer": " AE outperforms other compression methods because it provides more throughput benefits across lower bandwidth links.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}, {"question": " What is the takeaway regarding using non-learning-based compression techniques for compressing activations?", "answer": " Using non-learning-based compression techniques only slightly improves system throughput (by 1% or less) due to the large overhead of these methods.", "ref_chunk": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}], "doc_text": "Does compressing activations help model parallel training? Machine 1,2Machine 3,4 Transformer Layer Transformer Layer Transformer Layer Machine 1Machine 2 Activation Transformer Layer C C Activation C C Transformer Layer DC C DC DC g CTransformer Layer DC DC DC g Micro-batch Figure 3. Illustration of compression on a 6-Layer Transformer model with 4 machines. Machine 1 and Machine 2 maintain the \ufb01rst three layers according to the TP strategy (pipeline stage 1). g stands for an all-reduce operation in the forward pass. A compression method C is used to reduce the message size for the all-reduce operation to reduce TP communication time. Correspondingly, a de-compression method DC is used after the communication. For instance, if AE are used, then C is an encoder, and DC is a decoder. Machine 3 and Machine 4 are responsible for the last three layers (pipeline stage 2). A compression method is used before Machine 1 sends the activation to Machine 3, and before Machine 2 sends the activation to Machine 4 to reduce PP communication time. The goal of this paper is to study the effect of different pairs of C and DC. number of the tuple represents the tensor model-parallel degree and the second number of the tuple represents the pipeline model-parallel degree. Notation A1 Description AE with encoder output dimension 50 We also evaluate compression algorithms with different parameters. For AE, we use different dimension after com- pression from {50, 100}. For Top-K and Random-K algo- rithms, we use two comparable settings: (1) Keep the same compression ratio as AE (i.e., we compress the activation around 10 and 20 times.) (2) Keep the same communica- tion cost as AE. Finally, we also tune the parameters for quantization and compress the activation to {2, 4, 8} bits. A2 T1/R1 T2/R2 T3/R3 T4/R4 Q1 Q2 AE with encoder output dimension 100 Top/Rand-K: same comm. cost as A1 Top/Rand-K: same comm. cost as A2 Top/Rand-K: same comp. ratio as A1 Top/Rand-K: same comp. ratio as A2 Quantization: reduce the precision to 2 bits Quantization: reduce the precision to 4 bits By default, we perform experiments on BERTLarge model with 24 layers and compress the activation for the last 12 layers. For instance, when the pipeline model-parallel de- gree is 2 and tensor model-parallel degree is 2, we compress the activation between two pipeline stages and the communi- cation cost over tensor parallelism in the last 12 layers. We also vary the number of layers compressed in Section 4.5. TP Tensor model-parallelism degree PP Pipeline model-parallelism degree Table 1. Notation Table. For ease of notation, we use TP/PP to denote the degree of tensor/pipeline model parallelism. \u2018comm\u2019 and \u2018comp\u2019 are short for \u2018communication\u2019 and \u2018compression\u2019. 4.2 Throughput Bene\ufb01ts for Fine-Tuning 17.8% when using learning-based compression methods on a machine without NVLink. Takeaway 1 Using non-learning-based compression tech- niques to compress activations only slightly improves system throughput (by 1% or less) due to the large overhead of these methods. However, we see end-to-end speedups of up to When running \ufb01ne-tune experiments on a p3.8xlarge in- stance on Amazon EC2, we cannot improve system through- put by using non-learning-based compression algorithms (Table 2). Comparing Tables 2 and 3, we can see that the net- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=1, PP=4 591.96 591.36 591.47 594.81 595.53 599.65 605.05 TP=2, PP=2 440.71 437.98 444.02 465.73 473.64 493.16 528.93 TP=4, PP=1 261.48 270.22 275.54 314.37 323.90 356.57 409.23 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=1, PP=4 591.96 749.56 1,008.64 1,824.36 5,572.87 595.29 595.45 TP=2, PP=2 440.71 3,377.59 6,616.30 17,117.01 71,058.64 489.27 486.54 TP=4, PP=1 261.48 3,254.01 6,561.22 16,990.88 65,121.79 347.68 350.50 Table 2. The average iteration time (ms) for \ufb01ne-tuning with various compression techniques by varying the distributed setting. The results are collected from the AWS p3.8xlarge machine with NVLink by using batch size 32, and sequence length 512. The best setting is bolded in the table. And the settings which see bene\ufb01ts compared with the baseline, are underlined. With NVLink w/o A1 A2 4.3 Effect of Compression on Model Accuracy while Fine-tuning TP=1, PP=4 TP=2, PP=2 591.96 440.71 591.36 437.98 591.47 444.02 Takeaway 2 Among all evaluated compression algorithms, only AE and quantization preserve \ufb01ne-tuning accuracy. TP=4, PP=1 261.48 270.22 275.54 Without NVLink w/o A1 A2 TP=1, PP=4 633.17 620.10 620.44 TP=2, PP=2 646.14 586.65 595.25 TP=4, PP=1 736.01 624.62 636.15 Table 3. The average iteration time (ms) for \ufb01ne-tuning with/without NVLink. We compare time without compression and with AE on different distributed settings, with batch size 32, and sequence length 512. The best setting on each machine is bolded. And the settings, under which we can gain bene\ufb01ts com- pared with the baseline, are underlined. From Table 5, we can see that, when using AE and quan- tization algorithm for compression, the accuracy loss is within 3% except for CoLA and RTE. In Figure 2, we have shown that the activation for models is not low-rank. There- fore, sparsi\ufb01cation-based compression algorithms (Top- K/Random-K) lose important information and do not pre- serve model accuracy. Given that there is signi\ufb01cant accu- racy difference for CoLA and RTE, we study the impact of varying the number and range of layers compressed for these two datasets in Section 4.5. 4.4 Throughput Bene\ufb01ts for Pre-training Takeaway 3 Only AE and Top-K algorithms improve throughput when performing distributed pre-training. work bandwidth across the GPUs can affect the performance bene\ufb01ts from compression. In other words, we can improve system throughput by at most 17.8% when compressing activation for \ufb01ne-tuning tasks on a 4-GPU machine without NVLink. That\u2019s because, without NVLink, the communica- tion time for model parallelism is much longer. Thus, while the message encoding and decoding time remain unchanged, compression methods can provide more throughput bene\ufb01ts across lower bandwidth links. Furthermore, from Tables 2 and 3, we observe that AE out- performs other compression methods. In Table 4, we break- down the time taken by each algorithm and \ufb01nd that"}