{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Multi-lingual_and_Multi-cultural_Figurative_Language_Understanding_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of translating test sets to English in the text?", "answer": " To discern the gap in performance due to cross-lingual transfer and concept shift in metaphors.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " Why are results for Yoruba not reported for XLM-R in the text?", "answer": " Because the XLM-R model was not trained on any Yoruba data.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " How is the performance of GPT-3 described when tested zero-shot in the text?", "answer": " Not very good on most languages, but with a reasonable zero-shot performance on the English development set.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " What does a higher concept shift gap indicate for a model and language in the text?", "answer": " It indicates a more significant challenge for the model and language.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " What procedure is followed to predict using GPT-3 and BLOOM-176B in the text?", "answer": " The standard procedure of choosing the answer with a higher predicted probability.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " What are the four cases into which the model performance is divided in the text?", "answer": " Easy examples, linguistically challenging examples, difficult-to-translate or incorrectly translated examples, hard examples.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " What does the Effect of English MT in the text refer to?", "answer": " It refers to the effect of machine translation on cross-lingual transfer.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " What is the Strategy most beneficial for XLM-Rlarge with more than 10 examples in the target language in the text?", "answer": " Adding up to 50 examples in the target language to the English training data.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " What is the main focus of research on figurative language discussed in the text?", "answer": " Understanding metaphors and linking metaphorical phrases to their literal meanings through paraphrase detection or generation.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}, {"question": " Why is there limited research on understanding metaphors according to the text?", "answer": " Most research focuses on detecting the presence of metaphors or paraphrasing them, rather than inferring their correct meanings.", "ref_chunk": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}], "doc_text": "9.26 17.09 17.40 13.93 7.31 - mBERTbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 70.88 \u00b12.46 51.32 \u00b10.94 56.56 \u00b11.66 55.06 \u00b11.70 52.63 \u00b11.15 52.87 \u00b11.67 52.12 \u00b11.09 50.52 \u00b11.04 70.88 \u00b12.46 59.45 \u00b11.77 63.30 \u00b11.12 60.76 \u00b12.31 56.70 \u00b10.77 59.37 \u00b12.37 63.57 \u00b10.78 50.60 \u00b11.28 0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08 0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28 text-davinci-003 en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27 74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77 0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51 0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10 Table 8: Averaged zero-shot evaluation \u00b1 standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set\u2019s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data. examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English. fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9. Translate-EN . Correct g i r Incorrect O Correct 53.06% 19.09% Incorrect 15.52% 12.33% 6 Error Analysis Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined. 6.1 Effect of English MT As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans- First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti- XLM-R Base su Accuracy Change (%) kn 4 0 sw 24681020304050Number of examples 24681020304050Number of examples 24681020304050Number of examples hi jv id XLM-R LargeLanguage 2 2 yo mBERT Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F. cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It\u2019s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G. 6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture. 7 Related Work 7.1 Figurative Language English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022\u037e Liu et al., 2022\u037e Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020\u037e Stowe and Palmer, 2018\u037e 11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult. Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model\u2019s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010\u037e Mao et al., 2018). Some studies investigate LMs\u2019 ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b\u037e Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding. Extension to Multilingual: Research in corpus linguistics (D\u00edaz-Vera and Caballero, 2013\u037e K\u00f6vec- ses, 2004\u037e Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013\u037e Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors"}