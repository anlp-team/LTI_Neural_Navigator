{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Value_Kaleidoscope:_Engaging_AI_with_Pluralistic_Human_Values,_Rights,_and_Duties_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the VALUEPRISM Dataset Statistics table show?,answer: The total, number of unique, and average number of generated values, rights, and duties per situation.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " According to the Task Dataset Statistics table, how many total entries are there?,answer: 1.1 million.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " What is the effect of dataset mixture on performance, according to the text?,answer: All tasks except relevance benefit from a mixture, suggesting that the tasks are complementary.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " How does model size affect performance?,answer: For all tasks, larger models perform better. Perplexity improves steadily with model size, and classification accuracies see a large boost with larger models.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " What is the Statistics of 30k situations sourced from the Delphi user demo table showing?,answer: It shows the count and percentage of unique entries or n-grams for situation values.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " What are the top 3 opposing values when identifying hate speech?,answer: Respect, Equality, and Tolerance.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " How many annotations were collected in the CloudResearch Results study?,answer: 31,000 annotations.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " What percentage of the annotators agree with the values, rights, and duties on average?,answer: 81%.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " What do some non-null responses in the CloudResearch Results study mention?,answer: That the person has a missing perspective.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}, {"question": " What does ANOVA testing in the CloudResearch Results study aim to analyze?,answer: It aims to analyze each demographic group.", "ref_chunk": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}], "doc_text": "co-occurrs with situations labeled as good and self-reliance and opposing value. Type Total Unique Per Situation Situations Values Rights Duties 31.0k 97.7k 49.0k 71.6k 31.0k 4.2k 4.6k 12.8k 1 3.15 1.58 2.31 Table 6: VALUEPRISM Dataset Statistics. The total, number of unique, and average number of generated values, rights, and duties per situation are shown. Relev. Valence Gen. Expl. Mixture Train Val Test 349k 44k 44k 175k 22k 22k 175k 22k 22k 175k 22k 22k 874k 109k 109k Total 437k 219k 219k 219k 1.1M Table 7: Task Dataset Statistics D Additional Experiments D.1 Ablated performance on VALUEPRISM test data We measure model performance against VALUEPRISM\u2019s test set in order to understand how model sizes and dataset mixtures interact with performance in Table 12. What is the effect of dataset mixture on performance? Our base model was trained with a mixture of all four task. We find that all tasks except relevance are benefited from a mixture as opposed to training a separete model for each, suggesting that the tasks are complementary. As we ablate each task out of the mixture individually, we see minimal changes in performance across all tasks, suggesting that no one task is crucial to the gain in performance seen from mix- ing. What is the effect of model size on performance? For all tasks, larger models perform better. Perplexity improves steadily with model size, whereas classification accuracies (Relevance and Valence) see a large boost going from 60M to 220M parameters. As there are not large performance gains in going from the 3B to the 11B model ( 1% accuracy and 0.01-0.15 perplexity), we think that the 3B model has a good trade-off between performance and computational cost. Data Type Entries # % 2-grams # % 3-grams # % Situation Value 30,513 97.3 20,923 40.1 66,802 36.8 20,489 26.9 98,696 65.6 26,259 47.6 Table 8: Statistics of 30k situations that we source from the Delphi user demo. # and % indicate the count and percent- age of unique entries or n-grams, respectively. Our data con- tains diverse entries with high lexical variations. # examples Summary of the cluster 732 stealing bread in order to alleviate hunger and starvation in various situations. 81 donating or giving money, resources, or effort to charity 77 the act of killing or saving mosquitoes 68 the act of killing a bear 68 the ethical dilemma known as the \u201ctrolley problem\u201d 66 saving someone\u2019s life 65 the interaction and involvement with cats 64 the act of ignoring a phone call for various reasons 62 lying to friends in order to protect their feelings, avoid hurting them, or preserve the friendship 62 physical violence or the act of punching someone Table 9: Top 10 clusters with the most examples based on agglomerative clustering on situations. # examples Summary of the cluster 177 the duty or responsibility to promote and protect the welfare of various entities. 158 the duty or responsibility to ensure safety, both for oneself and for others 87 the concept of respect for autonomy 83 the duty and responsibility towards family 82 well-being, specifically human well-being 81 the right to property 71 the duty to promote, maintain, uphold, and protect peace at various levels 69 the duty and responsibility towards the community 67 the duty or responsibility to protect and care for children 64 the duty to treat others with respect, equality, fairness, impartiality, kindness, and compassion Table 10: Top 10 clusters with the most examples based on agglomerative clustering on values. D.2 System performance ablations on VALUEPRISM-Test Similarly, we also compare the outputs of different sized sys- tems with Rouge-score against the GPT-4 outputs (See Table 13). D.3 Values manifested in identifying hate speech We run KALEIDOSYS on Social Bias Frames (Sap et al. 2020), a dataset containing instances of online speech, some of which is labeled as hate speech and some of which is not. We look at the most frequent values generated, and find that the most common opposing values are Respect (for oth- ers), Equality, Tolerance, and Right to Privacy, while the top supporting values are Freedom of speech (or expression), Humor, Honesty, and Right to freedom of speech. These val- ues represent 17% and 26% of the generated oppose/sup- port value counts, respectively. Even though KALEIDO is not trained explicitly to recognize hate speech, it is able to surface values that are violated by hate speech, along with values that run counter to excessive speech moderation. E CloudResearch Results In our study, we collect 31k annotations from 612 annotators across 683 values, rights, and duties in the context of 100 situations. The annotators mark 1) each value for whether or not they agree with it and 2) whether they have an opin- ion or perspective that is missing from the data for a given situation. Results are found in Tables 18 and 20. We find that annotators agree with the values, rights, and duties 81% of the time on average, and state that they have a missing perspective 30% of the time. Note that this is less agreement and more missing perspectives than we saw for the quality annotation. This is not surprising to us, as some annotators may consider a value output high-quality and rea- sonable according to someone, even though they may not agree with it themselves (a much more subjective measure). Additionally, people were allowed to list missing perspec- tives in a free-form text box. Responses are hand-coded by the authors as having content or not, and the variable \u201dhas a missing perspective\u201d is binarized. We find that many of the non-null responses merely state that the person has a missing perspective, not what it is (e.g., \u201dYes\u201d) or do not map cleanly onto the values, rights, and duties framework (e.g., \u201ddo what is correct\u201d, \u201dTake care of orphan is not wrong\u201d). This high- lights a weakness of the framework: not all perspectives fit neatly into it. We conduct 2 statistical analyses on the data. First, with ANOVA testing for each demographic group, we"}