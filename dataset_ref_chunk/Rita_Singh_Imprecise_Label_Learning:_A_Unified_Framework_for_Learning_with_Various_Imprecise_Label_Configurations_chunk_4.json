{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the primary focus of the text?,answer: The text primarily focuses on learning with imprecise labels.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " What does the unified framework for learning with imprecise labels aim to achieve?,answer: The unified framework aims to consider all possible labeling scenarios and their likelihood to train the model, rather than using a single rectified label.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " How is imprecise label information I viewed in the context of the text?,answer: Imprecise label information I is viewed as a variable representing information about the labels, not as the labels themselves.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " What distribution does P(Y|I) have when I contains the precise true labels of the data?,answer: P(Y|I) would be a delta distribution, taking a value of 1 at the true label and 0 elsewhere.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " What principle guides the estimation of the model in the text?,answer: The maximum likelihood estimation (MLE) principle guides the estimation of the model to maximize the likelihood of the data/information provided.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " What algorithm is popularly used for maximizing a tight variational lower bound on the log-likelihood?,answer: The expectation-maximization (EM) algorithm is popularly used for this purpose.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " What does the expectation over the posterior P(Y|X, I; \u03b8t) entail?,answer: The expectation considers all labeling entailed by the imprecise label information I, rather than any single choice of label.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " How is the property of the term log P(I|X, Y ; \u03b8) affected by the nature of the imprecise label I?,answer: The property of the term is dependent on the nature of the imprecise label I. It can be reduced to P(I|Y) if I contains information about the true labels Y.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " What is the general framework described in the text towards unifying label configurations?,answer: The general framework aims to unify any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}, {"question": " How does the text connect the unified EM framework with prior arts and learning paradigms?,answer: The text demonstrates that learning objectives derived from the unified EM framework naturally include a consistency term with the posterior as the soft target, connecting with prior arts and learning paradigms.", "ref_chunk": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}], "doc_text": "consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise label configurations and provides a unified solution instead. A unified framework for learning with imprecise labels (ILL). Let X = {xi}i\u2208[N ] represent the features and Y = {yi}i\u2208[N ] represent their precise labels for the training data. Ideally, Y would be fully specified for X. In the imprecise label scenario, however, Y is not provided; instead we obtain imprecise label information I. We view I not as labels, but more abstractly as a variable representing the information about the labels. From this perspective, the actual labels Y would have a distribution P (Y |I). When the information I provided is the precise true labels of the data, P (Y |I) would be a delta distribution, taking a value 1 at the true label, and 0 elsewhere. If I represents partial labels, then P (Y |I) would have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy labels, P (Y |I) would represent the distribution of the true labels, given the noisy labels. When I does not contain any information, i.e., unlabeled data, Y can take any value. By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the data/information we have been provided, namely X and I. Let P (X, I; \u03b8) represent a parametric form for the joint distribution of X and I 3 Explicitly considering the labels Y , we have P (X, I; \u03b8) = (cid:80) Y P (X, Y, I; \u03b8). The maximum likelihood principle requires us to find: \u03b8\u2217 = arg max log P (X, I; \u03b8) = arg max log (cid:88) P (X, Y, I; \u03b8), \u03b8 \u03b8 Y with \u03b8\u2217 denotes the optimal value of \u03b8. Eq. (4) features the log of an expectation and cannot generally be solved in closed form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm (Dempster et al., 1977), which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes: \u03b8t+1 = arg max EY |X,I;\u03b8t [log P (X, Y, I; \u03b8)] \u03b8 = arg max EY |X,I;\u03b8t [log P (Y |X; \u03b8) + log P (I|X, Y ; \u03b8)] , \u03b8 where \u03b8t is the tth estimate of the optimal \u03b8. The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior P (Y |X, I; \u03b8t) equates to considering all labeling entailed by the imprecise label information I, rather than any single (possibly corrected) choice of label. The computing of the posterior conditioned on imprecise label information I can be related to a non-deterministic finite automaton (NFA) (Hopcroft et al., 2001); details are in Appendix C.3. (ii) The property of the second term log P (I|X, Y ; \u03b8) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or \u03b8 and thus can be ignored from Eq. (5). If I represents the noisy labels, P (I|X, Y ; \u03b8) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize this solution to partial label learning, semi-supervised learning, noisy label learning, and the mixture of them below. 3The actual parameters \u03b8 may apply only to some component such as P (Y |X; \u03b8) of the overall distribution; we will nonetheless tag the entire distribution P (X, I; \u03b8) with \u03b8 to indicate that it is dependent on \u03b8 overall. 5 (4) (5) Preprint 3.2 INSTANTIATING THE UNIFIED EM FORMULATION We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi- supervised learning, and noisy label learning, and derive the loss function4 for each setting here. The actual imprecise labels only affect the manner in which the posterior P (Y |X, I; \u03b8t) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the unified framework to three learning paradigms here, it is flexible and can expand to learning with a mixture of imprecise labels with robust performance, as we will show in Appendix C.2 and Section 4.4. Partial label learning (PLL). The imprecise label I for partial labels is defined as the label candidate sets S = {si}i\u2208[N ] containing the true labels. These partial labels indicate that the posterior P (Y |X, S; \u03b8t) can only assign its masses on the candidate labels. Since S contains the information about the true labels Y , P (S|X, Y ; \u03b8) reduces to P (S|Y ), and thus can be ignored, as we discussed earlier. Substituting S in Eq. (5), we have the loss function of PLL derived using ILL: LPLL ILL = \u2212 (cid:88) P (Y |X, S; \u03b8t) log P (Y |X; \u03b8) = LCE (cid:0)p(y|As(x); \u03b8), p(y|Aw(x), s; \u03b8t)(cid:1) , Y \u2208[C] where p(y|Aw(x), s; \u03b8t) is the normalized probability that (cid:80) k\u2208C pk = 1, and pk = 0, \u2200k \u2208 s. Eq. (6) corresponds exactly to consistency regularization (Xie et al., 2020a), with the normalized predicted probability as the soft pseudo-targets. This realization on PLL shares"}