{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Paaploss:_A_Phonetic-Aligned_Acoustic_Parameter_Loss_for_Speech_Enhancement_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the proposed learning objective in the text?", "answer": " The main focus is to formalize differences in perceptual quality in speech enhancement models using domain knowledge of acoustic-phonetics.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " How are non-differentiable temporal acoustic parameters dealt with in the text?", "answer": " A neural network estimator is developed to accurately predict the values of non-differentiable temporal acoustic parameters across an utterance.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " What impact does adding phoneme-specific weights for each feature have in the proposed method?", "answer": " Adding phoneme-specific weights helps optimize speech outputs to match clean speech values by considering the different behavior of acoustic parameters in different phonemes.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " What is the goal of incorporating domain knowledge through fundamental speech features in the text?", "answer": " The goal is to address problems with optimization of DNNs by using acoustic parameters that provide critical information about frequency content, energy/amplitude, and other spectral qualities of speech signals.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " How has single-channel speech enhancement improved in the last decade according to the text?", "answer": " Single-channel speech enhancement has greatly improved by transitioning from traditional signal processing techniques to deep neural networks (DNN) which provide better performance and flexibility.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " What limitations are mentioned regarding classic losses used in training single-channel SE models in the text?", "answer": " Classic losses fail to capture pitch, show relatively low improvement for low-energy phonemes, and are not highly correlated with speech quality at the signal level.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " What is the purpose of introducing a phonetic-aligned acoustic parameter loss in the text?", "answer": " The purpose is to improve speech outputs by minimizing the difference between phonetically-aligned acoustic parameters in enhanced speech and clean speech.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " How does the two-step approach in the proposed method work to improve speech outputs?", "answer": " The two-step approach involves introducing a differentiable estimator of temporal acoustic parameters and calculating phoneme-specific weights for each parameter to optimize the model end-to-end to match clean speech values.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " What is the significance of emphasizing the interpretability aspect of the proposed method in the text?", "answer": " Emphasizing interpretability helps demonstrate the improvements made by the method and allows for a detailed analysis of the phoneme-dependent enhancement on acoustic parameters.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}, {"question": " What are some common applications affected by noisy environments as mentioned in the text?", "answer": " Applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks are commonly affected by noisy environments.", "ref_chunk": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}], "doc_text": "3 2 0 2 b e F 6 1 ] D S . s c [ 1 v 5 9 0 8 0 . 2 0 3 2 : v i X r a PAAPLOSS: A PHONETIC-ALIGNED ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT Muqiao Yang1, Joseph Konan1, David Bick1, Yunyang Zeng1, Shuo Han1, Anurag Kumar2, Shinji Watanabe1, Bhiksha Raj1 1 Carnegie Mellon University, 2Meta Reality Labs Research ABSTRACT Despite rapid advancement in recent years, current speech en- hancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowl- edge of acoustic-phonetics. We identify temporal acoustic parame- ters \u2013 such as spectral tilt, spectral \ufb02ux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-speci\ufb01c weights for each feature, as the acoustic parameters are known to show different behavior in differ- ent phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement work\ufb02ows in both time- domain and time-frequency domain, as measured by standard eval- uation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement. Index Terms\u2014 Speech Enhancement, Acoustic parameters, Phonetic alignment 1. INTRODUCTION Speech enhancement (SE) tries to extract clean speech from signals that have been degraded mainly by noise. The ability to remove noise from speech is extremely useful, as noisy environments com- monly affect applications such as VoIP and phone calls, hearing aids, and downstream speech processing tasks. Our focus is on the more ubiquitous single-channel speech enhancement which does not re- quire multi-microphone speech capture. Other approaches have sought to address these issues, including optimization of perceptual evaluation metrics. However, these are non-differentiable, so approximations offer limited improvements [13], [14], require cumbersome optimization [14], [15] or offer lit- tle to no interpretability through domain knowledge [16]. We aim to address these problems in this paper and we try to accomplish it by incorporating domain knowledge through fundamental speech features which we refer to as acoustic parameters. Before the rise of DNNs, features such as pitch, jitter, shim- mer, spectral tilt \u2013 to name a few \u2013 were used as inputs to shallow models, such as in speaker and emotion recognition [17]. They lost popularity as DNNs gained more success operating directly on wave- forms or spectrograms. Their non-differentiable computations also inhibit their straightforward use in optimization of DNNs. Neverthe- less, these parameters provide critical information about frequency content, energy/amplitude, and other spectral qualities of the speech signal. Prior perceptual studies have shown important associations of these features to voice quality [18]\u2013[20]. [21] introduced a differ- entiable estimator of utterance-level statistics for these parameters and improved state-of-the-art SE models through an auxiliary loss aimed to minimize the differences between parameter values of clean and enhanced speech. Similarly, we work with 25 acoustic param- eters enumerated in the extended Geneva Minimal Acoustic Param- eter Set [17]. However, unlike prior work which considered these acoustic parameters at the global (utterance) level through summary statistics, we incorporate the temporal aspects of these acoustic pa- rameters [22]. Furthermore, we incorporate the associations between acoustic parameters and phonemes which have been studied previ- ously in the sub-\ufb01eld of acoustic-phonetic. For example, plosives typically have a high amplitude followed by a very low amplitude, as they are produced by complete closure in the vocal tract followed by a sudden release of pressure [23]. Nasality in sounds introduces anti-formants because the nasal cavity introduces resonances that in- terfere with the resonances of the vocal tract [24]. Each vowel also has different formant structures based on the resonances created by different locations of constriction in the vocal tract [25]. In the last decade, single-channel SE has greatly improved by moving from traditional signal processing techniques to deep neu- ral networks (DNN) [1]\u2013[5]. Deep Noise Suppression (DNS) chal- lenges have further stimulated single-channel SE work by providing a large corpus of audio synthesized over a wide range of noise types and levels [6], [7]. It also provides a common test set to measure performance. Single-channel SE models are usually trained by comparing en- hanced speech to clean speech using point-wise differences between waveforms or spectrograms. While this paradigm has been effec- tive, SE models often still generate unnatural sounding speech [8]. Limitations with these classic losses include failure to capture pitch [9], and relatively low improvement for low-energy phonemes [10]. Additionally, [11] and [12] describe that (cid:96)1 or (cid:96)2 difference at the signal level is not highly correlated with speech quality. In this paper, we introduce a phonetic-aligned acoustic param- eter (PAAP) loss to improve speech outputs from SE systems. We accomplish this by minimizing the difference between phonetically- aligned acoustic parameters in enhanced speech and clean speech. This is done with a two-step approach. First, we introduce a dif- ferentiable estimator of temporal acoustic parameters, to obtain the time series of each parameter across an utterance. Second, we cal- culate differentiable phoneme-speci\ufb01c weights for each acoustic pa- rameter based on their ability to predict phoneme logits. This al- lows us to put different emphases on acoustic parameters at one time step, depending on the predicted phoneme at the same time step. These two components allow us to optimize the original model end- to-end to match clean speech with phonetic-aligned acoustic parame- ters. Our approach leads to improvements over competitive SE mod- els. More importantly though, we demonstrate the interpretability of our method, by analyzing the phoneme-dependent improvement on acoustic parameters. 2. RELATED WORK Various works have tried to introduce losses aimed at improving the perceptual quality. Some techniques include optimization of non- differentiable perceptual metrics through generative adversarial net-"}