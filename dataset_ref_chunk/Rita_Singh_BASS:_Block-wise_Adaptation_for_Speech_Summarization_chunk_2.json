{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_BASS:_Block-wise_Adaptation_for_Speech_Summarization_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main contribution of the paper regarding speech summarization?,answer: The main contributions include introducing Block-wise Adaptation for Speech Summarization (BASS) and an explicit layer of semantic representation.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " How does the BASS algorithm predict speech summaries?,answer: BASS predicts a speech summary after consuming a new block of input speech and allows modification of summaries if necessary.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " What is the goal of summarization mentioned in the text?,answer: The goal of summarization is to produce a summary token sequence shorter than the original sequence but containing relevant semantic information.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " How is the input speech sequence represented in terms of blocks?,answer: The input speech sequence can be represented as a sequence of abutting blocks with a specified block size.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " What does the chain rule of probability imply for the model being described?,answer: The chain rule implies that the model is causal (streaming), meaning present output cannot depend on future outputs or inputs.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " What is the general decomposition based on the chain rule and the streaming assumption?,answer: The general decomposition involves maximizing the probability of block-level output being close to the ground-truth summary given past and current block inputs.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " How is the Viterbi assumption used in the final formulation?,answer: The Viterbi assumption is used to optimize the block-level output sequence with the highest probability as context for future predictions.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " What is the purpose of backpropagation in the described process?,answer: Backpropagation is performed to update neural network parameters after every block by computing the divergence between the block-level output and the ground-truth summary.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " What is the semantic representation variable introduced in the BASS model?,answer: The semantic representation variable is S, which comprises F-dimensional vectors encoding semantic information in the speech.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}, {"question": " What are the three distinctive spaces mentioned in the text for speech summarization?,answer: The three distinctive spaces are the acoustic space, semantic space, and summary space.", "ref_chunk": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}], "doc_text": "acoustics may not be as informative, and output summaries could be erroneous or change entirely with new blocks. In summary, this paper makes the following contributions: 1. We introduce Block-wise Adaptation for Speech Summariza- tion (BASS), a novel algorithm for training speech summa- rization models. BASS predicts a speech summary after con- suming a new block of the input speech and allows new sum- maries to be modified fully if necessary. 2. We introduce an explicit layer of semantic representation, which aggregates semantics from the input acoustics and is not affected by how it is expressed. We then describe mech- anisms to carry this semantic context across blocks for adap- tation and training. 3. We evaluate the relative strengths of block-wise adapta- tion from a pretrained model and block-wise training from scratch, and show that BASS improves performance under adaptation settings by 3 points on ROUGE-L. 2. Proposed Approach 2.1. Formulating Block-wise Training Given a long audio instance with N frames of D-dimensional speech features X = (xi \u2208 RD|i = 1, 2, \u00b7 \u00b7 \u00b7 , N ), the goal of summarization is to produce a summary token sequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 yL] of length L, which is shorter than the original sequence but still contains the relevant semantic information. The input sequence X can also be represented as a se- quence of T abutting blocks with block size B such that X = {X 1, X 2, ...X T }. The i-th input block X i produces a block- level output \u02c6Y i, which is the model hypothesis for the full ref- erence Y . We use the notation X 1:T to represent X 1, \u00b7 \u00b7 \u00b7 , X T and Y 1:T to represent Y 1, \u00b7 \u00b7 \u00b7 , Y T . The goal of a block-wise model is to generate the best possi- ble summary \u02c6Y T after seeing the T blocks of input. Equation 1 expresses the probability of observing the final output sequence Y T given the input blocks X 1:T based on the joint conditional density P(Y T , Y 1:T \u22121|X 1:T ). P(Y T |X 1:T ) = (cid:88) \u00b7 \u00b7 (cid:88) P(Y T , Y 1:T \u22121|X 1:T ) Y 1 Y T \u22121 Using the chain rule of probability, we can represent the inner term P(Y T , Y 1:T \u22121|X 1:T ) as shown in Equation 2. (1) P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T ) (2) Based on the fact that the model is causal (streaming), the present output cannot depend on future outputs or inputs. This implies P(Y 1:T \u22121|X 1:T ) = P(Y 1:T \u22121|X 1:T \u22121). Combining this with Equation 2 results in Equation 3. P(Y 1:T |X 1:T ) = P(Y T |X 1:T , Y 1:T \u22121)P(Y 1:T \u22121|X 1:T \u22121) (3) This leads to the following general decomposition based on the chain rule and the streaming assumption, shown in Equation 4. P (Y 1:T |X 1:T ) = P (Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 P (Y 1|X 1) (4) Consider Equation 1 which involves marginalizing over the out- put variables Y 1:T \u22121, and is challenging to compute. Rather than evaluating all possible values for past context Y 1:T \u22121, we can perform this optimization in a greedy manner, i.e., by choosing the block-level output sequence with the highest prob- ability as context for future predictions. Combining this Viterbi assumption with Equations 1 and 4 leads us to the final formu- lation in Equation 5. P(Y T |X 1:T ) \u2248 max Y T P(Y T |X 1:T , Y 1:T \u22121) \u00b7 \u00b7 \u00b7 max Y 1 P(Y 1|X 1) (5) In summary, Equation 5 demonstrates a setup wherein after re- ceiving a new block of input, we maximize the probability of the block level output being as close as possible to the ground-truth summary given past and current block inputs and past block level outputs. In practice, we take in a block of input and any context from the past, then we compute a divergence between the block-level output and the ground-truth summary. We per- form backpropagation with this criterion to update the neural network parameters after every block. Apart from the aforementioned assumptions, we can also make the Markov assumption while modeling contextual depen- dence. To minimize the impact of context from further away blocks on the current block, we can rewrite P(Y 1:i|X 1:i) = P(Y i\u2212M :i|X i\u2212M :i). 2.2. Modeling Strategy and Architecture The proposed BASS model is shown in Figure 1. Different from past work in summarization, we explicitly introduce a semantic representation variable S = (si \u2208 RF |i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , M ), which comprises M F -dimensional vectors. S contains the semantic information encoded in the speech X, and separates the acoustics from the summary. Modifying the input language or ambient environment changes the acoustics, but not the se- mantics. Summaries are generated by sampling from this rich semantic representation, and modifying S leads to a different summary Y . The process of speech summarization occurs at the intersec- tion of three distinctive spaces- the acoustic space, the semantic space, and the summary space. The acoustic space comprises the acoustic input X, which it transforms into semantic rep- resentations. The summary Y can be produced in the summary space by sampling based on the semantic representations S. We can reasonably assume that the acoustics and summary are con- ditionally independent given the semantics, and thus disentan- gle the acoustics and the summary. Consider the task of estimating the most likely summary Y given the input speech X under this setting. Figure 2: Updater mechanisms (a) concatenation, (b) gated at- tention, and (c) hierarchical attention use previous embedding Si\u22121 and encoding Enc(X i) to produce current embedding \u02c6Y = arg max P(Y |X) Y ="}