{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ML-SUPERB:_Multilingual_Speech_Universal_PERformance_Benchmark_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the importance of limiting the training size in speech SSL works?", "answer": " Limiting the training size is important to keep the experiments within reasonable computational efforts.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " How long can a full evaluation cycle of ML-SUPERB take using 4 2080Ti GPUs?", "answer": " Up to 3 days.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is the purpose of reserving 20 languages for few-shot learning scenarios in the training sets?", "answer": " To directly predict the correct orthography in the target language.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is the focus of the LID track?", "answer": " Language identification with the same training set of 143 languages in 10 minutes and 1 hour.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is a technique commonly used in previous literature for joint training of multilingual ASR and LID models?", "answer": " Adding the language ID to the start of the speech transcript.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " How does joint training of multilingual ASR and LID models improve performance?", "answer": " It can improve performance in certain scenarios and enhance model interpretability by separating language identification errors.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is the downstream model design based on in ML-SUPERB?", "answer": " The SUPERB concept.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is the learning rate used in the training of the downstream model?", "answer": " 0.0001.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is the batch size set for the downstream model training?", "answer": " 8 with gradient accumulation as 4.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}, {"question": " What is the evaluation metric used for the multilingual track?", "answer": " Character Error Rate (CER) for ASR evaluation and accuracy rate for LID evaluation.", "ref_chunk": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}], "doc_text": "which can help evaluate their robustness and generalization capability. (2) Reasonable performance: previous speech SSL works have frequently adopted 10-minute and 1-hour training sizes. Even in such extreme cases, the performances with SSL are generally reasonable [3, 4], indicating that this setting could be a feasible solution to the benchmark as well. (3) Training ef- ficiency: with 143 languages coverage, limiting the training size is important to keep the experiments within reasonable compu- tational efforts. Using a smaller training set size can help reduce the computational cost and make the training process more effi- cient. A full evaluation cycle of ML-SUPERB can take up to 3 days using 4 2080Ti GPUs. 10-minute train set and the 1-hour train set. For both training sets, we reserve 20 languages for few-shot learning scenarios as discussed in Sec. 2.1. In this track, the model is expected to directly predict the correct orthography in the target language. LID task: LID track focuses on language identification with the same training set of 143 languages in 10 minutes and 1 hour. However, we do not consider evaluation for languages with few- shot settings, given that the identification of those languages is very challenging due to the label biasing. Joint Multilingual ASR/LID task: A widely used technique in previous literature involves adding the language ID to the start of the speech transcript to facilitate joint training of mul- tilingual ASR and LID models [32\u201335]. Joint training can im- prove performance in certain scenarios, and it can also enhance model interpretability by separating language identification er- rors. Therefore, we have included this task in our multilingual track. The task\u2019s design is the same as the multilingual ASR task for ASR and the LID task for language identification. 2.4. Framework and Benchmark Settings Additionally, the benchmark includes few-shot cases with 20 languages and uses only 5 utterances in training for each language. These reserved few-shot training sets are not used in the monolingual ASR track. A detailed summary of the dataset is shown in Table 1. 2.2. Monolingual Track The literature suggests that speech SSL models are commonly fine-tuned on monolingual corpora [9\u201311]. In ML-SUPERB, we introduce a dedicated track for monolingual ASR to facil- itate this approach. We select nine languages based on geo- graphical and linguistic considerations to balance language and domain coverage with manageable experimental mass. In total, we introduce 14 monolingual exp. For a monolingual exp in language lang we select one dataset of this language and use it for training the model and for val- idation2. For evaluation of a monolingual exp, we use all the datasets of lang to test the trained model on various ac- cent or domain conditions. We select one pair (lang , data) for training for lang\u2208 {rus, swa, swe, jpn, cmn, xty}. For lang\u2208 {eng, fra, deu} we select respectively 3, 2 and 2 pairs (lang, data) in order to evaluate the impact of the training domain on the models\u2019 performances. For instance, for eng we have 3 monolingual exp, with (eng , MLS), (eng , NCHLT) and (eng , VoxPopuli). Toolkits: We utilize the S3PRL toolkit [2] for upstream mod- els, which offers a wide range of speech SSL model architec- tures and APIs that support customized SSL models from Hug- gingface [36] and user-defined models. For task-specific down- stream training, we use ESPnet [37]. We plan to publish ML- SUPERB as an all-in-one recipe in ESPnet\u2019s egs2 recipe col- lection, encompassing data preprocessing, training, inference, and evaluation3. Downstream model and training details: Our downstream model design is based on the SUPERB concept. First, we com- pute a weighted summation of frozen speech SSL representa- tions using learnable weights. Next, we apply a convolutional downsample layer that reduces the sequence of speech SSL fea- tures by half, passing the resulting hidden states to a transformer model consisting of two layers with an attention dimension of 256, a feedforward layer dimension of 1024, and 8 attention heads. A dropout rate of 0.1 is employed, and the model is trained using the connectionist temporal Ccassification loss. We use the Adam optimizer with a learning rate of 0.0001 and 1e- 6 weight decay. Specaugment is applied to the representation (i.e., the weighted sum of speech SSL representation) following the SUPERB benchmark. The batch size is set to 8 with the gradient accumulation as 4. The same configuration is used for all tasks in both the monolingual and multilingual tracks. 2.3. Multilingual Track Multilingual ASR task: in the multilingual ASR task, we use the training set where combining text transcriptions from all 143 languages. The multilingual ASR task has two sub-tasks on the 1We used the original split for source datasets, with the exception of SWC, M-AILABS, LAD, and ALFFA. Therefore, all datasets except these four can be used for SSL pre-training. The number of iterations in training is the only difference across tasks. In the monolingual track, due to the small train- ing size, we set it to 15,000. In the multilingual track, we use 300,000 iterations for the 10-minute train set and 600,000 for the 1-hour train set. Evaluation metric: In the monolingual track, the phoneme er- ror rate is used for jpn and cmn, while Character Error Rate (CER) is used for the remaining languages. In the multilin- gual track, we use CER for ASR evaluation and accuracy rate 2Each monolingual exp is made of one experiment with the 10- 3https://github.com/espnet/espnet/tree/ minute set for training and one with the 1-hour set. master/egs2/ml_superb/asr1 Table 2: Description of the candidate models. Model Params (M) Pre-Training # Hours # Langs wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] 95 317 317 95 317 317 317 1k 60k 65k 100k 100k 56k 400k 1 1 1 23 23 53 128 HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 95 317 95 317 95 1k 60k 10k 10k 14k 1 1 1 1 3 for LID evaluation, reporting results separately for the normal training set"}